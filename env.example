# 应用配置
APP_NAME=AI笔记本
APP_VERSION=1.0.0
DEBUG_MODE=false

# 服务器配置
SERVER_HOST=0.0.0.0
SERVER_PORT=8000

# 数据库配置
DATABASE_URL=sqlite:///./data/notebook.db
CHROMA_DB_PATH=./data/chroma_db

# --- AI模型配置 ---
# 本地AI服务(如Ollama, LMStudio)的OpenAI兼容API地址
# 在Docker外本地运行后端时，使用localhost
# 在Docker容器内运行后端时，应在docker-compose.yml中设置为 http://host.docker.internal:端口
API_URL=http://localhost:11434

# API密钥 (如果您的服务需要)
API_KEY=

# 您在本地AI服务中加载的语言模型和嵌入模型的名称
LLM_MODEL_NAME=llama3.1:8b
EMBEDDING_MODEL_NAME=mxbai-embed-large

LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2048
EMBEDDING_DIMENSION=1024

# 文件存储配置
NOTES_ROOT_PATH=./data/notes
MAX_FILE_SIZE=10485760

# 搜索配置
SEARCH_CHUNK_SIZE=1000
SEARCH_OVERLAP=200

# 前端配置
REACT_APP_API_BASE_URL=http://localhost:8000 