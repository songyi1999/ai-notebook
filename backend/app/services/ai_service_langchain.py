# LangChain-Chromaç‰ˆæœ¬çš„AIService

from typing import List, Optional, Dict, Any
import logging
from sqlalchemy.orm import Session
from langchain_openai import ChatOpenAI
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
import requests
import os
import hashlib
import threading
import time
import json
from functools import lru_cache

from ..models.file import File
from ..models.embedding import Embedding
from ..models.tag import Tag
from ..config import settings
from .mcp_service import MCPClientService
from ..schemas.mcp import MCPToolCallRequest

logger = logging.getLogger(__name__)

class OpenAICompatibleEmbeddings(Embeddings):
    """OpenAIå…¼å®¹çš„åµŒå…¥æ¨¡å‹åŒ…è£…å™¨ï¼Œç”¨äºLangChain"""
    
    def __init__(self, base_url: str, api_key: str, model: str):
        self.base_url = base_url
        self.api_key = api_key
        self.model = model
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡åµŒå…¥æ–‡æ¡£"""
        embeddings = []
        for text in texts:
            embedding = self._get_embedding(text)
            if embedding:
                embeddings.append(embedding)
            else:
                # å¦‚æœæŸä¸ªæ–‡æ¡£åµŒå…¥å¤±è´¥ï¼Œç”¨é›¶å‘é‡å ä½
                embeddings.append([0.0] * settings.embedding_dimension)
        return embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """åµŒå…¥æŸ¥è¯¢æ–‡æœ¬"""
        embedding = self._get_embedding(text)
        return embedding if embedding else [0.0] * settings.embedding_dimension
    
    def _get_embedding(self, text: str) -> List[float]:
        """ä½¿ç”¨OpenAIå…¼å®¹æ¥å£è·å–åµŒå…¥å‘é‡"""
        try:
            # ç¡®ä¿URLæ ¼å¼æ­£ç¡®ï¼Œé¿å…é‡å¤çš„/v1
            base_url = self.base_url.rstrip('/')
            if base_url.endswith('/v1'):
                url = f"{base_url}/embeddings"
            else:
                url = f"{base_url}/v1/embeddings"
            payload = {
                "model": self.model,
                "input": text,
                "encoding_format": "float"
            }
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            
            response = requests.post(url, json=payload, headers=headers, timeout=30)
            response.raise_for_status()
            
            result = response.json()
            if "data" in result and result["data"] and len(result["data"]) > 0:
                return result["data"][0]["embedding"]
            else:
                logger.error(f"åµŒå…¥å“åº”æ ¼å¼é”™è¯¯: {result}")
                return []
                
        except Exception as e:
            logger.error(f"è·å–åµŒå…¥å‘é‡å¤±è´¥: {e}")
            return []

class ChromaDBManager:
    """ChromaDBå•ä¾‹ç®¡ç†å™¨ï¼Œé¿å…å¤šå®ä¾‹å†²çª"""
    _instance = None
    _lock = threading.Lock()
    _vector_store = None
    _embeddings = None
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super(ChromaDBManager, cls).__new__(cls)
        return cls._instance
    
    def get_vector_store(self):
        """è·å–å‘é‡å­˜å‚¨å®ä¾‹"""
        if self._vector_store is None:
            with self._lock:
                if self._vector_store is None:
                    try:
                        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
                        if self._embeddings is None:
                            self._embeddings = OpenAICompatibleEmbeddings(
                                base_url=settings.get_embedding_base_url(),
                                api_key=settings.get_embedding_api_key(),
                                model=settings.embedding_model_name
                            )
                        
                        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
                        self._vector_store = Chroma(
                            collection_name="document_embeddings",
                            embedding_function=self._embeddings,
                            persist_directory=settings.chroma_db_path,
                            collection_metadata={"description": "AIç¬”è®°æœ¬æ–‡æ¡£åµŒå…¥å‘é‡"}
                        )
                        logger.info("ChromaDBå•ä¾‹åˆå§‹åŒ–æˆåŠŸ")
                        
                    except Exception as e:
                        logger.error(f"ChromaDBå•ä¾‹åˆå§‹åŒ–å¤±è´¥: {e}")
                        self._vector_store = None
        
        return self._vector_store
    
    def reset(self):
        """é‡ç½®å•ä¾‹ï¼ˆç”¨äºæµ‹è¯•æˆ–é‡æ–°åˆå§‹åŒ–ï¼‰"""
        with self._lock:
            self._vector_store = None
            self._embeddings = None

class AIService:
    """AIæœåŠ¡ç±»ï¼Œä½¿ç”¨LangChain-Chromaè¿›è¡Œå‘é‡å­˜å‚¨ - å•ä¾‹ç‰ˆæœ¬"""
    
    def __init__(self, db: Session):
        self.db = db
        self.openai_api_key = settings.openai_api_key
        self.openai_base_url = settings.openai_base_url
        
        # åˆå§‹åŒ–LLM
        if self.openai_api_key:
            self.llm = ChatOpenAI(
                openai_api_key=self.openai_api_key,
                base_url=self.openai_base_url,
                model=settings.openai_model
            )
            # åˆå§‹åŒ–æµå¼LLM
            self.streaming_llm = ChatOpenAI(
                openai_api_key=self.openai_api_key,
                base_url=self.openai_base_url,
                model=settings.openai_model,
                streaming=True
            )
        else:
            logger.warning("æœªé…ç½®OpenAI APIå¯†é’¥ï¼ŒAIåŠŸèƒ½å°†ä¸å¯ç”¨")
            self.llm = None
            self.streaming_llm = None
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = OpenAICompatibleEmbeddings(
            base_url=settings.get_embedding_base_url(),
            api_key=settings.get_embedding_api_key(),
            model=settings.embedding_model_name
        )
        
        # æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=100,
            length_function=len,
        )
        
        # ä½¿ç”¨å•ä¾‹ç®¡ç†å™¨è·å–å‘é‡å­˜å‚¨
        self.chroma_manager = ChromaDBManager()
        self.vector_store = self.chroma_manager.get_vector_store()
        
        # æ·»åŠ æŸ¥è¯¢å‘é‡ç¼“å­˜
        self._query_cache = {}
        self._cache_lock = threading.Lock()
        self._max_cache_size = 100  # æœ€å¤§ç¼“å­˜100ä¸ªæŸ¥è¯¢
        
        # åˆå§‹åŒ–MCPæœåŠ¡
        self.mcp_service = MCPClientService(db)

    def is_available(self) -> bool:
        """æ£€æŸ¥AIæœåŠ¡æ˜¯å¦å¯ç”¨"""
        return bool(self.openai_api_key and self.vector_store)

    def create_embeddings(self, file: File, progress_callback=None) -> bool:
        """ä¸ºæ–‡ä»¶åˆ›å»ºå‘é‡åµŒå…¥ - ä½¿ç”¨æ™ºèƒ½å¤šå±‚æ¬¡åˆ†å—"""
        if not self.is_available():
            logger.warning("AIæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•åˆ›å»ºåµŒå…¥")
            return False
        
        try:
            logger.info(f"å¼€å§‹ä¸ºæ–‡ä»¶åˆ›å»ºæ™ºèƒ½åµŒå…¥: {file.file_path}")
            
            # 1. æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç°æœ‰çš„åµŒå…¥è®°å½•
            existing_embeddings_count = self.db.query(Embedding).filter(Embedding.file_id == file.id).count()
            
            if existing_embeddings_count > 0:
                logger.info(f"æ–‡ä»¶ {file.id} å­˜åœ¨ {existing_embeddings_count} ä¸ªç°æœ‰åµŒå…¥ï¼Œéœ€è¦æ¸…ç†")
                
                # 1.1 åˆ é™¤ç°æœ‰çš„å‘é‡å­˜å‚¨ä¸­çš„æ–‡æ¡£ï¼ˆå…ˆåˆ é™¤å‘é‡å­˜å‚¨ï¼‰
                try:
                    existing_docs = self.vector_store.get(
                        where={"file_id": file.id}
                    )
                    if existing_docs and existing_docs.get('ids'):
                        self.vector_store.delete(ids=existing_docs['ids'])
                        logger.info(f"ä»LangChainå‘é‡å­˜å‚¨åˆ é™¤æ–‡ä»¶ {file.id} çš„æ–‡æ¡£: {len(existing_docs['ids'])} ä¸ª")
                except Exception as e:
                    logger.warning(f"åˆ é™¤ç°æœ‰å‘é‡å­˜å‚¨æ—¶å‡ºé”™: {e}")
                
                # 1.2 åˆ é™¤ç°æœ‰çš„SQLiteåµŒå…¥è®°å½•ï¼ˆç„¶ååˆ é™¤SQLiteè®°å½•ï¼‰
                try:
                    deleted_count = self.db.query(Embedding).filter(Embedding.file_id == file.id).delete()
                    self.db.commit()  # ç«‹å³æäº¤åˆ é™¤æ“ä½œ
                    logger.info(f"æˆåŠŸåˆ é™¤æ–‡ä»¶çš„å‘é‡ç´¢å¼•: file_id={file.id}, SQLiteåˆ é™¤äº† {deleted_count} ä¸ªè®°å½•")
                except Exception as e:
                    logger.warning(f"åˆ é™¤SQLiteåµŒå…¥è®°å½•æ—¶å‡ºé”™: {e}")
                    self.db.rollback()
            else:
                logger.info(f"æ–‡ä»¶ {file.id} æ²¡æœ‰ç°æœ‰åµŒå…¥ï¼Œç›´æ¥åˆ›å»ºæ–°çš„")
                
            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿åˆ é™¤æ“ä½œå®Œå…¨å®Œæˆ
            import time
            time.sleep(0.1)
            
            # 3. ä½¿ç”¨æ™ºèƒ½å¤šå±‚æ¬¡åˆ†å—ï¼ˆæ¯ä¸ªæ–‡ä»¶éƒ½æœ‰æ±‡æ€»æçº²ï¼‰
            documents = self._create_hierarchical_chunks(file, progress_callback)
            
            # 4. æ‰¹é‡æ·»åŠ åˆ°å‘é‡å­˜å‚¨
            if documents:
                if progress_callback:
                    progress_callback("å‘é‡å­˜å‚¨", f"æ­£åœ¨ä¿å­˜ {len(documents)} ä¸ªåˆ†å—åˆ°å‘é‡æ•°æ®åº“")
                
                # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§å¤„ç†è¿‡å¤šæ–‡æ¡£å¯¼è‡´è¶…æ—¶
                batch_size = 50  # æ¯æ‰¹å¤„ç†50ä¸ªæ–‡æ¡£
                total_docs = len(documents)
                logger.info(f"å¼€å§‹åˆ†æ‰¹å‘é‡åŒ–ï¼Œæ€»æ–‡æ¡£æ•°: {total_docs}, æ‰¹å¤§å°: {batch_size}")
                
                for i in range(0, total_docs, batch_size):
                    batch_start = i
                    batch_end = min(i + batch_size, total_docs)
                    batch_docs = documents[batch_start:batch_end]
                    
                    try:
                        # ä¸ºå½“å‰æ‰¹æ¬¡ç”ŸæˆID
                        batch_ids = [f"file_{file.id}_chunk_{doc.metadata['chunk_index']}_{doc.metadata['chunk_type']}" for doc in batch_docs]
                        
                        logger.info(f"æ­£åœ¨å¤„ç†ç¬¬ {i//batch_size + 1} æ‰¹ï¼Œæ–‡æ¡£ {batch_start+1}-{batch_end}/{total_docs}")
                        
                        if progress_callback:
                            progress_callback("å‘é‡å­˜å‚¨", f"æ­£åœ¨å¤„ç†ç¬¬ {i//batch_size + 1} æ‰¹ ({batch_start+1}-{batch_end}/{total_docs})")
                        
                        # ä¿å­˜åˆ°ChromaDB
                        self.vector_store.add_documents(batch_docs, ids=batch_ids)
                        logger.info(f"âœ… æˆåŠŸä¿å­˜ç¬¬ {i//batch_size + 1} æ‰¹åˆ°ChromaDBï¼ŒåŒ…å« {len(batch_docs)} ä¸ªæ–‡æ¡£")
                        
                        # çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…è¿‡åº¦å ç”¨èµ„æº
                        import time
                        time.sleep(0.1)
                        
                    except Exception as e:
                        logger.error(f"âŒ ä¿å­˜ç¬¬ {i//batch_size + 1} æ‰¹åˆ°ChromaDBå¤±è´¥: {e}")
                        # å¦‚æœæŸæ‰¹å¤±è´¥ï¼Œå¯ä»¥è€ƒè™‘ç»§ç»­å¤„ç†å…¶ä»–æ‰¹æ¬¡ï¼Œæˆ–è€…ç›´æ¥å¤±è´¥
                        self.db.rollback()
                        return False
                
                logger.info(f"ğŸ‰ æˆåŠŸæ·»åŠ æ‰€æœ‰ {len(documents)} ä¸ªæ–‡æ¡£åˆ°LangChain-Chroma")
            
            # 5. æäº¤SQLiteäº‹åŠ¡
            try:
                self.db.commit()
                logger.info(f"âœ… SQLiteäº‹åŠ¡æäº¤æˆåŠŸï¼Œæ–‡ä»¶: {file.file_path}")
            except Exception as e:
                logger.error(f"âŒ SQLiteäº‹åŠ¡æäº¤å¤±è´¥: {e}")
                self.db.rollback()
                return False
            
            if progress_callback:
                progress_callback("å®Œæˆ", f"æ™ºèƒ½åˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(documents)} ä¸ªå‘é‡")
            
            logger.info(f"ä¸ºæ–‡ä»¶ {file.file_path} åˆ›å»ºäº† {len(documents)} ä¸ªæ™ºèƒ½åµŒå…¥å‘é‡")
            return True
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ™ºèƒ½åµŒå…¥å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            self.db.rollback()
            return False
    

    
    def _create_hierarchical_chunks(self, file: File, progress_callback=None) -> List[Document]:
        """åˆ›å»ºæ™ºèƒ½å¤šå±‚æ¬¡åˆ†å—ï¼ˆåŸºäºLLMï¼‰"""
        try:
            from .hierarchical_splitter import IntelligentHierarchicalSplitter
            
            if progress_callback:
                progress_callback("åˆ†æä¸­", f"æ­£åœ¨åˆ†ææ–‡ä»¶ç»“æ„å’Œå†…å®¹")
            
            # åˆ›å»ºæ™ºèƒ½åˆ†å—å™¨ï¼Œä¼ å…¥LLMå®ä¾‹
            splitter = IntelligentHierarchicalSplitter(llm=self.llm)
            hierarchical_docs = splitter.split_document(file.content, file.title, file.id, progress_callback)
            
            all_documents = []
            
            # å¤„ç†æ‘˜è¦å±‚
            if progress_callback:
                progress_callback("æ‘˜è¦ç”Ÿæˆ", f"æ­£åœ¨å¤„ç†æ–‡ä»¶æ‘˜è¦")
            for doc in hierarchical_docs.get('summary', []):
                all_documents.append(doc)
                self._save_embedding_metadata(doc, file.id)
            
            # å¤„ç†å¤§çº²å±‚
            if progress_callback:
                progress_callback("å¤§çº²æå–", f"æ­£åœ¨å¤„ç†æ–‡ä»¶å¤§çº²")
            for doc in hierarchical_docs.get('outline', []):
                all_documents.append(doc)
                self._save_embedding_metadata(doc, file.id)
            
            # å¤„ç†å†…å®¹å±‚
            if progress_callback:
                progress_callback("å†…å®¹åˆ†å—", f"æ­£åœ¨å¤„ç†å†…å®¹åˆ†å—")
            for doc in hierarchical_docs.get('content', []):
                all_documents.append(doc)
                self._save_embedding_metadata(doc, file.id)
            
            logger.info(f"æ™ºèƒ½å¤šå±‚æ¬¡åˆ†å—å®Œæˆ: æ€»å…± {len(all_documents)} ä¸ªæ–‡æ¡£")
            return all_documents
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ™ºèƒ½å¤šå±‚æ¬¡åˆ†å—å¤±è´¥: {e}")
            # åˆ›å»ºæœ€åŸºæœ¬çš„æ‘˜è¦å’Œå†…å®¹å—ï¼ˆé™çº§ç­–ç•¥ï¼‰
            if progress_callback:
                progress_callback("é™çº§å¤„ç†", f"æ™ºèƒ½åˆ†å—å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬åˆ†å—ç­–ç•¥")
            return self._create_basic_fallback_chunks(file, progress_callback)
    
    def _create_basic_fallback_chunks(self, file: File, progress_callback=None) -> List[Document]:
        """åˆ›å»ºåŸºæœ¬çš„é™çº§åˆ†å—ï¼ˆç¡®ä¿æ¯ä¸ªæ–‡ä»¶éƒ½æœ‰æ‘˜è¦å’Œå†…å®¹å—ï¼‰"""
        try:
            documents = []
            
            # 1. åˆ›å»ºåŸºæœ¬æ‘˜è¦å—
            if progress_callback:
                progress_callback("åŸºæœ¬æ‘˜è¦", f"åˆ›å»ºåŸºæœ¬æ‘˜è¦å—")
            
            summary_text = f"æ–‡ä»¶ï¼š{file.title}\nå†…å®¹é¢„è§ˆï¼š{file.content[:500]}..."
            summary_doc = Document(
                page_content=summary_text,
                metadata={
                    "file_id": file.id,
                    "file_path": file.file_path,
                    "chunk_index": 0,
                    "chunk_hash": hashlib.sha256(summary_text.encode()).hexdigest(),
                    "title": file.title,
                    "vector_model": settings.embedding_model_name,
                    "chunk_type": "summary",
                    "chunk_level": 1,
                    "parent_heading": None,
                    "section_path": "åŸºæœ¬æ‘˜è¦",
                    "generation_method": "basic_fallback"
                }
            )
            documents.append(summary_doc)
            self._save_embedding_metadata(summary_doc, file.id)
            
            # 2. åˆ›å»ºå†…å®¹å—
            if progress_callback:
                progress_callback("å†…å®¹åˆ†å—", f"æ­£åœ¨åˆ›å»ºå†…å®¹åˆ†å—")
            
            # ä½¿ç”¨æ–‡æœ¬åˆ†å‰²å™¨åˆ›å»ºå†…å®¹å—
            content_chunks = self.text_splitter.split_text(file.content)
            
            for i, chunk in enumerate(content_chunks):
                content_doc = Document(
                    page_content=chunk,
                    metadata={
                        "file_id": file.id,
                        "file_path": file.file_path,
                        "chunk_index": i + 1,
                        "chunk_hash": hashlib.sha256(chunk.encode()).hexdigest(),
                        "title": file.title,
                        "vector_model": settings.embedding_model_name,
                        "chunk_type": "content",
                        "chunk_level": 3,
                        "parent_heading": None,
                        "section_path": f"å†…å®¹å—{i+1}",
                        "generation_method": "basic_fallback"
                    }
                )
                documents.append(content_doc)
                self._save_embedding_metadata(content_doc, file.id)
            
            logger.info(f"åŸºæœ¬åˆ†å—å®Œæˆ: 1ä¸ªæ‘˜è¦å— + {len(content_chunks)}ä¸ªå†…å®¹å—")
            return documents
            
        except Exception as e:
            logger.error(f"åˆ›å»ºåŸºæœ¬åˆ†å—å¤±è´¥: {e}")
            return []

    def _save_embedding_metadata(self, doc: Document, file_id: int):
        """ä¿å­˜åµŒå…¥å…ƒæ•°æ®åˆ°SQLite"""
        try:
            # åˆ›å»ºåµŒå…¥è®°å½•
            embedding = Embedding(
                file_id=file_id,
                chunk_index=doc.metadata['chunk_index'],
                chunk_hash=doc.metadata['chunk_hash'],
                vector_model=doc.metadata['vector_model'],
                chunk_type=doc.metadata.get('chunk_type', 'content'),
                chunk_level=doc.metadata.get('chunk_level', 1),
                parent_heading=doc.metadata.get('parent_heading'),
                section_path=doc.metadata.get('section_path'),
                generation_method=doc.metadata.get('generation_method', 'hierarchical')
            )
            self.db.add(embedding)
            # ä¸åœ¨è¿™é‡Œæäº¤ï¼Œè®©ä¸Šå±‚ç»Ÿä¸€æäº¤
            
        except Exception as e:
            logger.error(f"ä¿å­˜åµŒå…¥å…ƒæ•°æ®å¤±è´¥: {e}")
            raise

    def semantic_search(self, query: str, limit: int = 10, similarity_threshold: float = None) -> List[Dict[str, Any]]:
        """è¯­ä¹‰æœç´¢ - æ”¯æŒå¤šå±‚æ¬¡æ£€ç´¢ï¼Œå¸¦ç¼“å­˜ä¼˜åŒ–"""
        if not self.is_available():
            logger.warning("AIæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œè¯­ä¹‰æœç´¢")
            return []
        
        # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤é˜ˆå€¼
        if similarity_threshold is None:
            similarity_threshold = settings.semantic_search_threshold
        
        try:
            start_time = time.time()
            logger.info(f"å¼€å§‹è¯­ä¹‰æœç´¢ï¼ŒæŸ¥è¯¢: {query}, é˜ˆå€¼: {similarity_threshold}")
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨å¤šå±‚æ¬¡æ£€ç´¢
            if settings.enable_hierarchical_chunking:
                results = self._hierarchical_semantic_search(query, limit, similarity_threshold)
            else:
                results = self._traditional_semantic_search(query, limit, similarity_threshold)
            
            total_time = time.time() - start_time
            logger.info(f"è¯­ä¹‰æœç´¢å®Œæˆï¼ŒæŸ¥è¯¢: {query}, ç»“æœ: {len(results)}, æ€»è€—æ—¶: {total_time:.3f}ç§’")
            return results
            
        except Exception as e:
            logger.error(f"è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return []
    
    def _traditional_semantic_search(self, query: str, limit: int, similarity_threshold: float) -> List[Dict[str, Any]]:
        """ä¼ ç»Ÿè¯­ä¹‰æœç´¢ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰"""
        try:
            # ä½¿ç”¨LangChainçš„similarity_search_with_scoreæ–¹æ³•ï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰
            search_results = self.vector_store.similarity_search_with_score(
                query=query,
                k=limit * 2,  # è·å–æ›´å¤šç»“æœç”¨äºè¿‡æ»¤
                filter=None  # å¯ä»¥æ·»åŠ è¿‡æ»¤æ¡ä»¶
            )
            
            logger.info(f"ä¼ ç»Ÿæœç´¢è¿”å› {len(search_results)} ä¸ªç»“æœ")
            
            # å¤„ç†æœç´¢ç»“æœå¹¶å»é‡
            results = []
            seen_files = {}  # ç”¨äºæ–‡ä»¶å»é‡ï¼šfile_id -> æœ€ä½³åŒ¹é…ç»“æœ
            
            for doc, score in search_results:
                # LangChain-Chromaè¿”å›çš„scoreæ˜¯è·ç¦»ï¼Œè·ç¦»è¶Šå°è¶Šç›¸ä¼¼
                distance = score
                
                logger.info(f"æ–‡æ¡£: {doc.metadata.get('file_path', 'unknown')}, è·ç¦»: {distance:.4f}")
                
                # è¿‡æ»¤è·ç¦»è¿‡å¤§çš„ç»“æœï¼ˆè·ç¦»å°äºé˜ˆå€¼çš„ä¿ç•™ï¼‰
                if distance > similarity_threshold:
                    logger.info(f"è·ç¦» {distance:.4f} å¤§äºé˜ˆå€¼ {similarity_threshold}ï¼Œè·³è¿‡")
                    continue
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä»ç„¶å­˜åœ¨ä¸”æœªåˆ é™¤
                file_id = doc.metadata.get('file_id')
                if file_id:
                    file = self.db.query(File).filter(
                        File.id == file_id,
                        File.is_deleted == False
                    ).first()
                    
                    if file:
                        result_item = {
                            'file_id': file_id,
                            'file_path': doc.metadata.get('file_path', ''),
                            'title': doc.metadata.get('title', ''),
                            'chunk_text': doc.page_content,
                            'chunk_index': doc.metadata.get('chunk_index', 0),
                            'similarity': float(1 - distance),  # ä¸ºå…¼å®¹æ€§è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼ˆ1-è·ç¦»ï¼‰
                            'distance': float(distance),  # ä¿å­˜åŸå§‹è·ç¦»ç”¨äºæ¯”è¾ƒ
                            'created_at': file.created_at.isoformat() if file.created_at else None,
                            'updated_at': file.updated_at.isoformat() if file.updated_at else None,
                        }
                        
                        # æ–‡ä»¶å»é‡ï¼šä¿ç•™æ¯ä¸ªæ–‡ä»¶çš„æœ€ä½³åŒ¹é…ï¼ˆè·ç¦»æœ€å°ï¼‰
                        if file_id not in seen_files or distance < seen_files[file_id]['distance']:
                            seen_files[file_id] = result_item
                            logger.info(f"æ·»åŠ /æ›´æ–°æœ€ä½³åŒ¹é…: {doc.metadata.get('file_path')}, è·ç¦»: {distance:.4f}")
                        else:
                            logger.info(f"è·³è¿‡é‡å¤æ–‡ä»¶ï¼ˆè·ç¦»æ›´å¤§ï¼‰: {doc.metadata.get('file_path')}, è·ç¦»: {distance:.4f}")
                    else:
                        logger.info(f"æ–‡ä»¶ä¸å­˜åœ¨æˆ–å·²åˆ é™¤: file_id={file_id}")
            
            # å°†å»é‡åçš„ç»“æœè½¬æ¢ä¸ºåˆ—è¡¨ï¼ŒæŒ‰è·ç¦»æ’åº
            results = list(seen_files.values())
            results.sort(key=lambda x: x['distance'])  # æŒ‰è·ç¦»å‡åºæ’åºï¼ˆæœ€ç›¸ä¼¼çš„åœ¨å‰ï¼‰
            
            # ç§»é™¤ä¸´æ—¶çš„distanceå­—æ®µï¼Œé™åˆ¶ç»“æœæ•°é‡
            for result in results[:limit]:
                result.pop('distance', None)
            
            return results
            
        except Exception as e:
            logger.error(f"ä¼ ç»Ÿè¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            return []
    
    def _hierarchical_semantic_search(self, query: str, limit: int, similarity_threshold: float) -> List[Dict[str, Any]]:
        """å¤šå±‚æ¬¡è¯­ä¹‰æœç´¢"""
        try:
            logger.info(f"å¼€å§‹å¤šå±‚æ¬¡è¯­ä¹‰æœç´¢: {query}")
            
            # å¤šè·¯å¬å›ï¼šåŒæ—¶æœç´¢ä¸‰ä¸ªå±‚æ¬¡
            summary_results = self._search_by_chunk_type(query, "summary", limit//3, similarity_threshold)
            outline_results = self._search_by_chunk_type(query, "outline", limit//3, similarity_threshold)
            content_results = self._search_by_chunk_type(query, "content", limit, similarity_threshold)
            
            # æ™ºèƒ½ä¸Šä¸‹æ–‡æ‰©å±•
            expanded_results = []
            
            # å¤„ç†æ‘˜è¦åŒ¹é…ç»“æœ
            for result in summary_results:
                expanded_results.append(result)
                # è·å–è¯¥æ–‡ä»¶çš„å¤§çº²å’Œå†…å®¹
                file_outline = self._get_file_outline(result['file_id'])
                expanded_results.extend(file_outline[:2])  # æ·»åŠ å‰2ä¸ªå¤§çº²é¡¹
            
            # å¤„ç†å¤§çº²åŒ¹é…ç»“æœ
            for result in outline_results:
                expanded_results.append(result)
                # è·å–è¯¥ç« èŠ‚ä¸‹çš„å†…å®¹å—
                section_content = self._get_section_content(result['file_id'], result.get('section_path'))
                expanded_results.extend(section_content[:2])  # æ·»åŠ å‰2ä¸ªå†…å®¹å—
            
            # å¤„ç†å†…å®¹åŒ¹é…ç»“æœ
            expanded_results.extend(content_results)
            
            # å»é‡å¹¶é™åˆ¶ç»“æœæ•°é‡
            final_results = self._deduplicate_and_rank(expanded_results, limit)
            
            logger.info(f"å¤šå±‚æ¬¡æœç´¢å®Œæˆ: æ‘˜è¦={len(summary_results)}, å¤§çº²={len(outline_results)}, å†…å®¹={len(content_results)}, æœ€ç»ˆ={len(final_results)}")
            return final_results
            
        except Exception as e:
            logger.error(f"å¤šå±‚æ¬¡è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            # é™çº§åˆ°ä¼ ç»Ÿæœç´¢
            return self._traditional_semantic_search(query, limit, similarity_threshold)
    
    def _search_by_chunk_type(self, query: str, chunk_type: str, limit: int, similarity_threshold: float) -> List[Dict[str, Any]]:
        """æŒ‰åˆ†å—ç±»å‹æœç´¢"""
        try:
            search_results = self.vector_store.similarity_search_with_score(
                query=query,
                k=limit * 2,
                filter={"chunk_type": chunk_type}
            )
            
            results = []
            for doc, score in search_results:
                if score <= similarity_threshold:
                    file_id = doc.metadata.get('file_id')
                    if file_id:
                        file = self.db.query(File).filter(
                            File.id == file_id,
                            File.is_deleted == False
                        ).first()
                        
                        if file:
                            result_item = {
                                'file_id': file_id,
                                'file_path': doc.metadata.get('file_path', ''),
                                'title': doc.metadata.get('title', ''),
                                'chunk_text': doc.page_content,
                                'chunk_index': doc.metadata.get('chunk_index', 0),
                                'chunk_type': chunk_type,
                                'chunk_level': doc.metadata.get('chunk_level', 3),
                                'parent_heading': doc.metadata.get('parent_heading'),
                                'section_path': doc.metadata.get('section_path'),
                                'similarity': float(1 - score),
                                'created_at': file.created_at.isoformat() if file.created_at else None,
                                'updated_at': file.updated_at.isoformat() if file.updated_at else None,
                            }
                            results.append(result_item)
            
            return results[:limit]
            
        except Exception as e:
            logger.error(f"æŒ‰ç±»å‹æœç´¢å¤±è´¥ ({chunk_type}): {e}")
            return []
    
    def _get_file_outline(self, file_id: int) -> List[Dict[str, Any]]:
        """è·å–æ–‡ä»¶çš„å¤§çº²"""
        try:
            # ä»å‘é‡å­˜å‚¨ä¸­è·å–è¯¥æ–‡ä»¶çš„outlineç±»å‹æ–‡æ¡£
            docs = self.vector_store.get(
                where={"file_id": file_id, "chunk_type": "outline"},
                limit=10
            )
            
            results = []
            if docs and docs.get('documents'):
                for i, doc_content in enumerate(docs['documents']):
                    metadata = docs['metadatas'][i]
                    result_item = {
                        'file_id': file_id,
                        'file_path': metadata.get('file_path', ''),
                        'title': metadata.get('title', ''),
                        'chunk_text': doc_content,
                        'chunk_index': metadata.get('chunk_index', 0),
                        'chunk_type': 'outline',
                        'chunk_level': 2,
                        'parent_heading': metadata.get('parent_heading'),
                        'section_path': metadata.get('section_path'),
                        'similarity': 0.8,  # ä¸Šä¸‹æ–‡ç›¸å…³æ€§
                    }
                    results.append(result_item)
            
            return results
            
        except Exception as e:
            logger.error(f"è·å–æ–‡ä»¶å¤§çº²å¤±è´¥: {e}")
            return []
    
    def _get_section_content(self, file_id: int, section_path: str) -> List[Dict[str, Any]]:
        """è·å–ç« èŠ‚å†…å®¹"""
        try:
            # ä»å‘é‡å­˜å‚¨ä¸­è·å–è¯¥ç« èŠ‚çš„å†…å®¹
            docs = self.vector_store.get(
                where={"file_id": file_id, "chunk_type": "content", "parent_heading": section_path},
                limit=5
            )
            
            results = []
            if docs and docs.get('documents'):
                for i, doc_content in enumerate(docs['documents']):
                    metadata = docs['metadatas'][i]
                    result_item = {
                        'file_id': file_id,
                        'file_path': metadata.get('file_path', ''),
                        'title': metadata.get('title', ''),
                        'chunk_text': doc_content,
                        'chunk_index': metadata.get('chunk_index', 0),
                        'chunk_type': 'content',
                        'chunk_level': 3,
                        'parent_heading': metadata.get('parent_heading'),
                        'section_path': metadata.get('section_path'),
                        'similarity': 0.7,  # ä¸Šä¸‹æ–‡ç›¸å…³æ€§
                    }
                    results.append(result_item)
            
            return results
            
        except Exception as e:
            logger.error(f"è·å–ç« èŠ‚å†…å®¹å¤±è´¥: {e}")
            return []
    
    def _deduplicate_and_rank(self, results: List[Dict[str, Any]], limit: int) -> List[Dict[str, Any]]:
        """å»é‡å¹¶æ’åº"""
        seen_chunks = set()
        unique_results = []
        
        for result in results:
            chunk_key = (result['file_id'], result['chunk_index'], result.get('chunk_type', 'content'))
            if chunk_key not in seen_chunks:
                seen_chunks.add(chunk_key)
                unique_results.append(result)
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        unique_results.sort(key=lambda x: x.get('similarity', 0), reverse=True)
        
        return unique_results[:limit]

    def clear_vector_database(self) -> bool:
        """æ¸…ç©ºå‘é‡æ•°æ®åº“"""
        try:
            # 1. æ¸…ç©ºSQLiteä¸­çš„åµŒå…¥å‘é‡
            self.db.query(Embedding).delete()
            self.db.commit()
            
            # 2. æ¸…ç©ºLangChainå‘é‡å­˜å‚¨
            if self.vector_store:
                try:
                    # è·å–æ‰€æœ‰æ–‡æ¡£IDå¹¶åˆ é™¤
                    all_docs = self.vector_store.get()
                    if all_docs and all_docs.get('ids'):
                        self.vector_store.delete(ids=all_docs['ids'])
                        logger.info(f"æ¸…ç©ºLangChainå‘é‡å­˜å‚¨ï¼Œåˆ é™¤äº† {len(all_docs['ids'])} ä¸ªæ–‡æ¡£")
                except Exception as e:
                    logger.warning(f"æ¸…ç©ºLangChainå‘é‡å­˜å‚¨æ—¶å‡ºé”™: {e}")
            
            logger.info("å‘é‡æ•°æ®åº“å·²æ¸…ç©º")
            return True
            
        except Exception as e:
            logger.error(f"æ¸…ç©ºå‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            self.db.rollback()
            return False

    def delete_document_by_file_path(self, file_path: str) -> bool:
        """æ ¹æ®æ–‡ä»¶è·¯å¾„åˆ é™¤æ–‡æ¡£çš„å‘é‡ç´¢å¼•"""
        try:
            # æ ¹æ®æ–‡ä»¶è·¯å¾„æŸ¥æ‰¾æ–‡ä»¶
            file = self.db.query(File).filter(File.file_path == file_path).first()
            if not file:
                logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•åˆ é™¤å‘é‡ç´¢å¼•: {file_path}")
                return False
            
            return self.delete_document_by_file_id(file.id)
            
        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡ä»¶å‘é‡ç´¢å¼•å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            return False

    def delete_document_by_file_id(self, file_id: int) -> bool:
        """æ ¹æ®æ–‡ä»¶IDåˆ é™¤æ–‡æ¡£çš„å‘é‡ç´¢å¼•"""
        try:
            # 1. åˆ é™¤SQLiteä¸­çš„åµŒå…¥è®°å½•
            deleted_count = self.db.query(Embedding).filter(Embedding.file_id == file_id).delete()
            
            # 2. åˆ é™¤LangChainå‘é‡å­˜å‚¨ä¸­çš„æ–‡æ¡£
            if self.vector_store:
                try:
                    existing_docs = self.vector_store.get(
                        where={"file_id": file_id}
                    )
                    if existing_docs and existing_docs.get('ids'):
                        self.vector_store.delete(ids=existing_docs['ids'])
                        logger.info(f"ä»LangChainå‘é‡å­˜å‚¨åˆ é™¤æ–‡ä»¶ {file_id} çš„æ–‡æ¡£: {len(existing_docs['ids'])} ä¸ª")
                except Exception as e:
                    logger.warning(f"ä»LangChainå‘é‡å­˜å‚¨åˆ é™¤æ–‡æ¡£æ—¶å‡ºé”™: {e}")
            
            self.db.commit()
            logger.info(f"æˆåŠŸåˆ é™¤æ–‡ä»¶çš„å‘é‡ç´¢å¼•: file_id={file_id}, SQLiteåˆ é™¤äº† {deleted_count} ä¸ªè®°å½•")
            return True
            
        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡ä»¶å‘é‡ç´¢å¼•å¤±è´¥: file_id={file_id}, é”™è¯¯: {e}")
            self.db.rollback()
            return False

    def get_vector_count(self) -> int:
        """è·å–å‘é‡æ•°æ®åº“ä¸­çš„å‘é‡æ•°é‡ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œé¿å…è·å–æ‰€æœ‰æ•°æ®"""
        try:
            if self.vector_store:
                # ä½¿ç”¨ChromaDBçš„countæ–¹æ³•ï¼Œé¿å…è·å–æ‰€æœ‰æ•°æ®
                try:
                    # å°è¯•ä½¿ç”¨ChromaDBçš„å†…éƒ¨æ–¹æ³•è·å–æ•°é‡
                    collection = self.vector_store._collection
                    if hasattr(collection, 'count'):
                        return collection.count()
                    elif hasattr(collection, '_count'):
                        return collection._count()
                    else:
                        # å¦‚æœæ²¡æœ‰ç›´æ¥çš„countæ–¹æ³•ï¼Œä½¿ç”¨limit=1çš„æŸ¥è¯¢æ¥æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
                        # è¿™æ ·é¿å…è·å–æ‰€æœ‰æ•°æ®
                        sample = self.vector_store.get(limit=1)
                        if sample and sample.get('ids'):
                            # æœ‰æ•°æ®ä½†æ— æ³•è·å–ç²¾ç¡®æ•°é‡ï¼Œè¿”å›ä¼°ç®—å€¼
                            return -1  # ä½¿ç”¨-1è¡¨ç¤º"æœ‰æ•°æ®ä½†æ•°é‡æœªçŸ¥"
                        else:
                            return 0
                except Exception as e:
                    logger.warning(f"æ— æ³•è·å–ç²¾ç¡®å‘é‡æ•°é‡: {e}")
                    # é™çº§æ–¹æ¡ˆï¼šå°è¯•ç®€å•æŸ¥è¯¢æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
                    try:
                        sample = self.vector_store.get(limit=1)
                        return -1 if sample and sample.get('ids') else 0
                    except:
                        return 0
            return 0
        except Exception as e:
            logger.error(f"è·å–å‘é‡æ•°é‡å¤±è´¥: {e}")
            return 0

    def add_document_to_vector_db(self, file_id: int, title: str, content: str, metadata: Dict[str, Any] = None) -> bool:
        """å‘å‘é‡æ•°æ®åº“æ·»åŠ æ–‡æ¡£"""
        try:
            # è·å–æ–‡ä»¶å¯¹è±¡
            file = self.db.query(File).filter(File.id == file_id).first()
            if not file:
                logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_id}")
                return False
            
            # åˆ›å»ºåµŒå…¥å‘é‡
            success = self.create_embeddings(file)
            if success:
                logger.info(f"æ–‡æ¡£å·²æ·»åŠ åˆ°å‘é‡æ•°æ®åº“: {title}")
            return success
            
        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            return False

    def generate_summary(self, content: str, max_length: int = 200) -> Optional[str]:
        """ç”Ÿæˆæ–‡æ¡£æ‘˜è¦"""
        if not self.llm:
            logger.warning("LLMä¸å¯ç”¨ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦")
            return None
        
        try:
            prompt = f"""è¯·ä¸ºä»¥ä¸‹å†…å®¹ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼Œä¸è¶…è¿‡{max_length}å­—ï¼š

å†…å®¹ï¼š
{content[:2000]}  # é™åˆ¶è¾“å…¥é•¿åº¦

æ‘˜è¦ï¼š"""
            
            response = self.llm.invoke(prompt)
            summary = response.content.strip()
            
            logger.info(f"æ‘˜è¦ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(summary)}")
            return summary
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
            return None

    def suggest_tags(self, title: str, content: str, max_tags: int = 5) -> List[str]:
        """æ™ºèƒ½æ ‡ç­¾å»ºè®® - æ”¯æŒå¤šå±‚æ¬¡åˆ†æï¼Œä»é¢„è®¾å’Œæ•°æ®åº“ç°æœ‰æ ‡ç­¾ä¸­é€‰æ‹©"""
        if not self.llm:
            logger.warning("LLMä¸å¯ç”¨ï¼Œæ— æ³•ç”Ÿæˆæ ‡ç­¾å»ºè®®")
            return []
        
        try:
            # 1. å®šä¹‰é¢„è®¾çš„å¸¸è§„æ ‡ç­¾
            predefined_tags = [
                "é‡ç‚¹", "å‰ç«¯", "åç«¯", "AIå¤§æ¨¡å‹", "æŠ€å·§", 
                "ç¬”è®°", "æ€»ç»“", "æ•™ç¨‹", "æ–‡æ¡£", "é…ç½®",
                "é—®é¢˜", "è§£å†³æ–¹æ¡ˆ", "ä»£ç ", "å·¥å…·", "æ¡†æ¶",
                "æ•°æ®åº“", "ç½‘ç»œ", "å®‰å…¨", "æ€§èƒ½", "æµ‹è¯•",
                "éƒ¨ç½²", "è¿ç»´", "ç®—æ³•", "æ¶æ„", "è®¾è®¡",
                "å­¦ä¹ ", "èµ„æº", "å‚è€ƒ", "ç¤ºä¾‹", "æ¨¡æ¿"
            ]
            
            # 2. ä»æ•°æ®åº“è·å–ç°æœ‰çš„ä¸é‡å¤æ ‡ç­¾
            existing_tags = []
            try:
                # ä½¿ç”¨ distinct æˆ– group by è·å–ä¸é‡å¤çš„æ ‡ç­¾åç§°ï¼ŒæŒ‰ä½¿ç”¨æ¬¡æ•°é™åºæ’åˆ—
                db_tags = self.db.query(Tag.name).filter(Tag.name.isnot(None)).distinct().order_by(Tag.usage_count.desc()).limit(50).all()
                existing_tags = [tag.name for tag in db_tags if tag.name]
                logger.info(f"ä»æ•°æ®åº“è·å–åˆ° {len(existing_tags)} ä¸ªç°æœ‰æ ‡ç­¾")
            except Exception as e:
                logger.warning(f"è·å–æ•°æ®åº“æ ‡ç­¾å¤±è´¥: {e}")
            
            # 3. åˆå¹¶å€™é€‰æ ‡ç­¾ï¼Œå»é‡å¹¶ä¿æŒé¡ºåº
            candidate_tags = []
            seen = set()
            
            # å…ˆæ·»åŠ é¢„è®¾æ ‡ç­¾
            for tag in predefined_tags:
                if tag not in seen:
                    candidate_tags.append(tag)
                    seen.add(tag)
            
            # å†æ·»åŠ æ•°æ®åº“ä¸­çš„ç°æœ‰æ ‡ç­¾
            for tag in existing_tags:
                if tag not in seen:
                    candidate_tags.append(tag)
                    seen.add(tag)
            
            logger.info(f"æ€»å…±æœ‰ {len(candidate_tags)} ä¸ªå€™é€‰æ ‡ç­¾")
            
            # 4. å‡†å¤‡åˆ†æå†…å®¹ï¼ˆæ”¯æŒå¤šå±‚æ¬¡åˆ†æï¼‰
            analysis_content = self._prepare_content_for_tagging(title, content)
            
            # 5. æ„å»ºæç¤ºè¯ï¼Œè¦æ±‚ä»å€™é€‰æ ‡ç­¾ä¸­é€‰æ‹©
            candidate_tags_text = "ã€".join(candidate_tags)
            
            prompt = f"""è¯·ä»ä»¥ä¸‹å€™é€‰æ ‡ç­¾ä¸­é€‰æ‹©æœ€å¤š{max_tags}ä¸ªæœ€é€‚åˆçš„æ ‡ç­¾æ¥æ ‡è®°ä¸‹é¢çš„æ–‡æ¡£ã€‚

**å€™é€‰æ ‡ç­¾åˆ—è¡¨ï¼š**
{candidate_tags_text}

**æ–‡æ¡£ä¿¡æ¯ï¼š**
{analysis_content}

**è¦æ±‚ï¼š**
1. åªèƒ½ä»ä¸Šè¿°å€™é€‰æ ‡ç­¾åˆ—è¡¨ä¸­é€‰æ‹©ï¼Œä¸è¦åˆ›é€ æ–°æ ‡ç­¾
2. é€‰æ‹©æœ€ç›¸å…³çš„{max_tags}ä¸ªæ ‡ç­¾
3. æ ‡ç­¾è¦å‡†ç¡®åæ˜ æ–‡æ¡£çš„ä¸»è¦å†…å®¹å’Œç‰¹å¾
4. æ¯è¡Œè¿”å›ä¸€ä¸ªæ ‡ç­¾åç§°

**è¿”å›æ ¼å¼ï¼š**
è¯·åªè¿”å›é€‰ä¸­çš„æ ‡ç­¾ï¼Œæ¯è¡Œä¸€ä¸ªï¼š"""
            
            response = self.llm.invoke(prompt)
            tags_text = response.content.strip()
            
            # è§£ææ ‡ç­¾å¹¶éªŒè¯
            suggested_tags = [tag.strip() for tag in tags_text.split('\n') if tag.strip()]
            
            # è¿‡æ»¤ï¼šåªä¿ç•™åœ¨å€™é€‰æ ‡ç­¾ä¸­çš„æ ‡ç­¾
            valid_tags = []
            for tag in suggested_tags[:max_tags]:
                if tag in candidate_tags:
                    valid_tags.append(tag)
                else:
                    logger.warning(f"LLMè¿”å›äº†ä¸åœ¨å€™é€‰åˆ—è¡¨ä¸­çš„æ ‡ç­¾: {tag}")
            
            # å¦‚æœæœ‰æ•ˆæ ‡ç­¾å¤ªå°‘ï¼Œä»å€™é€‰æ ‡ç­¾ä¸­è¡¥å……ä¸€äº›é€šç”¨æ ‡ç­¾
            if len(valid_tags) < max_tags and len(valid_tags) < 3:
                # æ·»åŠ ä¸€äº›é€šç”¨çš„åå¤‡æ ‡ç­¾
                fallback_tags = ["ç¬”è®°", "æ–‡æ¡£", "é‡ç‚¹"]
                for fallback in fallback_tags:
                    if fallback in candidate_tags and fallback not in valid_tags and len(valid_tags) < max_tags:
                        valid_tags.append(fallback)
            
            logger.info(f"æ ‡ç­¾å»ºè®®ç”ŸæˆæˆåŠŸ: {valid_tags} (ä» {len(candidate_tags)} ä¸ªå€™é€‰æ ‡ç­¾ä¸­é€‰æ‹©)")
            return valid_tags
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ ‡ç­¾å»ºè®®å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return []
    
    def _prepare_content_for_tagging(self, title: str, content: str) -> str:
        """ä¸ºæ ‡ç­¾ç”Ÿæˆå‡†å¤‡åˆ†æå†…å®¹"""
        if settings.enable_hierarchical_chunking:
            # å¤šå±‚æ¬¡æ¨¡å¼ï¼šæå–å…³é”®ä¿¡æ¯
            summary = self._generate_file_summary_for_linking(content, title)
            
            # æå–å¯èƒ½çš„ç« èŠ‚æ ‡é¢˜
            from .hierarchical_splitter import HierarchicalTextSplitter
            splitter = HierarchicalTextSplitter()
            structure = splitter._extract_document_structure(content)
            
            sections = []
            for item in structure[:5]:  # æœ€å¤š5ä¸ªç« èŠ‚
                sections.append(item.get('heading', ''))
            
            analysis_parts = [
                f"æ ‡é¢˜ï¼š{title}",
                f"æ–‡æ¡£æ‘˜è¦ï¼š{summary[:500]}",
            ]
            
            if sections:
                analysis_parts.append(f"ä¸»è¦ç« èŠ‚ï¼š{', '.join(sections)}")
            
            analysis_parts.append(f"å†…å®¹ç‰‡æ®µï¼š{content[:800]}")
            
            return "\n\n".join(analysis_parts)
        else:
            # ä¼ ç»Ÿæ¨¡å¼
            return f"æ ‡é¢˜ï¼š{title}\nå†…å®¹ï¼š{content[:1000]}"

    def analyze_content(self, content: str) -> Dict[str, Any]:
        """å†…å®¹åˆ†æ"""
        if not self.llm:
            logger.warning("LLMä¸å¯ç”¨ï¼Œæ— æ³•åˆ†æå†…å®¹")
            return {}
        
        try:
            prompt = f"""è¯·åˆ†æä»¥ä¸‹å†…å®¹ï¼Œæä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š
1. ä¸»è¦è¯é¢˜
2. å†…å®¹ç±»å‹ï¼ˆæŠ€æœ¯æ–‡æ¡£ã€ç¬”è®°ã€æ€»ç»“ç­‰ï¼‰
3. é‡è¦æ€§è¯„åˆ†ï¼ˆ1-10ï¼‰
4. å»ºè®®çš„å¤„ç†æ–¹å¼

å†…å®¹ï¼š
{content[:1500]}

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœã€‚"""
            
            response = self.llm.invoke(prompt)
            # è¿™é‡Œåº”è¯¥è§£æJSONå“åº”ï¼Œä¸ºç®€åŒ–ç›´æ¥è¿”å›æ–‡æœ¬
            
            analysis = {
                "raw_response": response.content,
                "analyzed": True
            }
            
            logger.info("å†…å®¹åˆ†æå®Œæˆ")
            return analysis
            
        except Exception as e:
            logger.error(f"å†…å®¹åˆ†æå¤±è´¥: {e}")
            return {}

    def generate_related_questions(self, content: str, num_questions: int = 3) -> List[str]:
        """ç”Ÿæˆç›¸å…³é—®é¢˜"""
        if not self.llm:
            logger.warning("LLMä¸å¯ç”¨ï¼Œæ— æ³•ç”Ÿæˆç›¸å…³é—®é¢˜")
            return []
        
        try:
            prompt = f"""åŸºäºä»¥ä¸‹å†…å®¹ï¼Œç”Ÿæˆ{num_questions}ä¸ªç›¸å…³çš„é—®é¢˜ï¼Œè¿™äº›é—®é¢˜åº”è¯¥èƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·æ›´æ·±å…¥åœ°ç†è§£å†…å®¹ï¼š

å†…å®¹ï¼š
{content[:1500]}

è¯·åªè¿”å›é—®é¢˜åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªé—®é¢˜ï¼š"""
            
            response = self.llm.invoke(prompt)
            questions_text = response.content.strip()
            
            # è§£æé—®é¢˜
            questions = [q.strip() for q in questions_text.split('\n') if q.strip()]
            questions = questions[:num_questions]  # é™åˆ¶æ•°é‡
            
            logger.info(f"ç›¸å…³é—®é¢˜ç”ŸæˆæˆåŠŸ: {questions}")
            return questions
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆç›¸å…³é—®é¢˜å¤±è´¥: {e}")
            return []

    def discover_smart_links(self, file_id: int, content: str, title: str) -> List[Dict[str, Any]]:
        """æ™ºèƒ½é“¾æ¥å‘ç° - æ”¯æŒå¤šå±‚æ¬¡é“¾æ¥å‘ç°"""
        if not self.llm:
            logger.warning("LLMä¸å¯ç”¨ï¼Œæ— æ³•å‘ç°æ™ºèƒ½é“¾æ¥")
            return []
        
        try:
            logger.info(f"å¼€å§‹æ™ºèƒ½é“¾æ¥å‘ç°: {title}")
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨å¤šå±‚æ¬¡æ¨¡å¼
            if settings.enable_hierarchical_chunking:
                return self._hierarchical_smart_links(file_id, content, title)
            else:
                return self._traditional_smart_links(file_id, content, title)
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½é“¾æ¥å‘ç°å¤±è´¥: {e}")
            return []
    
    def _traditional_smart_links(self, file_id: int, content: str, title: str) -> List[Dict[str, Any]]:
        """ä¼ ç»Ÿæ™ºèƒ½é“¾æ¥å‘ç°ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰"""
        try:
            # å…ˆé€šè¿‡è¯­ä¹‰æœç´¢æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ - æ™ºèƒ½é“¾æ¥ä½¿ç”¨æ›´é«˜çš„é˜ˆå€¼ç¡®ä¿é“¾æ¥è´¨é‡
            link_threshold = max(settings.semantic_search_threshold + 0.2, 0.6)  # è‡³å°‘0.6ï¼Œç¡®ä¿é“¾æ¥è´¨é‡
            related_results = self.semantic_search(content[:500], limit=5, similarity_threshold=link_threshold)
            
            if not related_results:
                logger.info("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œæ— æ³•ç”Ÿæˆæ™ºèƒ½é“¾æ¥")
                return []
            
            # æ„å»ºç›¸å…³æ–‡æ¡£ä¿¡æ¯
            files_text = ""
            for result in related_results:
                if result['file_id'] != file_id:  # æ’é™¤è‡ªå·±
                    files_text += f"æ–‡ä»¶ID: {result['file_id']}, æ ‡é¢˜: {result['title']}, è·¯å¾„: {result['file_path']}\n"
            
            if not files_text:
                logger.info("æ²¡æœ‰å…¶ä»–ç›¸å…³æ–‡æ¡£ï¼Œæ— æ³•ç”Ÿæˆæ™ºèƒ½é“¾æ¥")
                return []
            
            return self._generate_links_with_llm(file_id, content, title, files_text, related_results)
            
        except Exception as e:
            logger.error(f"ä¼ ç»Ÿæ™ºèƒ½é“¾æ¥å‘ç°å¤±è´¥: {e}")
            return []
    
    def _hierarchical_smart_links(self, file_id: int, content: str, title: str) -> List[Dict[str, Any]]:
        """å¤šå±‚æ¬¡æ™ºèƒ½é“¾æ¥å‘ç°"""
        try:
            logger.info(f"å¼€å§‹å¤šå±‚æ¬¡é“¾æ¥å‘ç°: {title}")
            
            # Step 1: ç”Ÿæˆå½“å‰æ–‡ä»¶çš„æ‘˜è¦ç”¨äºæ¯”è¾ƒ
            current_summary = self._generate_file_summary_for_linking(content, title)
            
            # Step 2: ä»æ‘˜è¦å±‚æœç´¢ç›¸å…³æ–‡ä»¶ï¼ˆæ–‡ä»¶çº§åˆ«çš„å…³è”ï¼‰
            summary_results = self._search_by_chunk_type(current_summary, "summary", 10, 0.8)
            
            # Step 3: ä»å¤§çº²å±‚æœç´¢ç›¸å…³ç« èŠ‚ï¼ˆç« èŠ‚çº§åˆ«çš„å…³è”ï¼‰
            outline_results = self._search_by_chunk_type(content[:800], "outline", 8, 0.7)
            
            # Step 4: æ™ºèƒ½é“¾æ¥åˆ†æ
            candidate_files = {}
            
            # å¤„ç†æ–‡ä»¶çº§åˆ«çš„å…³è”ï¼ˆæ‘˜è¦å±‚åŒ¹é…ï¼‰
            for result in summary_results:
                if result['file_id'] != file_id:
                    candidate_files[result['file_id']] = {
                        'file_id': result['file_id'],
                        'title': result['title'],
                        'file_path': result['file_path'],
                        'link_level': 'file',  # æ–‡ä»¶çº§åˆ«å…³è”
                        'similarity': result['similarity'],
                        'match_type': 'summary',
                        'match_content': result['chunk_text']
                    }
            
            # å¤„ç†ç« èŠ‚çº§åˆ«çš„å…³è”ï¼ˆå¤§çº²å±‚åŒ¹é…ï¼‰
            for result in outline_results:
                if result['file_id'] != file_id:
                    file_id_key = result['file_id']
                    if file_id_key in candidate_files:
                        # å¦‚æœå·²ç»æœ‰æ–‡ä»¶çº§åˆ«çš„å…³è”ï¼Œå‡çº§ä¸ºç« èŠ‚çº§åˆ«
                        candidate_files[file_id_key]['link_level'] = 'section'
                        candidate_files[file_id_key]['section_info'] = {
                            'section_path': result.get('section_path'),
                            'parent_heading': result.get('parent_heading'),
                            'section_similarity': result['similarity']
                        }
                    else:
                        # æ–°çš„ç« èŠ‚çº§åˆ«å…³è”
                        candidate_files[file_id_key] = {
                            'file_id': result['file_id'],
                            'title': result['title'],
                            'file_path': result['file_path'],
                            'link_level': 'section',
                            'similarity': result['similarity'],
                            'match_type': 'outline',
                            'match_content': result['chunk_text'],
                            'section_info': {
                                'section_path': result.get('section_path'),
                                'parent_heading': result.get('parent_heading'),
                                'section_similarity': result['similarity']
                            }
                        }
            
            if not candidate_files:
                logger.info("æœªæ‰¾åˆ°å€™é€‰å…³è”æ–‡ä»¶")
                return []
            
            # Step 5: æ„å»ºæ–‡ä»¶ä¿¡æ¯ç”¨äºLLMåˆ†æ
            files_info = []
            for file_info in candidate_files.values():
                if file_info['link_level'] == 'file':
                    files_info.append(f"æ–‡ä»¶ID: {file_info['file_id']}, æ ‡é¢˜: {file_info['title']}, è·¯å¾„: {file_info['file_path']}, å…³è”çº§åˆ«: æ–‡ä»¶çº§(æ•´ä½“ç›¸å…³), ç›¸ä¼¼åº¦: {file_info['similarity']:.2f}")
                elif file_info['link_level'] == 'section':
                    section_path = file_info.get('section_info', {}).get('section_path', 'æœªçŸ¥ç« èŠ‚')
                    files_info.append(f"æ–‡ä»¶ID: {file_info['file_id']}, æ ‡é¢˜: {file_info['title']}, è·¯å¾„: {file_info['file_path']}, å…³è”çº§åˆ«: ç« èŠ‚çº§({section_path}), ç›¸ä¼¼åº¦: {file_info['similarity']:.2f}")
            
            files_text = "\n".join(files_info)
            
            # Step 6: ä½¿ç”¨LLMç”Ÿæˆæ™ºèƒ½é“¾æ¥
            smart_links = self._generate_enhanced_links_with_llm(file_id, content, title, files_text, list(candidate_files.values()))
            
            logger.info(f"å¤šå±‚æ¬¡é“¾æ¥å‘ç°å®Œæˆ: æ‰¾åˆ° {len(smart_links)} ä¸ªæ™ºèƒ½é“¾æ¥")
            return smart_links
            
        except Exception as e:
            logger.error(f"å¤šå±‚æ¬¡æ™ºèƒ½é“¾æ¥å‘ç°å¤±è´¥: {e}")
            # é™çº§åˆ°ä¼ ç»Ÿæ–¹æ³•
            return self._traditional_smart_links(file_id, content, title)
    
    def _generate_file_summary_for_linking(self, content: str, title: str) -> str:
        """ä¸ºé“¾æ¥å‘ç°ç”Ÿæˆæ–‡ä»¶æ‘˜è¦"""
        # ç”Ÿæˆç®€æ´çš„æ–‡ä»¶æ‘˜è¦ç”¨äºæ–‡ä»¶çº§åˆ«çš„å…³è”åˆ¤æ–­
        summary_parts = [f"æ ‡é¢˜: {title}"]
        
        # æå–å‰å‡ æ®µé‡è¦å†…å®¹
        lines = content.split('\n')
        important_lines = []
        char_count = 0
        max_chars = 800
        
        for line in lines:
            line = line.strip()
            if line and char_count < max_chars:
                important_lines.append(line)
                char_count += len(line)
            if char_count >= max_chars:
                break
        
        summary_parts.extend(important_lines)
        return '\n'.join(summary_parts)
    
    def _generate_links_with_llm(self, file_id: int, content: str, title: str, files_text: str, related_results: List[Dict]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨LLMç”Ÿæˆä¼ ç»Ÿæ™ºèƒ½é“¾æ¥"""
        try:
            prompt = f"""å½“å‰æ–‡æ¡£ï¼š
æ ‡é¢˜ï¼š{title}
å†…å®¹ï¼š{content[:500]}

ç›¸å…³æ–‡æ¡£ï¼š
{files_text}

è¯·åˆ†æå½“å‰æ–‡æ¡£ä¸è¿™äº›ç›¸å…³æ–‡æ¡£ä¹‹é—´çš„å…³ç³»ç±»å‹ï¼Œå¹¶ä¸ºæ¯ä¸ªå»ºè®®çš„é“¾æ¥æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š
1. é“¾æ¥ç±»å‹ï¼ˆreference/related/follow_up/prerequisite/example/contradictionï¼‰
2. é“¾æ¥ç†ç”±ï¼ˆç®€çŸ­è¯´æ˜ä¸ºä»€ä¹ˆè¦å»ºç«‹è¿™ä¸ªé“¾æ¥ï¼‰
3. å»ºè®®çš„é“¾æ¥æ–‡æœ¬

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[
    {{
        "target_file_id": æ–‡ä»¶ID,
        "link_type": "é“¾æ¥ç±»å‹",
        "reason": "é“¾æ¥ç†ç”±",
        "suggested_text": "å»ºè®®çš„é“¾æ¥æ–‡æœ¬"
    }}
]

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼š"""
            
            response = self.llm.invoke(prompt)
            result_text = response.content.strip()
            
            # å°è¯•è§£æJSON
            import json
            try:
                smart_links = json.loads(result_text)
                logger.info(f"æ™ºèƒ½é“¾æ¥ç”ŸæˆæˆåŠŸ: {len(smart_links)} ä¸ªé“¾æ¥")
                return smart_links
            except json.JSONDecodeError as e:
                logger.error(f"è§£ææ™ºèƒ½é“¾æ¥JSONå¤±è´¥: {e}")
                return []
                
        except Exception as e:
            logger.error(f"LLMç”Ÿæˆé“¾æ¥å¤±è´¥: {e}")
            return []
    
    def _generate_enhanced_links_with_llm(self, file_id: int, content: str, title: str, files_text: str, candidate_files: List[Dict]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨LLMç”Ÿæˆå¢å¼ºçš„å¤šå±‚æ¬¡æ™ºèƒ½é“¾æ¥"""
        try:
            prompt = f"""å½“å‰æ–‡æ¡£ï¼š
æ ‡é¢˜ï¼š{title}
å†…å®¹ï¼š{content[:600]}

å€™é€‰å…³è”æ–‡æ¡£ï¼ˆåŒ…å«å…³è”çº§åˆ«å’Œç›¸ä¼¼åº¦ï¼‰ï¼š
{files_text}

è¯·åŸºäºå¤šå±‚æ¬¡å…³è”åˆ†æï¼Œä¸ºæ¯ä¸ªå€™é€‰æ–‡æ¡£è¯„ä¼°æ˜¯å¦åº”è¯¥å»ºç«‹é“¾æ¥ï¼Œä»¥åŠé“¾æ¥çš„ç±»å‹å’Œå¼ºåº¦ã€‚

å…³è”çº§åˆ«è¯´æ˜ï¼š
- æ–‡ä»¶çº§ï¼šæ•´ä¸ªæ–‡æ¡£åœ¨ä¸»é¢˜æˆ–å†…å®¹ä¸Šç›¸å…³
- ç« èŠ‚çº§ï¼šç‰¹å®šç« èŠ‚æˆ–ä¸»é¢˜ç›¸å…³

è¯·ä¸ºæ¯ä¸ªå»ºè®®çš„é“¾æ¥æä¾›ï¼š
1. é“¾æ¥ç±»å‹ï¼ˆreference/related/follow_up/prerequisite/example/contradiction/complementï¼‰
2. é“¾æ¥å¼ºåº¦ï¼ˆstrong/medium/weakï¼‰
3. é“¾æ¥ç†ç”±ï¼ˆè¯¦ç»†è¯´æ˜å…³è”åŸå› å’Œå…³è”çº§åˆ«ï¼‰
4. å»ºè®®çš„é“¾æ¥æ–‡æœ¬
5. æ˜¯å¦æ¨èå»ºç«‹é“¾æ¥ï¼ˆtrue/falseï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[
    {{
        "target_file_id": æ–‡ä»¶ID,
        "link_type": "é“¾æ¥ç±»å‹",
        "link_strength": "é“¾æ¥å¼ºåº¦",
        "reason": "é“¾æ¥ç†ç”±",
        "suggested_text": "å»ºè®®çš„é“¾æ¥æ–‡æœ¬",
        "recommended": trueæˆ–false
    }}
]

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼š"""
            
            response = self.llm.invoke(prompt)
            result_text = response.content.strip()
            
            # å°è¯•è§£æJSON
            import json
            try:
                smart_links = json.loads(result_text)
                # åªè¿”å›æ¨èçš„é“¾æ¥
                recommended_links = [link for link in smart_links if link.get('recommended', False)]
                logger.info(f"å¢å¼ºæ™ºèƒ½é“¾æ¥ç”ŸæˆæˆåŠŸ: {len(recommended_links)} ä¸ªæ¨èé“¾æ¥ï¼ˆä» {len(smart_links)} ä¸ªå€™é€‰ä¸­ç­›é€‰ï¼‰")
                return recommended_links
            except json.JSONDecodeError as e:
                logger.error(f"è§£æå¢å¼ºæ™ºèƒ½é“¾æ¥JSONå¤±è´¥: {e}")
                return []
                
        except Exception as e:
            logger.error(f"LLMç”Ÿæˆå¢å¼ºé“¾æ¥å¤±è´¥: {e}")
            return []

    def _get_cached_query_embedding(self, query: str) -> List[float]:
        """è·å–ç¼“å­˜çš„æŸ¥è¯¢å‘é‡"""
        with self._cache_lock:
            query_hash = hashlib.md5(query.encode()).hexdigest()
            
            if query_hash in self._query_cache:
                logger.info(f"ä½¿ç”¨ç¼“å­˜çš„æŸ¥è¯¢å‘é‡: {query[:50]}...")
                return self._query_cache[query_hash]
            
            # ç”Ÿæˆæ–°çš„æŸ¥è¯¢å‘é‡
            embedding = self.embeddings.embed_query(query)
            
            # ç¼“å­˜ç®¡ç†ï¼šå¦‚æœç¼“å­˜è¿‡å¤§ï¼Œæ¸…ç†æœ€æ—§çš„æ¡ç›®
            if len(self._query_cache) >= self._max_cache_size:
                # ç®€å•çš„FIFOæ¸…ç†ç­–ç•¥
                oldest_key = next(iter(self._query_cache))
                del self._query_cache[oldest_key]
                logger.info(f"æŸ¥è¯¢å‘é‡ç¼“å­˜å·²æ»¡ï¼Œæ¸…ç†æœ€æ—§æ¡ç›®")
            
            self._query_cache[query_hash] = embedding
            logger.info(f"ç”Ÿæˆå¹¶ç¼“å­˜æ–°çš„æŸ¥è¯¢å‘é‡: {query[:50]}...")
            return embedding

    def chat_with_context(self, question: str, max_context_length: int = 3000, search_limit: int = 5, enable_tools: bool = True) -> Dict[str, Any]:
        """åŸºäºä¸Šä¸‹æ–‡çš„æ™ºèƒ½é—®ç­” - RAGå®ç°ï¼Œæ”¯æŒMCPå·¥å…·è°ƒç”¨å’Œå¤šå±‚æ¬¡æ£€ç´¢"""
        if not self.is_available():
            logger.warning("AIæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæ™ºèƒ½é—®ç­”")
            return {
                "answer": "æŠ±æ­‰ï¼ŒAIæœåŠ¡å½“å‰ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚",
                "related_documents": [],
                "search_query": question,
                "error": "AIæœåŠ¡ä¸å¯ç”¨"
            }
        
        try:
            start_time = time.time()
            logger.info(f"å¼€å§‹RAGé—®ç­”ï¼Œé—®é¢˜: {question}, å·¥å…·è°ƒç”¨: {enable_tools}")
            
            # 1. æ™ºèƒ½ä¸Šä¸‹æ–‡æ£€ç´¢ï¼ˆæ”¯æŒå¤šå±‚æ¬¡ï¼‰
            if settings.enable_hierarchical_chunking:
                search_results = self._hierarchical_context_search(question, search_limit)
            else:
                search_results = self.semantic_search(
                    query=question,
                    limit=search_limit,
                    similarity_threshold=settings.semantic_search_threshold
                )
            
            logger.info(f"æœç´¢åˆ° {len(search_results)} ä¸ªç›¸å…³æ–‡æ¡£")
            
            # 2. æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            related_docs = []
            current_length = 0
            
            for result in search_results:
                chunk_text = result.get('chunk_text', '')
                file_path = result.get('file_path', '')
                title = result.get('title', '')
                similarity = result.get('similarity', 0)
                chunk_type = result.get('chunk_type', 'content')
                section_path = result.get('section_path', '')
                
                # æ ¹æ®åˆ†å—ç±»å‹è°ƒæ•´ä¸Šä¸‹æ–‡æ ¼å¼
                if chunk_type == 'summary':
                    context_part = f"ã€æ–‡æ¡£æ‘˜è¦ã€‘{title}\nè·¯å¾„ï¼š{file_path}\næ‘˜è¦ï¼š{chunk_text}\n"
                elif chunk_type == 'outline':
                    context_part = f"ã€ç« èŠ‚å¤§çº²ã€‘{title} - {section_path}\nè·¯å¾„ï¼š{file_path}\nå¤§çº²ï¼š{chunk_text}\n"
                else:
                    context_part = f"ã€å†…å®¹ç‰‡æ®µã€‘{title}\nè·¯å¾„ï¼š{file_path}\nå†…å®¹ï¼š{chunk_text}\n"
                
                # æ£€æŸ¥é•¿åº¦é™åˆ¶
                if current_length + len(context_part) > max_context_length:
                    logger.info(f"ä¸Šä¸‹æ–‡é•¿åº¦è¾¾åˆ°é™åˆ¶ {max_context_length}ï¼Œåœæ­¢æ·»åŠ æ›´å¤šæ–‡æ¡£")
                    break
                
                context_parts.append(context_part)
                current_length += len(context_part)
                
                # æ·»åŠ åˆ°ç›¸å…³æ–‡æ¡£åˆ—è¡¨ï¼ˆç”¨äºå‰ç«¯æ˜¾ç¤ºï¼‰
                related_docs.append({
                    'file_id': result.get('file_id'),
                    'file_path': file_path,
                    'title': title,
                    'similarity': similarity,
                    'chunk_text': chunk_text[:200] + '...' if len(chunk_text) > 200 else chunk_text,
                    'chunk_type': chunk_type,
                    'section_path': section_path
                })
            
            context = "\n\n".join(context_parts)
            
            # 3. è·å–å¯ç”¨çš„MCPå·¥å…·
            tools = []
            tool_calls_history = []
            if enable_tools:
                try:
                    tools = self.mcp_service.get_tools_for_llm()
                    logger.info(f"è·å–åˆ° {len(tools)} ä¸ªå¯ç”¨å·¥å…·")
                except Exception as e:
                    logger.warning(f"è·å–MCPå·¥å…·å¤±è´¥: {e}")
            
            # 4. æ„å»ºæç¤ºè¯
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œä¸“é—¨å›ç­”åŸºäºç”¨æˆ·ç¬”è®°å†…å®¹çš„é—®é¢˜ã€‚è¯·æ ¹æ®ä»¥ä¸‹ç›¸å…³æ–‡æ¡£å†…å®¹æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{question}

ç›¸å…³æ–‡æ¡£å†…å®¹ï¼š
{context}

è¯·æ ¹æ®ä¸Šè¿°æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¦æ±‚ï¼š
1. å›ç­”è¦å‡†ç¡®ã€æœ‰ç”¨ï¼ŒåŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹
2. å¦‚æœæ–‡æ¡£å†…å®¹ä¸è¶³ä»¥å®Œå…¨å›ç­”é—®é¢˜ï¼Œè¯·è¯´æ˜å¹¶æä¾›ä½ èƒ½ç¡®å®šçš„éƒ¨åˆ†
3. å›ç­”è¦ç®€æ´æ˜äº†ï¼Œé‡ç‚¹çªå‡º
4. å¦‚æœå¯èƒ½ï¼Œè¯·å¼•ç”¨å…·ä½“çš„æ–‡æ¡£æ¥æº
5. å¦‚æœéœ€è¦é¢å¤–ä¿¡æ¯æˆ–æ‰§è¡Œç‰¹å®šä»»åŠ¡ï¼Œå¯ä»¥ä½¿ç”¨å¯ç”¨çš„å·¥å…·

å›ç­”ï¼š"""

            # 5. è°ƒç”¨LLMç”Ÿæˆå›ç­”ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰
            logger.info(f"è°ƒç”¨LLMç”Ÿæˆå›ç­”ï¼Œä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
            
            if tools:
                # ä½¿ç”¨å·¥å…·è°ƒç”¨
                llm_with_tools = self.llm.bind_tools(tools)
                response = llm_with_tools.invoke(prompt)
                
                # å¤„ç†å·¥å…·è°ƒç”¨
                if response.tool_calls:
                    logger.info(f"LLMå†³å®šè°ƒç”¨ {len(response.tool_calls)} ä¸ªå·¥å…·")
                    
                    # æ‰§è¡Œå·¥å…·è°ƒç”¨
                    tool_results = []
                    for tool_call in response.tool_calls:
                        try:
                            tool_name = tool_call["name"]
                            tool_args = tool_call["args"]
                            
                            logger.info(f"è°ƒç”¨å·¥å…·: {tool_name}, å‚æ•°: {tool_args}")
                            
                            # æ‰§è¡Œå·¥å…·è°ƒç”¨
                            import asyncio
                            
                            request = MCPToolCallRequest(
                                tool_name=tool_name,
                                arguments=tool_args,
                                session_id=f"chat_{int(time.time())}"
                            )
                            result = asyncio.run(self.mcp_service.call_tool(request))
                            
                            tool_results.append({
                                "tool_name": tool_name,
                                "arguments": tool_args,
                                "result": result.result if result.success else result.error,
                                "success": result.success,
                                "execution_time": result.execution_time_ms
                            })
                            
                            tool_calls_history.append({
                                "tool_name": tool_name,
                                "arguments": tool_args,
                                "result": result.result if result.success else result.error,
                                "success": result.success,
                                "execution_time": result.execution_time_ms
                            })
                            
                        except Exception as e:
                            logger.error(f"å·¥å…·è°ƒç”¨å¤±è´¥: {e}")
                            tool_results.append({
                                "tool_name": tool_call.get("name", "unknown"),
                                "arguments": tool_call.get("args", {}),
                                "result": f"å·¥å…·è°ƒç”¨å¤±è´¥: {str(e)}",
                                "success": False,
                                "execution_time": 0
                            })
                    
                    # å°†å·¥å…·ç»“æœæ•´åˆåˆ°æœ€ç»ˆå›ç­”ä¸­
                    if tool_results:
                        tool_summary = "\n\nå·¥å…·è°ƒç”¨ç»“æœï¼š\n"
                        for i, result in enumerate(tool_results, 1):
                            status = "æˆåŠŸ" if result["success"] else "å¤±è´¥"
                            tool_summary += f"{i}. {result['tool_name']} ({status}): {result['result']}\n"
                        
                        # é‡æ–°è°ƒç”¨LLMï¼Œæ•´åˆå·¥å…·ç»“æœ
                        final_prompt = f"{prompt}\n\n{tool_summary}\n\nè¯·æ ¹æ®ä¸Šè¿°ä¿¡æ¯å’Œå·¥å…·è°ƒç”¨ç»“æœï¼Œæä¾›æœ€ç»ˆçš„å›ç­”ï¼š"
                        final_response = self.llm.invoke(final_prompt)
                        answer = final_response.content.strip()
                    else:
                        answer = response.content.strip()
                else:
                    answer = response.content.strip()
            else:
                # ä¸ä½¿ç”¨å·¥å…·è°ƒç”¨
                response = self.llm.invoke(prompt)
                answer = response.content.strip()
            
            total_time = time.time() - start_time
            logger.info(f"RAGé—®ç­”å®Œæˆï¼Œè€—æ—¶: {total_time:.3f}ç§’ï¼Œå›ç­”é•¿åº¦: {len(answer)} å­—ç¬¦")
            
            result = {
                "answer": answer,
                "related_documents": related_docs,
                "search_query": question,
                "context_length": len(context),
                "processing_time": round(total_time, 3),
                "tools_used": len(tools) if tools else 0
            }
            
            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨å†å²ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
            if tool_calls_history:
                result["tool_calls"] = tool_calls_history
            
            return result
            
        except Exception as e:
            logger.error(f"RAGé—®ç­”å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            
            return {
                "answer": f"æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶å‡ºç°äº†é”™è¯¯ï¼š{str(e)}",
                "related_documents": [],
                "search_query": question,
                "error": str(e)
            }

    async def streaming_chat_with_context(self, question: str, max_context_length: int = 3000, search_limit: int = 5, enable_tools: bool = True):
        """åŸºäºä¸Šä¸‹æ–‡çš„æµå¼æ™ºèƒ½é—®ç­” - RAGå®ç°ï¼Œæ”¯æŒMCPå·¥å…·è°ƒç”¨"""
        if not self.is_available() or not self.streaming_llm:
            logger.warning("AIæœåŠ¡æˆ–æµå¼LLMä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæµå¼æ™ºèƒ½é—®ç­”")
            yield {
                "error": "AIæœåŠ¡ä¸å¯ç”¨",
                "related_documents": [],
                "search_query": question
            }
            return
        
        try:
            start_time = time.time()
            logger.info(f"å¼€å§‹æµå¼RAGé—®ç­”ï¼Œé—®é¢˜: {question}, å·¥å…·è°ƒç”¨: {enable_tools}")
            
            # 1. è¯­ä¹‰æœç´¢ç›¸å…³æ–‡æ¡£
            search_results = self.semantic_search(
                query=question,
                limit=search_limit,
                similarity_threshold=settings.semantic_search_threshold
            )
            
            logger.info(f"æœç´¢åˆ° {len(search_results)} ä¸ªç›¸å…³æ–‡æ¡£")
            
            # 2. æ„å»ºä¸Šä¸‹æ–‡
            context_parts = []
            related_docs = []
            current_length = 0
            
            for result in search_results:
                chunk_text = result.get('chunk_text', '')
                file_path = result.get('file_path', '')
                title = result.get('title', '')
                similarity = result.get('similarity', 0)
                
                # å‡†å¤‡ä¸Šä¸‹æ–‡ç‰‡æ®µ
                context_part = f"æ–‡æ¡£ï¼š{title}\nè·¯å¾„ï¼š{file_path}\nå†…å®¹ï¼š{chunk_text}\n"
                
                # æ£€æŸ¥é•¿åº¦é™åˆ¶
                if current_length + len(context_part) > max_context_length:
                    logger.info(f"ä¸Šä¸‹æ–‡é•¿åº¦è¾¾åˆ°é™åˆ¶ {max_context_length}ï¼Œåœæ­¢æ·»åŠ æ›´å¤šæ–‡æ¡£")
                    break
                
                context_parts.append(context_part)
                current_length += len(context_part)
                
                # æ·»åŠ åˆ°ç›¸å…³æ–‡æ¡£åˆ—è¡¨ï¼ˆç”¨äºå‰ç«¯æ˜¾ç¤ºï¼‰
                related_docs.append({
                    'file_id': result.get('file_id'),
                    'file_path': file_path,
                    'title': title,
                    'similarity': similarity,
                    'chunk_text': chunk_text[:200] + '...' if len(chunk_text) > 200 else chunk_text
                })
            
            context = "\n\n".join(context_parts)
            
            # 3. è·å–å¯ç”¨çš„MCPå·¥å…·
            tools = []
            tool_calls_history = []
            if enable_tools:
                try:
                    tools = self.mcp_service.get_tools_for_llm()
                    logger.info(f"è·å–åˆ° {len(tools)} ä¸ªå¯ç”¨å·¥å…·")
                except Exception as e:
                    logger.warning(f"è·å–MCPå·¥å…·å¤±è´¥: {e}")
            
            # 4. æ„å»ºæç¤ºè¯
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œä¸“é—¨å›ç­”åŸºäºç”¨æˆ·ç¬”è®°å†…å®¹çš„é—®é¢˜ã€‚è¯·æ ¹æ®ä»¥ä¸‹ç›¸å…³æ–‡æ¡£å†…å®¹æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{question}

ç›¸å…³æ–‡æ¡£å†…å®¹ï¼š
{context}

è¯·æ ¹æ®ä¸Šè¿°æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¦æ±‚ï¼š
1. å›ç­”è¦å‡†ç¡®ã€æœ‰ç”¨ï¼ŒåŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹
2. å¦‚æœæ–‡æ¡£å†…å®¹ä¸è¶³ä»¥å®Œå…¨å›ç­”é—®é¢˜ï¼Œè¯·è¯´æ˜å¹¶æä¾›ä½ èƒ½ç¡®å®šçš„éƒ¨åˆ†
3. å›ç­”è¦ç®€æ´æ˜äº†ï¼Œé‡ç‚¹çªå‡º
4. å¦‚æœå¯èƒ½ï¼Œè¯·å¼•ç”¨å…·ä½“çš„æ–‡æ¡£æ¥æº
5. å¦‚æœéœ€è¦é¢å¤–ä¿¡æ¯æˆ–æ‰§è¡Œç‰¹å®šä»»åŠ¡ï¼Œå¯ä»¥ä½¿ç”¨å¯ç”¨çš„å·¥å…·

å›ç­”ï¼š"""

            # 5. æ£€æŸ¥æ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨ï¼ˆå…ˆè¿›è¡Œéæµå¼æ£€æŸ¥ï¼‰
            if tools:
                # å…ˆç”¨éæµå¼LLMæ£€æŸ¥æ˜¯å¦éœ€è¦å·¥å…·è°ƒç”¨
                llm_with_tools = self.llm.bind_tools(tools)
                check_response = llm_with_tools.invoke(prompt)
                
                if check_response.tool_calls:
                    logger.info(f"LLMå†³å®šè°ƒç”¨ {len(check_response.tool_calls)} ä¸ªå·¥å…·")
                    
                    # å‘é€å·¥å…·è°ƒç”¨å¼€å§‹ä¿¡å·
                    yield {
                        "tool_calls_started": True,
                        "tool_count": len(check_response.tool_calls),
                        "related_documents": related_docs,
                        "search_query": question,
                        "context_length": len(context)
                    }
                    
                    # æ‰§è¡Œå·¥å…·è°ƒç”¨
                    tool_results = []
                    for i, tool_call in enumerate(check_response.tool_calls):
                        try:
                            tool_name = tool_call["name"]
                            tool_args = tool_call["args"]
                            
                            # å‘é€å·¥å…·è°ƒç”¨è¿›åº¦
                            yield {
                                "tool_call_progress": {
                                    "index": i + 1,
                                    "total": len(check_response.tool_calls),
                                    "tool_name": tool_name,
                                    "status": "executing"
                                }
                            }
                            
                            logger.info(f"è°ƒç”¨å·¥å…·: {tool_name}, å‚æ•°: {tool_args}")
                            
                            # æ‰§è¡Œå·¥å…·è°ƒç”¨
                            request = MCPToolCallRequest(
                                tool_name=tool_name,
                                arguments=tool_args,
                                session_id=f"stream_chat_{int(time.time())}"
                            )
                            result = await self.mcp_service.call_tool(request)
                            
                            tool_result = {
                                "tool_name": tool_name,
                                "arguments": tool_args,
                                "result": result.result if result.success else result.error,
                                "success": result.success,
                                "execution_time": result.execution_time_ms
                            }
                            
                            tool_results.append(tool_result)
                            tool_calls_history.append(tool_result)
                            
                            # å‘é€å·¥å…·è°ƒç”¨å®Œæˆ
                            yield {
                                "tool_call_progress": {
                                    "index": i + 1,
                                    "total": len(check_response.tool_calls),
                                    "tool_name": tool_name,
                                    "status": "completed",
                                    "result": tool_result
                                }
                            }
                            
                        except Exception as e:
                            logger.error(f"å·¥å…·è°ƒç”¨å¤±è´¥: {e}")
                            error_result = {
                                "tool_name": tool_call.get("name", "unknown"),
                                "arguments": tool_call.get("args", {}),
                                "result": f"å·¥å…·è°ƒç”¨å¤±è´¥: {str(e)}",
                                "success": False,
                                "execution_time": 0
                            }
                            tool_results.append(error_result)
                            
                            # å‘é€å·¥å…·è°ƒç”¨é”™è¯¯
                            yield {
                                "tool_call_progress": {
                                    "index": i + 1,
                                    "total": len(check_response.tool_calls),
                                    "tool_name": tool_call.get("name", "unknown"),
                                    "status": "error",
                                    "error": str(e)
                                }
                            }
                    
                    # å‘é€å·¥å…·è°ƒç”¨å®Œæˆä¿¡å·
                    yield {
                        "tool_calls_completed": True,
                        "tool_results": tool_results
                    }
                    
                    # å°†å·¥å…·ç»“æœæ•´åˆåˆ°æç¤ºè¯ä¸­
                    if tool_results:
                        tool_summary = "\n\nå·¥å…·è°ƒç”¨ç»“æœï¼š\n"
                        for i, result in enumerate(tool_results, 1):
                            status = "æˆåŠŸ" if result["success"] else "å¤±è´¥"
                            tool_summary += f"{i}. {result['tool_name']} ({status}): {result['result']}\n"
                        
                        # æ›´æ–°æç¤ºè¯
                        prompt = f"{prompt}\n\n{tool_summary}\n\nè¯·æ ¹æ®ä¸Šè¿°ä¿¡æ¯å’Œå·¥å…·è°ƒç”¨ç»“æœï¼Œæä¾›æœ€ç»ˆçš„å›ç­”ï¼š"
            
            # 6. æµå¼è°ƒç”¨LLMç”Ÿæˆå›ç­”
            logger.info(f"å¼€å§‹æµå¼è°ƒç”¨LLMï¼Œä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
            
            # ä½¿ç”¨LangChainçš„astreamæ–¹æ³•è¿›è¡ŒçœŸæ­£çš„æµå¼è¾“å‡º
            async for chunk in self.streaming_llm.astream(prompt):
                if chunk.content:  # åªæœ‰å½“chunkæœ‰å†…å®¹æ—¶æ‰yield
                    yield {
                        "chunk": chunk.content,
                        "related_documents": related_docs,
                        "search_query": question,
                        "context_length": len(context)
                    }
            
            # æµå¼ç»“æŸåå‘é€æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
            total_time = time.time() - start_time
            logger.info(f"æµå¼RAGé—®ç­”å®Œæˆï¼Œè€—æ—¶: {total_time:.3f}ç§’")
            
            final_result = {
                "finished": True,
                "processing_time": round(total_time, 3),
                "related_documents": related_docs,
                "search_query": question,
                "context_length": len(context),
                "tools_used": len(tools) if tools else 0
            }
            
            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨å†å²ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
            if tool_calls_history:
                final_result["tool_calls"] = tool_calls_history
            
            yield final_result
            
        except Exception as e:
            logger.error(f"æµå¼RAGé—®ç­”å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            
            yield {
                "error": str(e),
                "related_documents": [],
                "search_query": question
            }

    def _hierarchical_context_search(self, question: str, search_limit: int) -> List[Dict[str, Any]]:
        """å¤šå±‚æ¬¡ä¸Šä¸‹æ–‡æœç´¢ - ä¸ºRAGé—®ç­”ä¼˜åŒ–"""
        try:
            logger.info(f"å¼€å§‹å¤šå±‚æ¬¡ä¸Šä¸‹æ–‡æœç´¢: {question}")
            
            # åˆ†æé—®é¢˜ç±»å‹ï¼Œå†³å®šæœç´¢ç­–ç•¥
            question_type = self._analyze_question_type(question)
            
            context_results = []
            
            if question_type == 'overview':
                # æ¦‚è§ˆæ€§é—®é¢˜ï¼šä¼˜å…ˆæœç´¢æ‘˜è¦å±‚
                summary_results = self._search_by_chunk_type(question, "summary", search_limit//2, 0.8)
                outline_results = self._search_by_chunk_type(question, "outline", search_limit//2, 0.7)
                content_results = self._search_by_chunk_type(question, "content", search_limit//3, 0.7)
                
                # æ™ºèƒ½ä¸Šä¸‹æ–‡æ‰©å±•ï¼šä¸ºæ‘˜è¦åŒ¹é…çš„æ–‡ä»¶è·å–å…³é”®ç« èŠ‚
                for summary_result in summary_results:
                    context_results.append(summary_result)
                    # è·å–è¯¥æ–‡ä»¶çš„é‡è¦ç« èŠ‚
                    file_outlines = self._get_file_outline(summary_result['file_id'])
                    context_results.extend(file_outlines[:2])  # æ·»åŠ å‰2ä¸ªç« èŠ‚
                
                context_results.extend(outline_results)
                context_results.extend(content_results)
                
            elif question_type == 'specific':
                # å…·ä½“é—®é¢˜ï¼šä¼˜å…ˆæœç´¢å†…å®¹å±‚ï¼Œè¡¥å……ç›¸å…³å¤§çº²
                content_results = self._search_by_chunk_type(question, "content", search_limit, 0.7)
                outline_results = self._search_by_chunk_type(question, "outline", search_limit//2, 0.7)
                
                # ä¸ºå†…å®¹åŒ¹é…ç»“æœæ·»åŠ ä¸Šä¸‹æ–‡
                for content_result in content_results:
                    context_results.append(content_result)
                    
                    # å¦‚æœæœ‰ç« èŠ‚ä¿¡æ¯ï¼Œå°è¯•è·å–ç›¸é‚»å†…å®¹
                    if content_result.get('parent_heading'):
                        sibling_content = self._get_section_content(
                            content_result['file_id'], 
                            content_result['parent_heading']
                        )
                        context_results.extend(sibling_content[:1])  # æ·»åŠ 1ä¸ªç›¸é‚»å†…å®¹å—
                
                context_results.extend(outline_results)
                
            else:
                # é»˜è®¤ç­–ç•¥ï¼šå¹³è¡¡æœç´¢å„ä¸ªå±‚æ¬¡
                summary_results = self._search_by_chunk_type(question, "summary", search_limit//4, 0.8)
                outline_results = self._search_by_chunk_type(question, "outline", search_limit//3, 0.7)
                content_results = self._search_by_chunk_type(question, "content", search_limit, 0.7)
                
                context_results.extend(summary_results)
                context_results.extend(outline_results)
                context_results.extend(content_results)
            
            # å»é‡å¹¶æ’åº
            final_results = self._deduplicate_and_rank(context_results, search_limit * 2)
            
            logger.info(f"å¤šå±‚æ¬¡ä¸Šä¸‹æ–‡æœç´¢å®Œæˆ: è¿”å› {len(final_results)} ä¸ªç»“æœ")
            return final_results[:search_limit]
            
        except Exception as e:
            logger.error(f"å¤šå±‚æ¬¡ä¸Šä¸‹æ–‡æœç´¢å¤±è´¥: {e}")
            # é™çº§åˆ°ä¼ ç»Ÿæœç´¢
            return self.semantic_search(question, search_limit, settings.semantic_search_threshold)
    
    def _analyze_question_type(self, question: str) -> str:
        """åˆ†æé—®é¢˜ç±»å‹"""
        question_lower = question.lower()
        
        # æ¦‚è§ˆæ€§é—®é¢˜å…³é”®è¯
        overview_keywords = ['ä»€ä¹ˆæ˜¯', 'ä»‹ç»', 'æ¦‚è¿°', 'æ€»ç»“', 'æ•´ä½“', 'å…¨éƒ¨', 'æ‰€æœ‰', 'æ¦‚å†µ', 'æ€»ä½“']
        
        # å…·ä½“é—®é¢˜å…³é”®è¯
        specific_keywords = ['å¦‚ä½•', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å“ªé‡Œ', 'ä½•æ—¶', 'å…·ä½“', 'è¯¦ç»†', 'æ­¥éª¤', 'æ–¹æ³•']
        
        if any(keyword in question_lower for keyword in overview_keywords):
            return 'overview'
        elif any(keyword in question_lower for keyword in specific_keywords):
            return 'specific'
        else:
            return 'balanced'
