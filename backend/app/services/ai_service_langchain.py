# LangChain-Chromaç‰ˆæœ¬çš„AIService

from typing import List, Optional, Dict, Any, AsyncGenerator
import logging
from sqlalchemy.orm import Session
from langchain_openai import ChatOpenAI
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
import requests
import os
import hashlib
import threading
import time
import json
from functools import lru_cache
import asyncio
from concurrent.futures import ThreadPoolExecutor

from ..models.file import File
from ..models.embedding import Embedding
from ..models.tag import Tag
from ..dynamic_config import settings
from .mcp_service import MCPClientService
from .simple_memory_service import SimpleMemoryService
from ..schemas.mcp import MCPToolCallRequest

logger = logging.getLogger(__name__)

class OpenAICompatibleEmbeddings(Embeddings):
    """OpenAIå…¼å®¹çš„åµŒå…¥æ¨¡å‹åŒ…è£…å™¨ï¼Œç”¨äºLangChain"""
    
    def __init__(self, base_url: str, api_key: str, model: str):
        self.base_url = base_url
        self.api_key = api_key
        self.model = model
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡åµŒå…¥æ–‡æ¡£"""
        embeddings = []
        for text in texts:
            embedding = self._get_embedding(text)
            if embedding:
                embeddings.append(embedding)
            else:
                # å¦‚æœæŸä¸ªæ–‡æ¡£åµŒå…¥å¤±è´¥ï¼Œç”¨é›¶å‘é‡å ä½
                embeddings.append([0.0] * settings.embedding_dimension)
        return embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """åµŒå…¥æŸ¥è¯¢æ–‡æœ¬"""
        embedding = self._get_embedding(text)
        return embedding if embedding else [0.0] * settings.embedding_dimension
    
    def _get_embedding(self, text: str) -> List[float]:
        """ä½¿ç”¨OpenAIå…¼å®¹æ¥å£è·å–åµŒå…¥å‘é‡"""
        try:
            # ç¡®ä¿URLæ ¼å¼æ­£ç¡®ï¼Œé¿å…é‡å¤çš„/v1
            base_url = self.base_url.rstrip('/')
            if base_url.endswith('/v1'):
                url = f"{base_url}/embeddings"
            else:
                url = f"{base_url}/v1/embeddings"
            payload = {
                "model": self.model,
                "input": text,
                "encoding_format": "float"
            }
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            
            response = requests.post(url, json=payload, headers=headers, timeout=30)
            response.raise_for_status()
            
            result = response.json()
            if "data" in result and result["data"] and len(result["data"]) > 0:
                return result["data"][0]["embedding"]
            else:
                logger.error(f"åµŒå…¥å“åº”æ ¼å¼é”™è¯¯: {result}")
                return []
                
        except Exception as e:
            logger.error(f"è·å–åµŒå…¥å‘é‡å¤±è´¥: {e}")
            return []

class ChromaDBManager:
    """ChromaDBå•ä¾‹ç®¡ç†å™¨ï¼Œé¿å…å¤šå®ä¾‹å†²çª"""
    _instance = None
    _lock = threading.Lock()
    _vector_store = None
    _embeddings = None
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super(ChromaDBManager, cls).__new__(cls)
        return cls._instance
    
    def get_vector_store(self):
        """è·å–å‘é‡å­˜å‚¨å®ä¾‹"""
        if self._vector_store is None:
            with self._lock:
                if self._vector_store is None:
                    try:
                        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
                        if self._embeddings is None:
                            self._embeddings = OpenAICompatibleEmbeddings(
                                base_url=settings.get_embedding_base_url(),
                                api_key=settings.get_embedding_api_key(),
                                model=settings.embedding_model_name
                            )
                        
                        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
                        self._vector_store = Chroma(
                            collection_name="document_embeddings",
                            embedding_function=self._embeddings,
                            persist_directory=settings.chroma_db_path,
                            collection_metadata={"description": "AIç¬”è®°æœ¬æ–‡æ¡£åµŒå…¥å‘é‡"}
                        )
                        logger.info("ChromaDBå•ä¾‹åˆå§‹åŒ–æˆåŠŸ")
                        
                    except Exception as e:
                        logger.error(f"ChromaDBå•ä¾‹åˆå§‹åŒ–å¤±è´¥: {e}")
                        self._vector_store = None
        
        return self._vector_store
    
    def clear_collection(self):
        """æ¸…ç©ºChromaDB collectionä¸­çš„æ‰€æœ‰å‘é‡"""
        try:
            if self._vector_store is not None:
                # è·å–collectionä¸­çš„æ‰€æœ‰æ–‡æ¡£ID
                collection = self._vector_store._collection
                # åˆ é™¤æ‰€æœ‰æ–‡æ¡£
                all_docs = collection.get()
                if all_docs['ids']:
                    collection.delete(ids=all_docs['ids'])
                    logger.info(f"å·²æ¸…ç©ºChromaDB collectionï¼Œåˆ é™¤äº† {len(all_docs['ids'])} ä¸ªå‘é‡")
                else:
                    logger.info("ChromaDB collectionå·²ç»ä¸ºç©º")
                return True
        except Exception as e:
            logger.error(f"æ¸…ç©ºChromaDB collectionå¤±è´¥: {e}")
            return False
    
    def reset(self):
        """é‡ç½®å•ä¾‹ï¼ˆç”¨äºæµ‹è¯•æˆ–é‡æ–°åˆå§‹åŒ–ï¼‰"""
        with self._lock:
            self._vector_store = None
            self._embeddings = None

class AIService:
    """AIæœåŠ¡ç±»ï¼Œä½¿ç”¨LangChain-Chromaè¿›è¡Œå‘é‡å­˜å‚¨ - å•ä¾‹ç‰ˆæœ¬"""
    
    def __init__(self, db: Session):
        self.db = db
        self.openai_api_key = settings.openai_api_key
        self.openai_base_url = settings.openai_base_url
        
        # åˆå§‹åŒ–ç®€åŒ–çš„è®°å¿†æœåŠ¡
        self.memory_service = SimpleMemoryService()
        
        # åˆå§‹åŒ–LLM
        if self.openai_api_key:
            self.llm = ChatOpenAI(
                openai_api_key=self.openai_api_key,
                base_url=self.openai_base_url,
                model=settings.openai_model
            )
            # åˆå§‹åŒ–æµå¼LLM
            self.streaming_llm = ChatOpenAI(
                openai_api_key=self.openai_api_key,
                base_url=self.openai_base_url,
                model=settings.openai_model,
                streaming=True
            )
        else:
            logger.warning("æœªé…ç½®OpenAI APIå¯†é’¥ï¼ŒAIåŠŸèƒ½å°†ä¸å¯ç”¨")
            self.llm = None
            self.streaming_llm = None
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = OpenAICompatibleEmbeddings(
            base_url=settings.get_embedding_base_url(),
            api_key=settings.get_embedding_api_key(),
            model=settings.embedding_model_name
        )
        
        # æ–‡æœ¬åˆ†å‰²å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=100,
            length_function=len,
        )
        
        # ä½¿ç”¨å•ä¾‹ç®¡ç†å™¨è·å–å‘é‡å­˜å‚¨
        self.chroma_manager = ChromaDBManager()
        self.vector_store = self.chroma_manager.get_vector_store()
        
        # æ·»åŠ æŸ¥è¯¢å‘é‡ç¼“å­˜
        self._query_cache = {}
        self._cache_lock = threading.Lock()
        self._max_cache_size = 100  # æœ€å¤§ç¼“å­˜100ä¸ªæŸ¥è¯¢
        
        # åˆå§‹åŒ–MCPæœåŠ¡
        self.mcp_service = MCPClientService(db)

    def is_available(self) -> bool:
        """æ£€æŸ¥AIæœåŠ¡æ˜¯å¦å¯ç”¨"""
        # æ£€æŸ¥AIæ˜¯å¦åœ¨é…ç½®ä¸­å¯ç”¨
        if not settings.is_ai_enabled():
            return False
        return bool(self.openai_api_key and self.vector_store)

    def clear_all_embeddings(self) -> bool:
        """æ¸…ç©ºæ‰€æœ‰å‘é‡åµŒå…¥"""
        try:
            logger.info("å¼€å§‹æ¸…ç©ºæ‰€æœ‰å‘é‡åµŒå…¥...")
            
            # æ¸…ç©ºChromaDB collection
            success = self.chroma_manager.clear_collection()
            
            if success:
                # æ¸…ç©ºSQLiteä¸­çš„åµŒå…¥å…ƒæ•°æ®
                from ..models.embedding import Embedding
                deleted_count = self.db.query(Embedding).delete()
                self.db.commit()
                logger.info(f"å·²æ¸…ç©ºSQLiteä¸­çš„ {deleted_count} æ¡åµŒå…¥å…ƒæ•°æ®")
                
                logger.info("æ‰€æœ‰å‘é‡åµŒå…¥æ¸…ç©ºå®Œæˆ")
                return True
            else:
                logger.error("æ¸…ç©ºChromaDB collectionå¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"æ¸…ç©ºå‘é‡åµŒå…¥å¤±è´¥: {e}")
            self.db.rollback()
            return False

    def create_embeddings(self, file: File, progress_callback=None) -> bool:
        """ä¸ºæ–‡ä»¶åˆ›å»ºå‘é‡åµŒå…¥ - ä½¿ç”¨æ™ºèƒ½å¤šå±‚æ¬¡åˆ†å—"""
        if not self.is_available():
            logger.warning("AIæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•åˆ›å»ºåµŒå…¥")
            return False
        
        try:
            logger.info(f"å¼€å§‹ä¸ºæ–‡ä»¶åˆ›å»ºæ™ºèƒ½åµŒå…¥: {file.file_path}")
            
            # 1. æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç°æœ‰çš„åµŒå…¥è®°å½•
            existing_embeddings_count = self.db.query(Embedding).filter(Embedding.file_id == file.id).count()
            
            if existing_embeddings_count > 0:
                logger.info(f"æ–‡ä»¶ {file.id} å­˜åœ¨ {existing_embeddings_count} ä¸ªç°æœ‰åµŒå…¥ï¼Œéœ€è¦æ¸…ç†")
                
                # 1.1 åˆ é™¤ç°æœ‰çš„å‘é‡å­˜å‚¨ä¸­çš„æ–‡æ¡£ï¼ˆå…ˆåˆ é™¤å‘é‡å­˜å‚¨ï¼‰
                try:
                    existing_docs = self.vector_store.get(
                        where={"file_id": file.id}
                    )
                    if existing_docs and existing_docs.get('ids'):
                        self.vector_store.delete(ids=existing_docs['ids'])
                        logger.info(f"ä»LangChainå‘é‡å­˜å‚¨åˆ é™¤æ–‡ä»¶ {file.id} çš„æ–‡æ¡£: {len(existing_docs['ids'])} ä¸ª")
                except Exception as e:
                    logger.warning(f"åˆ é™¤ç°æœ‰å‘é‡å­˜å‚¨æ—¶å‡ºé”™: {e}")
                
                # 1.2 åˆ é™¤ç°æœ‰çš„SQLiteåµŒå…¥è®°å½•ï¼ˆç„¶ååˆ é™¤SQLiteè®°å½•ï¼‰
                try:
                    deleted_count = self.db.query(Embedding).filter(Embedding.file_id == file.id).delete()
                    self.db.commit()  # ç«‹å³æäº¤åˆ é™¤æ“ä½œ
                    logger.info(f"æˆåŠŸåˆ é™¤æ–‡ä»¶çš„å‘é‡ç´¢å¼•: file_id={file.id}, SQLiteåˆ é™¤äº† {deleted_count} ä¸ªè®°å½•")
                except Exception as e:
                    logger.warning(f"åˆ é™¤SQLiteåµŒå…¥è®°å½•æ—¶å‡ºé”™: {e}")
                    self.db.rollback()
            else:
                logger.info(f"æ–‡ä»¶ {file.id} æ²¡æœ‰ç°æœ‰åµŒå…¥ï¼Œç›´æ¥åˆ›å»ºæ–°çš„")
                
            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿åˆ é™¤æ“ä½œå®Œå…¨å®Œæˆ
            import time
            time.sleep(0.1)
            
            # 3. ä½¿ç”¨æ™ºèƒ½å¤šå±‚æ¬¡åˆ†å—ï¼ˆæ¯ä¸ªæ–‡ä»¶éƒ½æœ‰æ±‡æ€»æçº²ï¼‰
            logger.info(f"ğŸ§  å¼€å§‹è°ƒç”¨æ™ºèƒ½å¤šå±‚æ¬¡åˆ†å—å™¨ - æ–‡ä»¶: {file.file_path}")
            documents = self._create_hierarchical_chunks(file, progress_callback)
            logger.info(f"âœ… æ™ºèƒ½å¤šå±‚æ¬¡åˆ†å—å®Œæˆï¼Œå…±è¿”å› {len(documents)} ä¸ªæ–‡æ¡£")
            
            # éªŒè¯åˆ†å—ç»“æœ
            if not documents:
                logger.error(f"âŒ æ™ºèƒ½åˆ†å—è¿”å›ç©ºç»“æœï¼Œæ–‡ä»¶: {file.file_path}")
                return False
            
            # ç»Ÿè®¡åˆ†å—ç»“æœ
            doc_types = {}
            for doc in documents:
                chunk_type = doc.metadata.get('chunk_type', 'unknown')
                doc_types[chunk_type] = doc_types.get(chunk_type, 0) + 1
            
            logger.info(f"ğŸ“Š åˆ†å—ç»“æœç»Ÿè®¡: {doc_types}")
            
            # 4. æ‰¹é‡æ·»åŠ åˆ°å‘é‡å­˜å‚¨
            if documents:
                if progress_callback:
                    progress_callback("å‘é‡å­˜å‚¨", f"æ­£åœ¨ä¿å­˜ {len(documents)} ä¸ªåˆ†å—åˆ°å‘é‡æ•°æ®åº“")
                
                # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§å¤„ç†è¿‡å¤šæ–‡æ¡£å¯¼è‡´è¶…æ—¶
                batch_size = 50  # æ¯æ‰¹å¤„ç†50ä¸ªæ–‡æ¡£
                total_docs = len(documents)
                logger.info(f"å¼€å§‹åˆ†æ‰¹å‘é‡åŒ–ï¼Œæ€»æ–‡æ¡£æ•°: {total_docs}, æ‰¹å¤§å°: {batch_size}")
                
                for i in range(0, total_docs, batch_size):
                    batch_start = i
                    batch_end = min(i + batch_size, total_docs)
                    batch_docs = documents[batch_start:batch_end]
                    
                    try:
                        # ä¸ºå½“å‰æ‰¹æ¬¡ç”ŸæˆID
                        batch_ids = [f"file_{file.id}_chunk_{doc.metadata['chunk_index']}_{doc.metadata['chunk_type']}" for doc in batch_docs]
                        
                        logger.info(f"æ­£åœ¨å¤„ç†ç¬¬ {i//batch_size + 1} æ‰¹ï¼Œæ–‡æ¡£ {batch_start+1}-{batch_end}/{total_docs}")
                        
                        if progress_callback:
                            progress_callback("å‘é‡å­˜å‚¨", f"æ­£åœ¨å¤„ç†ç¬¬ {i//batch_size + 1} æ‰¹ ({batch_start+1}-{batch_end}/{total_docs})")
                        
                        # ä¿å­˜åˆ°ChromaDB
                        self.vector_store.add_documents(batch_docs, ids=batch_ids)
                        logger.info(f"âœ… æˆåŠŸä¿å­˜ç¬¬ {i//batch_size + 1} æ‰¹åˆ°ChromaDBï¼ŒåŒ…å« {len(batch_docs)} ä¸ªæ–‡æ¡£")
                        
                        # çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…è¿‡åº¦å ç”¨èµ„æº
                        import time
                        time.sleep(0.1)
                        
                    except Exception as e:
                        logger.error(f"âŒ ä¿å­˜ç¬¬ {i//batch_size + 1} æ‰¹åˆ°ChromaDBå¤±è´¥: {e}")
                        # å¦‚æœæŸæ‰¹å¤±è´¥ï¼Œå¯ä»¥è€ƒè™‘ç»§ç»­å¤„ç†å…¶ä»–æ‰¹æ¬¡ï¼Œæˆ–è€…ç›´æ¥å¤±è´¥
                        self.db.rollback()
                        return False
                
                logger.info(f"ğŸ‰ æˆåŠŸæ·»åŠ æ‰€æœ‰ {len(documents)} ä¸ªæ–‡æ¡£åˆ°LangChain-Chroma")
            
            # 5. æäº¤SQLiteäº‹åŠ¡
            try:
                self.db.commit()
                logger.info(f"âœ… SQLiteäº‹åŠ¡æäº¤æˆåŠŸï¼Œæ–‡ä»¶: {file.file_path}")
            except Exception as e:
                logger.error(f"âŒ SQLiteäº‹åŠ¡æäº¤å¤±è´¥: {e}")
                self.db.rollback()
                return False
            
            if progress_callback:
                progress_callback("å®Œæˆ", f"æ™ºèƒ½åˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(documents)} ä¸ªå‘é‡")
            
            logger.info(f"ä¸ºæ–‡ä»¶ {file.file_path} åˆ›å»ºäº† {len(documents)} ä¸ªæ™ºèƒ½åµŒå…¥å‘é‡")
            return True
            
        except Exception as e:
            logger.error(f"åˆ›å»ºæ™ºèƒ½åµŒå…¥å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            self.db.rollback()
            return False
    

    
    def _create_hierarchical_chunks(self, file: File, progress_callback=None) -> List[Document]:
        """åˆ›å»ºæ™ºèƒ½å¤šå±‚æ¬¡åˆ†å—ï¼ˆåŸºäºLLMï¼‰"""
        import time
        start_time = time.time()
        
        try:
            from .hierarchical_splitter import IntelligentHierarchicalSplitter
            
            logger.info(f"ğŸ§  å¼€å§‹åˆ›å»ºæ™ºèƒ½å¤šå±‚æ¬¡åˆ†å— - æ–‡ä»¶: {file.title}")
            logger.info(f"ğŸ“„ æ–‡ä»¶ä¿¡æ¯: ID={file.id}, è·¯å¾„={file.file_path}")
            logger.info(f"ğŸ“ å†…å®¹é•¿åº¦: {len(file.content)} å­—ç¬¦")
            
            if progress_callback:
                progress_callback("åˆ†æä¸­", f"æ­£åœ¨åˆ†ææ–‡ä»¶ç»“æ„å’Œå†…å®¹")
            
            # éªŒè¯æ–‡ä»¶å†…å®¹
            if not file.content or not file.content.strip():
                logger.error(f"âŒ æ–‡ä»¶å†…å®¹ä¸ºç©º: {file.file_path}")
                return []
            
            # åˆ›å»ºæ™ºèƒ½åˆ†å—å™¨ï¼Œä¼ å…¥LLMå®ä¾‹
            logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æ™ºèƒ½åˆ†å—å™¨...")
            splitter = IntelligentHierarchicalSplitter(llm=self.llm)
            
            logger.info("âš™ï¸ å¼€å§‹è°ƒç”¨æ™ºèƒ½åˆ†å—å™¨è¿›è¡Œæ–‡æ¡£åˆ†æ...")
            hierarchical_docs = splitter.split_document(file.content, file.title, file.id, file.file_path, progress_callback)
            
            # éªŒè¯åˆ†å—å™¨è¿”å›ç»“æœ
            if not hierarchical_docs:
                logger.error("âŒ æ™ºèƒ½åˆ†å—å™¨è¿”å›ç©ºç»“æœ")
                if progress_callback:
                    progress_callback("é™çº§å¤„ç†", f"æ™ºèƒ½åˆ†å—å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬åˆ†å—ç­–ç•¥")
                return self._create_basic_fallback_chunks(file, progress_callback)
            
            logger.info(f"âœ… æ™ºèƒ½åˆ†å—å™¨å®Œæˆï¼Œè¿”å›ç»“æ„: {list(hierarchical_docs.keys())}")
            
            # ç»Ÿè®¡å„å±‚çº§æ–‡æ¡£æ•°é‡
            summary_count = len(hierarchical_docs.get('summary', []))
            outline_count = len(hierarchical_docs.get('outline', []))
            content_count = len(hierarchical_docs.get('content', []))
            
            logger.info(f"ğŸ“Š åˆ†å—å™¨ç»“æœç»Ÿè®¡:")
            logger.info(f"  ğŸ“ æ‘˜è¦å±‚: {summary_count} ä¸ªæ–‡æ¡£")
            logger.info(f"  ğŸ“‹ å¤§çº²å±‚: {outline_count} ä¸ªæ–‡æ¡£")
            logger.info(f"  ğŸ“„ å†…å®¹å±‚: {content_count} ä¸ªæ–‡æ¡£")
            
            all_documents = []
            
            # å¤„ç†æ‘˜è¦å±‚
            if progress_callback:
                progress_callback("æ‘˜è¦ç”Ÿæˆ", f"æ­£åœ¨å¤„ç†æ–‡ä»¶æ‘˜è¦")
            
            logger.info("ğŸ—ï¸ å¼€å§‹å¤„ç†æ‘˜è¦å±‚æ–‡æ¡£...")
            for i, doc in enumerate(hierarchical_docs.get('summary', [])):
                try:
                    all_documents.append(doc)
                    self._save_embedding_metadata(doc, file.id)
                    logger.debug(f"  âœ… æ‘˜è¦æ–‡æ¡£ {i+1} å¤„ç†å®Œæˆ")
                except Exception as e:
                    logger.error(f"  âŒ å¤„ç†æ‘˜è¦æ–‡æ¡£ {i+1} å¤±è´¥: {e}")
            
            logger.info(f"âœ… æ‘˜è¦å±‚å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(hierarchical_docs.get('summary', []))} ä¸ªæ–‡æ¡£")
            
            # å¤„ç†å¤§çº²å±‚
            if progress_callback:
                progress_callback("å¤§çº²æå–", f"æ­£åœ¨å¤„ç†æ–‡ä»¶å¤§çº²")
            
            logger.info("ğŸ—ï¸ å¼€å§‹å¤„ç†å¤§çº²å±‚æ–‡æ¡£...")
            for i, doc in enumerate(hierarchical_docs.get('outline', [])):
                try:
                    all_documents.append(doc)
                    self._save_embedding_metadata(doc, file.id)
                    logger.debug(f"  âœ… å¤§çº²æ–‡æ¡£ {i+1} å¤„ç†å®Œæˆ")
                except Exception as e:
                    logger.error(f"  âŒ å¤„ç†å¤§çº²æ–‡æ¡£ {i+1} å¤±è´¥: {e}")
            
            logger.info(f"âœ… å¤§çº²å±‚å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(hierarchical_docs.get('outline', []))} ä¸ªæ–‡æ¡£")
            
            # å¤„ç†å†…å®¹å±‚
            if progress_callback:
                progress_callback("å†…å®¹åˆ†å—", f"æ­£åœ¨å¤„ç†å†…å®¹åˆ†å—")
            
            logger.info("ğŸ—ï¸ å¼€å§‹å¤„ç†å†…å®¹å±‚æ–‡æ¡£...")
            content_docs = hierarchical_docs.get('content', [])
            processed_content = 0
            
            for i, doc in enumerate(content_docs):
                try:
                    all_documents.append(doc)
                    self._save_embedding_metadata(doc, file.id)
                    processed_content += 1
                    
                    # æ¯50ä¸ªæ–‡æ¡£è¾“å‡ºä¸€æ¬¡è¿›åº¦
                    if (i + 1) % 50 == 0:
                        logger.info(f"  ğŸ“ˆ å†…å®¹å±‚è¿›åº¦: {i+1}/{len(content_docs)} ä¸ªæ–‡æ¡£å·²å¤„ç†")
                        
                except Exception as e:
                    logger.error(f"  âŒ å¤„ç†å†…å®¹æ–‡æ¡£ {i+1} å¤±è´¥: {e}")
            
            logger.info(f"âœ… å†…å®¹å±‚å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {processed_content}/{len(content_docs)} ä¸ªæ–‡æ¡£")
            
            # æœ€ç»ˆç»Ÿè®¡
            processing_time = time.time() - start_time
            logger.info(f"ğŸ“Š æ™ºèƒ½å¤šå±‚æ¬¡åˆ†å—æœ€ç»ˆç»Ÿè®¡:")
            logger.info(f"  âœ… æ€»æ–‡æ¡£æ•°: {len(all_documents)} ä¸ª")
            logger.info(f"  ğŸ“ æ‘˜è¦æ–‡æ¡£: {summary_count} ä¸ª")
            logger.info(f"  ğŸ“‹ å¤§çº²æ–‡æ¡£: {outline_count} ä¸ª")
            logger.info(f"  ğŸ“„ å†…å®¹æ–‡æ¡£: {processed_content} ä¸ª")
            logger.info(f"  â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
            
            # éªŒè¯æœ€ç»ˆç»“æœ
            if not all_documents:
                logger.error("âŒ æ™ºèƒ½å¤šå±‚æ¬¡åˆ†å—æœ€ç»ˆç»“æœä¸ºç©º")
                if progress_callback:
                    progress_callback("é™çº§å¤„ç†", f"æ™ºèƒ½åˆ†å—å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬åˆ†å—ç­–ç•¥")
                return self._create_basic_fallback_chunks(file, progress_callback)
            
            logger.info(f"ğŸ‰ æ™ºèƒ½å¤šå±‚æ¬¡åˆ†å—å®Œæˆ: æ€»å…± {len(all_documents)} ä¸ªæ–‡æ¡£")
            return all_documents
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"âŒ åˆ›å»ºæ™ºèƒ½å¤šå±‚æ¬¡åˆ†å—å¤±è´¥ (è€—æ—¶: {processing_time:.2f}s): {e}")
            import traceback
            logger.error(f"ğŸ“‹ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            
            # åˆ›å»ºæœ€åŸºæœ¬çš„æ‘˜è¦å’Œå†…å®¹å—ï¼ˆé™çº§ç­–ç•¥ï¼‰
            if progress_callback:
                progress_callback("é™çº§å¤„ç†", f"æ™ºèƒ½åˆ†å—å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬åˆ†å—ç­–ç•¥")
            
            logger.info("ğŸ”„ é™çº§åˆ°åŸºæœ¬åˆ†å—ç­–ç•¥...")
            return self._create_basic_fallback_chunks(file, progress_callback)
    
    def _create_basic_fallback_chunks(self, file: File, progress_callback=None) -> List[Document]:
        """åˆ›å»ºåŸºæœ¬çš„é™çº§åˆ†å—ï¼ˆç¡®ä¿æ¯ä¸ªæ–‡ä»¶éƒ½æœ‰æ‘˜è¦å’Œå†…å®¹å—ï¼‰"""
        try:
            documents = []
            
            # 1. åˆ›å»ºåŸºæœ¬æ‘˜è¦å—
            if progress_callback:
                progress_callback("åŸºæœ¬æ‘˜è¦", f"åˆ›å»ºåŸºæœ¬æ‘˜è¦å—")
            
            summary_text = f"æ–‡ä»¶ï¼š{file.title}\nå†…å®¹é¢„è§ˆï¼š{file.content[:500]}..."
            summary_doc = Document(
                page_content=summary_text,
                metadata={
                    "file_id": file.id,
                    "file_path": file.file_path,
                    "chunk_index": 0,
                    "chunk_hash": hashlib.sha256(summary_text.encode()).hexdigest(),
                    "title": file.title,
                    "vector_model": settings.embedding_model_name,
                    "chunk_type": "summary",
                    "chunk_level": 1,
                    "parent_heading": None,
                    "section_path": "åŸºæœ¬æ‘˜è¦",
                    "generation_method": "basic_fallback"
                }
            )
            documents.append(summary_doc)
            self._save_embedding_metadata(summary_doc, file.id)
            
            # 2. åˆ›å»ºå†…å®¹å—
            if progress_callback:
                progress_callback("å†…å®¹åˆ†å—", f"æ­£åœ¨åˆ›å»ºå†…å®¹åˆ†å—")
            
            # ä½¿ç”¨æ–‡æœ¬åˆ†å‰²å™¨åˆ›å»ºå†…å®¹å—
            content_chunks = self.text_splitter.split_text(file.content)
            
            for i, chunk in enumerate(content_chunks):
                content_doc = Document(
                    page_content=chunk,
                    metadata={
                        "file_id": file.id,
                        "file_path": file.file_path,
                        "chunk_index": i + 1,
                        "chunk_hash": hashlib.sha256(chunk.encode()).hexdigest(),
                        "title": file.title,
                        "vector_model": settings.embedding_model_name,
                        "chunk_type": "content",
                        "chunk_level": 3,
                        "parent_heading": None,
                        "section_path": f"å†…å®¹å—{i+1}",
                        "generation_method": "basic_fallback"
                    }
                )
                documents.append(content_doc)
                self._save_embedding_metadata(content_doc, file.id)
            
            logger.info(f"åŸºæœ¬åˆ†å—å®Œæˆ: 1ä¸ªæ‘˜è¦å— + {len(content_chunks)}ä¸ªå†…å®¹å—")
            return documents
            
        except Exception as e:
            logger.error(f"åˆ›å»ºåŸºæœ¬åˆ†å—å¤±è´¥: {e}")
            return []

    def _save_embedding_metadata(self, doc: Document, file_id: int):
        """ä¿å­˜åµŒå…¥å…ƒæ•°æ®åˆ°SQLite"""
        try:
            # è·å–vector_modelï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è®¾ç½®é»˜è®¤å€¼
            vector_model = doc.metadata.get('vector_model', 'unknown')
            
            # åˆ›å»ºåµŒå…¥è®°å½•
            embedding = Embedding(
                file_id=file_id,
                chunk_index=doc.metadata['chunk_index'],
                chunk_text=doc.page_content,  # æ·»åŠ ç¼ºå°‘çš„chunk_textå­—æ®µ
                chunk_hash=doc.metadata['chunk_hash'],
                vector_model=vector_model,
                chunk_type=doc.metadata.get('chunk_type', 'content'),
                chunk_level=doc.metadata.get('chunk_level', 1),
                parent_heading=doc.metadata.get('parent_heading'),
                section_path=doc.metadata.get('section_path')
                # ç§»é™¤äº†ä¸å­˜åœ¨çš„generation_methodå­—æ®µ
            )
            self.db.add(embedding)
            # ä¸åœ¨è¿™é‡Œæäº¤ï¼Œè®©ä¸Šå±‚ç»Ÿä¸€æäº¤
            
        except Exception as e:
            logger.error(f"ä¿å­˜åµŒå…¥å…ƒæ•°æ®å¤±è´¥: {e}")
            raise

    def semantic_search(self, query: str, limit: int = 10, similarity_threshold: float = None) -> List[Dict[str, Any]]:
        """è¯­ä¹‰æœç´¢ - æ”¯æŒå¤šå±‚æ¬¡æ£€ç´¢ï¼Œå¸¦ç¼“å­˜ä¼˜åŒ–"""
        if not self.is_available():
            logger.warning("AIæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œè¯­ä¹‰æœç´¢")
            return []
        
        # ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤é˜ˆå€¼
        if similarity_threshold is None:
            similarity_threshold = settings.semantic_search_threshold
        
        try:
            start_time = time.time()
            logger.info(f"å¼€å§‹è¯­ä¹‰æœç´¢ï¼ŒæŸ¥è¯¢: {query}, é˜ˆå€¼: {similarity_threshold}")
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨å¤šå±‚æ¬¡æ£€ç´¢
            if settings.enable_hierarchical_chunking:
                results = self._hierarchical_semantic_search(query, limit, similarity_threshold)
            else:
                results = self._traditional_semantic_search(query, limit, similarity_threshold)
            
            total_time = time.time() - start_time
            logger.info(f"è¯­ä¹‰æœç´¢å®Œæˆï¼ŒæŸ¥è¯¢: {query}, ç»“æœ: {len(results)}, æ€»è€—æ—¶: {total_time:.3f}ç§’")
            return results
            
        except Exception as e:
            logger.error(f"è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return []
    
    def _traditional_semantic_search(self, query: str, limit: int, similarity_threshold: float) -> List[Dict[str, Any]]:
        """ä¼ ç»Ÿè¯­ä¹‰æœç´¢ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰"""
        try:
            # ä½¿ç”¨LangChainçš„similarity_search_with_scoreæ–¹æ³•ï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰
            search_results = self.vector_store.similarity_search_with_score(
                query=query,
                k=limit * 2,  # è·å–æ›´å¤šç»“æœç”¨äºè¿‡æ»¤
                filter=None  # å¯ä»¥æ·»åŠ è¿‡æ»¤æ¡ä»¶
            )
            
            logger.info(f"ä¼ ç»Ÿæœç´¢è¿”å› {len(search_results)} ä¸ªç»“æœ")
            
            # å¤„ç†æœç´¢ç»“æœå¹¶å»é‡
            results = []
            seen_files = {}  # ç”¨äºæ–‡ä»¶å»é‡ï¼šfile_id -> æœ€ä½³åŒ¹é…ç»“æœ
            
            for doc, score in search_results:
                # LangChain-Chromaè¿”å›çš„scoreæ˜¯è·ç¦»ï¼Œè·ç¦»è¶Šå°è¶Šç›¸ä¼¼
                distance = score
                
                logger.info(f"æ–‡æ¡£: {doc.metadata.get('file_path', 'unknown')}, è·ç¦»: {distance:.4f}")
                
                # è¿‡æ»¤è·ç¦»è¿‡å¤§çš„ç»“æœï¼ˆè·ç¦»å°äºé˜ˆå€¼çš„ä¿ç•™ï¼‰
                if distance > similarity_threshold:
                    logger.info(f"è·ç¦» {distance:.4f} å¤§äºé˜ˆå€¼ {similarity_threshold}ï¼Œè·³è¿‡")
                    continue
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä»ç„¶å­˜åœ¨ä¸”æœªåˆ é™¤
                file_id = doc.metadata.get('file_id')
                if file_id:
                    file = self.db.query(File).filter(
                        File.id == file_id,
                        File.is_deleted == False
                    ).first()
                    
                    if file:
                        result_item = {
                            'file_id': file_id,
                            'file_path': doc.metadata.get('file_path', ''),
                            'title': doc.metadata.get('title', ''),
                            'chunk_text': doc.page_content,
                            'chunk_index': doc.metadata.get('chunk_index', 0),
                            'similarity': float(1 - distance),  # ä¸ºå…¼å®¹æ€§è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼ˆ1-è·ç¦»ï¼‰
                            'distance': float(distance),  # ä¿å­˜åŸå§‹è·ç¦»ç”¨äºæ¯”è¾ƒ
                            'created_at': file.created_at.isoformat() if file.created_at else None,
                            'updated_at': file.updated_at.isoformat() if file.updated_at else None,
                        }
                        
                        # æ–‡ä»¶å»é‡ï¼šä¿ç•™æ¯ä¸ªæ–‡ä»¶çš„æœ€ä½³åŒ¹é…ï¼ˆè·ç¦»æœ€å°ï¼‰
                        if file_id not in seen_files or distance < seen_files[file_id]['distance']:
                            seen_files[file_id] = result_item
                            logger.info(f"æ·»åŠ /æ›´æ–°æœ€ä½³åŒ¹é…: {doc.metadata.get('file_path')}, è·ç¦»: {distance:.4f}")
                        else:
                            logger.info(f"è·³è¿‡é‡å¤æ–‡ä»¶ï¼ˆè·ç¦»æ›´å¤§ï¼‰: {doc.metadata.get('file_path')}, è·ç¦»: {distance:.4f}")
                    else:
                        logger.info(f"æ–‡ä»¶ä¸å­˜åœ¨æˆ–å·²åˆ é™¤: file_id={file_id}")
            
            # å°†å»é‡åçš„ç»“æœè½¬æ¢ä¸ºåˆ—è¡¨ï¼ŒæŒ‰è·ç¦»æ’åº
            results = list(seen_files.values())
            results.sort(key=lambda x: x['distance'])  # æŒ‰è·ç¦»å‡åºæ’åºï¼ˆæœ€ç›¸ä¼¼çš„åœ¨å‰ï¼‰
            
            # ç§»é™¤ä¸´æ—¶çš„distanceå­—æ®µï¼Œé™åˆ¶ç»“æœæ•°é‡
            for result in results[:limit]:
                result.pop('distance', None)
            
            return results
            
        except Exception as e:
            logger.error(f"ä¼ ç»Ÿè¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            return []
    
    def _hierarchical_semantic_search(self, query: str, limit: int, similarity_threshold: float) -> List[Dict[str, Any]]:
        """å¤šå±‚æ¬¡è¯­ä¹‰æœç´¢"""
        try:
            logger.info(f"å¼€å§‹å¤šå±‚æ¬¡è¯­ä¹‰æœç´¢: {query}")
            
            # å¤šè·¯å¬å›ï¼šåŒæ—¶æœç´¢ä¸‰ä¸ªå±‚æ¬¡
            summary_results = self._search_by_chunk_type(query, "summary", limit//3, similarity_threshold)
            outline_results = self._search_by_chunk_type(query, "outline", limit//3, similarity_threshold)
            content_results = self._search_by_chunk_type(query, "content", limit, similarity_threshold)
            
            # è®°å½•æ¯å±‚çº§çš„è¯¦ç»†åŒ¹é…å†…å®¹
            logger.info(f"ğŸ“ æ‘˜è¦å±‚åŒ¹é…ç»“æœ ({len(summary_results)} ä¸ª):")
            for i, result in enumerate(summary_results, 1):
                logger.info(f"   {i}. æ–‡ä»¶: {result.get('title', 'Unknown')} (ç›¸ä¼¼åº¦: {result.get('similarity', 0):.3f})")
                logger.info(f"      æ‘˜è¦å†…å®¹: {result.get('chunk_text', '')[:200]}...")
            
            logger.info(f"ğŸ“‹ å¤§çº²å±‚åŒ¹é…ç»“æœ ({len(outline_results)} ä¸ª):")
            for i, result in enumerate(outline_results, 1):
                logger.info(f"   {i}. æ–‡ä»¶: {result.get('title', 'Unknown')} (ç›¸ä¼¼åº¦: {result.get('similarity', 0):.3f})")
                logger.info(f"      å¤§çº²å†…å®¹: {result.get('chunk_text', '')[:200]}...")
            
            logger.info(f"ğŸ“„ å†…å®¹å±‚åŒ¹é…ç»“æœ ({len(content_results)} ä¸ª):")
            for i, result in enumerate(content_results, 1):
                logger.info(f"   {i}. æ–‡ä»¶: {result.get('title', 'Unknown')} (ç›¸ä¼¼åº¦: {result.get('similarity', 0):.3f})")
                logger.info(f"      å†…å®¹ç‰‡æ®µ: {result.get('chunk_text', '')[:200]}...")
            
            # æ™ºèƒ½ä¸Šä¸‹æ–‡æ‰©å±•
            expanded_results = []
            
            # å¤„ç†æ‘˜è¦åŒ¹é…ç»“æœ
            for result in summary_results:
                expanded_results.append(result)
                # è·å–è¯¥æ–‡ä»¶çš„å¤§çº²å’Œå†…å®¹
                file_outline = self._get_file_outline(result['file_id'])
                expanded_results.extend(file_outline[:2])  # æ·»åŠ å‰2ä¸ªå¤§çº²é¡¹
            
            # å¤„ç†å¤§çº²åŒ¹é…ç»“æœ
            for result in outline_results:
                expanded_results.append(result)
                # è·å–è¯¥ç« èŠ‚ä¸‹çš„å†…å®¹å—
                section_content = self._get_section_content(result['file_id'], result.get('section_path'))
                expanded_results.extend(section_content[:2])  # æ·»åŠ å‰2ä¸ªå†…å®¹å—
            
            # å¤„ç†å†…å®¹åŒ¹é…ç»“æœ
            expanded_results.extend(content_results)
            
            # å»é‡å¹¶é™åˆ¶ç»“æœæ•°é‡
            final_results = self._deduplicate_and_rank(expanded_results, limit)
            
            # è®°å½•æœ€ç»ˆæ„å»ºçš„ä¸Šä¸‹æ–‡
            logger.info(f"ğŸ”§ æœ€ç»ˆæ„å»ºçš„æœç´¢ä¸Šä¸‹æ–‡:")
            total_context_length = 0
            for i, result in enumerate(final_results, 1):
                chunk_text = result.get('chunk_text', '')
                total_context_length += len(chunk_text)
                logger.info(f"   {i}. [{result.get('chunk_type', 'content')}] {result.get('title', 'Unknown')} - {len(chunk_text)} å­—ç¬¦")
                logger.info(f"      é¢„è§ˆ: {chunk_text[:150]}..." if len(chunk_text) > 150 else f"      å†…å®¹: {chunk_text}")
            
            logger.info(f"ğŸ“Š ä¸Šä¸‹æ–‡ç»Ÿè®¡: æ€»é•¿åº¦={total_context_length} å­—ç¬¦, ç‰‡æ®µæ•°={len(final_results)}")
            logger.info(f"å¤šå±‚æ¬¡æœç´¢å®Œæˆ: æ‘˜è¦={len(summary_results)}, å¤§çº²={len(outline_results)}, å†…å®¹={len(content_results)}, æœ€ç»ˆ={len(final_results)}")
            return final_results
            
        except Exception as e:
            logger.error(f"å¤šå±‚æ¬¡è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            # é™çº§åˆ°ä¼ ç»Ÿæœç´¢
            return self._traditional_semantic_search(query, limit, similarity_threshold)
    
    def _search_by_chunk_type(self, query: str, chunk_type: str, limit: int, similarity_threshold: float) -> List[Dict[str, Any]]:
        """æŒ‰åˆ†å—ç±»å‹æœç´¢"""
        try:
            logger.info(f"ğŸ” å¼€å§‹æŒ‰ç±»å‹æœç´¢: {chunk_type}, æŸ¥è¯¢: '{query}', é˜ˆå€¼: {similarity_threshold}")
            
            search_results = self.vector_store.similarity_search_with_score(
                query=query,
                k=limit * 2,
                filter={"chunk_type": chunk_type}
            )
            
            logger.info(f"ğŸ“Š å‘é‡æ•°æ®åº“è¿”å› {len(search_results)} ä¸ª {chunk_type} ç±»å‹çš„åŸå§‹ç»“æœ")
            
            results = []
            filtered_count = 0
            
            for i, (doc, score) in enumerate(search_results, 1):
                distance = score
                similarity = 1 - distance
                
                logger.info(f"   åŸå§‹ç»“æœ {i}: è·ç¦»={distance:.4f}, ç›¸ä¼¼åº¦={similarity:.4f}, æ–‡ä»¶={doc.metadata.get('title', 'Unknown')}")
                logger.info(f"     å†…å®¹é¢„è§ˆ: {doc.page_content[:100]}...")
                
                if score <= similarity_threshold:
                    file_id = doc.metadata.get('file_id')
                    if file_id:
                        file = self.db.query(File).filter(
                            File.id == file_id,
                            File.is_deleted == False
                        ).first()
                        
                        if file:
                            result_item = {
                                'file_id': file_id,
                                'file_path': doc.metadata.get('file_path', ''),
                                'title': doc.metadata.get('title', ''),
                                'chunk_text': doc.page_content,
                                'chunk_index': doc.metadata.get('chunk_index', 0),
                                'chunk_type': chunk_type,
                                'chunk_level': doc.metadata.get('chunk_level', 3),
                                'parent_heading': doc.metadata.get('parent_heading'),
                                'section_path': doc.metadata.get('section_path'),
                                'similarity': float(similarity),
                                'created_at': file.created_at.isoformat() if file.created_at else None,
                                'updated_at': file.updated_at.isoformat() if file.updated_at else None,
                            }
                            results.append(result_item)
                            logger.info(f"     âœ… é€šè¿‡é˜ˆå€¼ç­›é€‰ï¼ŒåŠ å…¥ç»“æœåˆ—è¡¨")
                        else:
                            logger.info(f"     âŒ æ–‡ä»¶ä¸å­˜åœ¨æˆ–å·²åˆ é™¤: file_id={file_id}")
                else:
                    filtered_count += 1
                    logger.info(f"     âŒ æœªé€šè¿‡é˜ˆå€¼ç­›é€‰ (è·ç¦» {distance:.4f} > {similarity_threshold})")
            
            final_results = results[:limit]
            logger.info(f"ğŸ¯ {chunk_type} æœç´¢å®Œæˆ: åŸå§‹={len(search_results)}, è¿‡æ»¤={filtered_count}, é€šè¿‡={len(results)}, æœ€ç»ˆ={len(final_results)}")
            
            return final_results
            
        except Exception as e:
            logger.error(f"æŒ‰ç±»å‹æœç´¢å¤±è´¥ ({chunk_type}): {e}")
            return []
    
    def _get_file_outline(self, file_id: int) -> List[Dict[str, Any]]:
        """è·å–æ–‡ä»¶çš„å¤§çº²"""
        try:
            # ä»å‘é‡å­˜å‚¨ä¸­è·å–è¯¥æ–‡ä»¶çš„outlineç±»å‹æ–‡æ¡£ - ä½¿ç”¨æ­£ç¡®çš„ChromaDBæŸ¥è¯¢è¯­æ³•
            docs = self.vector_store.get(
                where={
                    "$and": [
                        {"file_id": {"$eq": file_id}},
                        {"chunk_type": {"$eq": "outline"}}
                    ]
                },
                limit=10
            )
            
            results = []
            if docs and docs.get('documents'):
                for i, doc_content in enumerate(docs['documents']):
                    metadata = docs['metadatas'][i]
                    result_item = {
                        'file_id': file_id,
                        'file_path': metadata.get('file_path', ''),
                        'title': metadata.get('title', ''),
                        'chunk_text': doc_content,
                        'chunk_index': metadata.get('chunk_index', 0),
                        'chunk_type': 'outline',
                        'chunk_level': 2,
                        'parent_heading': metadata.get('parent_heading'),
                        'section_path': metadata.get('section_path'),
                        'similarity': 0.8,  # ä¸Šä¸‹æ–‡ç›¸å…³æ€§
                    }
                    results.append(result_item)
            
            return results
            
        except Exception as e:
            logger.error(f"è·å–æ–‡ä»¶å¤§çº²å¤±è´¥: {e}")
            return []
    
    def _get_section_content(self, file_id: int, section_path: str) -> List[Dict[str, Any]]:
        """è·å–ç« èŠ‚å†…å®¹"""
        try:
            # ä»å‘é‡å­˜å‚¨ä¸­è·å–è¯¥ç« èŠ‚çš„å†…å®¹ - ä½¿ç”¨æ­£ç¡®çš„ChromaDBæŸ¥è¯¢è¯­æ³•
            docs = self.vector_store.get(
                where={
                    "$and": [
                        {"file_id": {"$eq": file_id}},
                        {"chunk_type": {"$eq": "content"}},
                        {"parent_heading": {"$eq": section_path}}
                    ]
                },
                limit=5
            )
            
            results = []
            if docs and docs.get('documents'):
                for i, doc_content in enumerate(docs['documents']):
                    metadata = docs['metadatas'][i]
                    result_item = {
                        'file_id': file_id,
                        'file_path': metadata.get('file_path', ''),
                        'title': metadata.get('title', ''),
                        'chunk_text': doc_content,
                        'chunk_index': metadata.get('chunk_index', 0),
                        'chunk_type': 'content',
                        'chunk_level': 3,
                        'parent_heading': metadata.get('parent_heading'),
                        'section_path': metadata.get('section_path'),
                        'similarity': 0.7,  # ä¸Šä¸‹æ–‡ç›¸å…³æ€§
                    }
                    results.append(result_item)
            
            return results
            
        except Exception as e:
            logger.error(f"è·å–ç« èŠ‚å†…å®¹å¤±è´¥: {e}")
            return []
    
    def _deduplicate_and_rank(self, results: List[Dict[str, Any]], limit: int) -> List[Dict[str, Any]]:
        """å»é‡å¹¶æ’åº"""
        seen_chunks = set()
        unique_results = []
        
        for result in results:
            chunk_key = (result['file_id'], result['chunk_index'], result.get('chunk_type', 'content'))
            if chunk_key not in seen_chunks:
                seen_chunks.add(chunk_key)
                unique_results.append(result)
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        unique_results.sort(key=lambda x: x.get('similarity', 0), reverse=True)
        
        return unique_results[:limit]

    def clear_vector_database(self) -> bool:
        """æ¸…ç©ºå‘é‡æ•°æ®åº“"""
        try:
            # 1. æ¸…ç©ºSQLiteä¸­çš„åµŒå…¥å‘é‡
            self.db.query(Embedding).delete()
            self.db.commit()
            
            # 2. æ¸…ç©ºLangChainå‘é‡å­˜å‚¨
            if self.vector_store:
                try:
                    # è·å–æ‰€æœ‰æ–‡æ¡£IDå¹¶åˆ é™¤
                    all_docs = self.vector_store.get()
                    if all_docs and all_docs.get('ids'):
                        self.vector_store.delete(ids=all_docs['ids'])
                        logger.info(f"æ¸…ç©ºLangChainå‘é‡å­˜å‚¨ï¼Œåˆ é™¤äº† {len(all_docs['ids'])} ä¸ªæ–‡æ¡£")
                except Exception as e:
                    logger.warning(f"æ¸…ç©ºLangChainå‘é‡å­˜å‚¨æ—¶å‡ºé”™: {e}")
            
            logger.info("å‘é‡æ•°æ®åº“å·²æ¸…ç©º")
            return True
            
        except Exception as e:
            logger.error(f"æ¸…ç©ºå‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            self.db.rollback()
            return False

    def delete_document_by_file_path(self, file_path: str) -> bool:
        """æ ¹æ®æ–‡ä»¶è·¯å¾„åˆ é™¤æ–‡æ¡£çš„å‘é‡ç´¢å¼•"""
        try:
            # æ ¹æ®æ–‡ä»¶è·¯å¾„æŸ¥æ‰¾æ–‡ä»¶
            file = self.db.query(File).filter(File.file_path == file_path).first()
            if not file:
                logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•åˆ é™¤å‘é‡ç´¢å¼•: {file_path}")
                return False
            
            return self.delete_document_by_file_id(file.id)
            
        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡ä»¶å‘é‡ç´¢å¼•å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            return False

    def delete_document_by_file_id(self, file_id: int) -> bool:
        """æ ¹æ®æ–‡ä»¶IDåˆ é™¤æ–‡æ¡£çš„å‘é‡ç´¢å¼•"""
        try:
            # 1. åˆ é™¤SQLiteä¸­çš„åµŒå…¥è®°å½•
            deleted_count = self.db.query(Embedding).filter(Embedding.file_id == file_id).delete()
            
            # 2. åˆ é™¤LangChainå‘é‡å­˜å‚¨ä¸­çš„æ–‡æ¡£
            if self.vector_store:
                try:
                    existing_docs = self.vector_store.get(
                        where={"file_id": file_id}
                    )
                    if existing_docs and existing_docs.get('ids'):
                        self.vector_store.delete(ids=existing_docs['ids'])
                        logger.info(f"ä»LangChainå‘é‡å­˜å‚¨åˆ é™¤æ–‡ä»¶ {file_id} çš„æ–‡æ¡£: {len(existing_docs['ids'])} ä¸ª")
                except Exception as e:
                    logger.warning(f"ä»LangChainå‘é‡å­˜å‚¨åˆ é™¤æ–‡æ¡£æ—¶å‡ºé”™: {e}")
            
            self.db.commit()
            logger.info(f"æˆåŠŸåˆ é™¤æ–‡ä»¶çš„å‘é‡ç´¢å¼•: file_id={file_id}, SQLiteåˆ é™¤äº† {deleted_count} ä¸ªè®°å½•")
            return True
            
        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡ä»¶å‘é‡ç´¢å¼•å¤±è´¥: file_id={file_id}, é”™è¯¯: {e}")
            self.db.rollback()
            return False

    def get_vector_count(self) -> int:
        """è·å–å‘é‡æ•°æ®åº“ä¸­çš„å‘é‡æ•°é‡ - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œé¿å…è·å–æ‰€æœ‰æ•°æ®"""
        try:
            if self.vector_store:
                # ä½¿ç”¨ChromaDBçš„countæ–¹æ³•ï¼Œé¿å…è·å–æ‰€æœ‰æ•°æ®
                try:
                    # å°è¯•ä½¿ç”¨ChromaDBçš„å†…éƒ¨æ–¹æ³•è·å–æ•°é‡
                    collection = self.vector_store._collection
                    if hasattr(collection, 'count'):
                        return collection.count()
                    elif hasattr(collection, '_count'):
                        return collection._count()
                    else:
                        # å¦‚æœæ²¡æœ‰ç›´æ¥çš„countæ–¹æ³•ï¼Œä½¿ç”¨limit=1çš„æŸ¥è¯¢æ¥æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
                        # è¿™æ ·é¿å…è·å–æ‰€æœ‰æ•°æ®
                        sample = self.vector_store.get(limit=1)
                        if sample and sample.get('ids'):
                            # æœ‰æ•°æ®ä½†æ— æ³•è·å–ç²¾ç¡®æ•°é‡ï¼Œè¿”å›ä¼°ç®—å€¼
                            return -1  # ä½¿ç”¨-1è¡¨ç¤º"æœ‰æ•°æ®ä½†æ•°é‡æœªçŸ¥"
                        else:
                            return 0
                except Exception as e:
                    logger.warning(f"æ— æ³•è·å–ç²¾ç¡®å‘é‡æ•°é‡: {e}")
                    # é™çº§æ–¹æ¡ˆï¼šå°è¯•ç®€å•æŸ¥è¯¢æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
                    try:
                        sample = self.vector_store.get(limit=1)
                        return -1 if sample and sample.get('ids') else 0
                    except:
                        return 0
            return 0
        except Exception as e:
            logger.error(f"è·å–å‘é‡æ•°é‡å¤±è´¥: {e}")
            return 0

    def add_document_to_vector_db(self, file_id: int, title: str, content: str, metadata: Dict[str, Any] = None) -> bool:
        """å‘å‘é‡æ•°æ®åº“æ·»åŠ æ–‡æ¡£"""
        try:
            # è·å–æ–‡ä»¶å¯¹è±¡
            file = self.db.query(File).filter(File.id == file_id).first()
            if not file:
                logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_id}")
                return False
            
            # åˆ›å»ºåµŒå…¥å‘é‡
            success = self.create_embeddings(file)
            if success:
                logger.info(f"æ–‡æ¡£å·²æ·»åŠ åˆ°å‘é‡æ•°æ®åº“: {title}")
            return success
            
        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            return False

    def update_file_path_in_vectors(self, file_id: int, old_path: str, new_path: str, new_title: str) -> bool:
        """æ›´æ–°å‘é‡æ•°æ®åº“ä¸­æ–‡ä»¶çš„è·¯å¾„ä¿¡æ¯"""
        try:
            if not self.vector_store:
                logger.warning("å‘é‡å­˜å‚¨ä¸å¯ç”¨ï¼Œæ— æ³•æ›´æ–°æ–‡ä»¶è·¯å¾„")
                return False
            
            logger.info(f"å¼€å§‹æ›´æ–°å‘é‡æ•°æ®åº“ä¸­çš„æ–‡ä»¶è·¯å¾„: {old_path} -> {new_path}")
            
            # 1. è·å–è¯¥æ–‡ä»¶çš„æ‰€æœ‰å‘é‡æ–‡æ¡£
            existing_docs = self.vector_store.get(
                where={"file_id": file_id}
            )
            
            if not existing_docs or not existing_docs.get('ids'):
                logger.warning(f"æœªæ‰¾åˆ°æ–‡ä»¶ {file_id} çš„å‘é‡æ•°æ®")
                return True  # æ²¡æœ‰å‘é‡æ•°æ®ä¹Ÿç®—æˆåŠŸ
            
            # 2. æ”¶é›†éœ€è¦æ›´æ–°çš„æ–‡æ¡£ä¿¡æ¯
            doc_ids = existing_docs['ids']
            metadatas = existing_docs['metadatas']
            embeddings = existing_docs['embeddings']
            documents = existing_docs['documents']
            
            logger.info(f"æ‰¾åˆ° {len(doc_ids)} ä¸ªå‘é‡æ–‡æ¡£éœ€è¦æ›´æ–°")
            
            # 3. åˆ é™¤æ—§çš„æ–‡æ¡£
            self.vector_store.delete(ids=doc_ids)
            logger.info(f"åˆ é™¤äº†æ—§çš„å‘é‡æ–‡æ¡£: {len(doc_ids)} ä¸ª")
            
            # 4. æ›´æ–°å…ƒæ•°æ®å¹¶é‡æ–°æ·»åŠ 
            updated_documents = []
            updated_ids = []
            
            for i, (doc_id, metadata, embedding, document_content) in enumerate(zip(doc_ids, metadatas, embeddings, documents)):
                # æ›´æ–°å…ƒæ•°æ®ä¸­çš„æ–‡ä»¶è·¯å¾„å’Œæ ‡é¢˜
                metadata['file_path'] = new_path
                metadata['title'] = new_title
                
                # åˆ›å»ºæ–°çš„æ–‡æ¡£å¯¹è±¡
                doc = Document(
                    page_content=document_content,
                    metadata=metadata
                )
                updated_documents.append(doc)
                updated_ids.append(doc_id)
            
            # 5. é‡æ–°æ·»åŠ æ–‡æ¡£ï¼ˆä½¿ç”¨é¢„è®¡ç®—çš„åµŒå…¥å‘é‡ï¼‰
            self.vector_store.add_documents(
                documents=updated_documents,
                ids=updated_ids
            )
            
            logger.info(f"æˆåŠŸæ›´æ–° {len(updated_documents)} ä¸ªå‘é‡æ–‡æ¡£çš„è·¯å¾„ä¿¡æ¯")
            return True
            
        except Exception as e:
            logger.error(f"æ›´æ–°å‘é‡æ•°æ®åº“æ–‡ä»¶è·¯å¾„å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return False

    def generate_summary(self, content: str, max_length: int = 200) -> Optional[str]:
        """ç”Ÿæˆæ–‡æ¡£æ‘˜è¦"""
        if not self.llm:
            logger.warning("LLMä¸å¯ç”¨ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦")
            return None
        
        try:
            prompt = f"""è¯·ä¸ºä»¥ä¸‹å†…å®¹ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼Œä¸è¶…è¿‡{max_length}å­—ï¼š

å†…å®¹ï¼š
{content[:2000]}  # é™åˆ¶è¾“å…¥é•¿åº¦

æ‘˜è¦ï¼š"""
            
            response = self.llm.invoke(prompt)
            summary = response.content.strip()
            
            logger.info(f"æ‘˜è¦ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(summary)}")
            return summary
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
            return None

    def generate_outline(self, content: str, max_items: int = 10) -> Optional[str]:
        """ç”Ÿæˆæ–‡æ¡£æçº²"""
        if not self.llm:
            logger.warning("LLMä¸å¯ç”¨ï¼Œæ— æ³•ç”Ÿæˆæçº²")
            return None
        
        try:
            prompt = f"""è¯·ä¸ºä»¥ä¸‹å†…å®¹ç”Ÿæˆä¸€ä¸ªæ¸…æ™°çš„æçº²ï¼ŒåŒ…å«ä¸»è¦ç« èŠ‚å’Œè¦ç‚¹ï¼Œä¸è¶…è¿‡{max_items}ä¸ªè¦ç‚¹ï¼š

å†…å®¹ï¼š
{content[:3000]}  # é™åˆ¶è¾“å…¥é•¿åº¦

è¦æ±‚ï¼š
1. æå–ä¸»è¦ç« èŠ‚å’Œå…³é”®è¦ç‚¹
2. ä½¿ç”¨å±‚çº§ç»“æ„ï¼ˆå¦‚ï¼šä¸€ã€äºŒã€ä¸‰... æˆ– 1. 2. 3...ï¼‰
3. ä¿æŒé€»è¾‘æ¸…æ™°ï¼Œç»“æ„åˆç†
4. æ¯ä¸ªè¦ç‚¹ç®€æ´æ˜äº†

æçº²ï¼š"""
            
            response = self.llm.invoke(prompt)
            outline = response.content.strip()
            
            newline_count = outline.count('\n')
            logger.info(f"æçº²ç”ŸæˆæˆåŠŸï¼ŒåŒ…å« {newline_count} è¡Œ")
            return outline
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæçº²å¤±è´¥: {e}")
            return None

    def suggest_tags(self, title: str, content: str, max_tags: int = 5) -> List[str]:
        """æ™ºèƒ½æ ‡ç­¾å»ºè®® - æ”¯æŒå¤šå±‚æ¬¡åˆ†æï¼Œä»é¢„è®¾å’Œæ•°æ®åº“ç°æœ‰æ ‡ç­¾ä¸­é€‰æ‹©"""
        if not self.llm:
            logger.warning("LLMä¸å¯ç”¨ï¼Œæ— æ³•ç”Ÿæˆæ ‡ç­¾å»ºè®®")
            return []
        
        try:
            # 1. å®šä¹‰é¢„è®¾çš„å¸¸è§„æ ‡ç­¾
            predefined_tags = [
                "é‡ç‚¹", "å‰ç«¯", "åç«¯", "AIå¤§æ¨¡å‹", "æŠ€å·§", 
                "ç¬”è®°", "æ€»ç»“", "æ•™ç¨‹", "æ–‡æ¡£", "é…ç½®",
                "é—®é¢˜", "è§£å†³æ–¹æ¡ˆ", "ä»£ç ", "å·¥å…·", "æ¡†æ¶",
                "æ•°æ®åº“", "ç½‘ç»œ", "å®‰å…¨", "æ€§èƒ½", "æµ‹è¯•",
                "éƒ¨ç½²", "è¿ç»´", "ç®—æ³•", "æ¶æ„", "è®¾è®¡",
                "å­¦ä¹ ", "èµ„æº", "å‚è€ƒ", "ç¤ºä¾‹", "æ¨¡æ¿"
            ]
            
            # 2. ä»æ•°æ®åº“è·å–ç°æœ‰çš„ä¸é‡å¤æ ‡ç­¾
            existing_tags = []
            try:
                # ä½¿ç”¨ distinct æˆ– group by è·å–ä¸é‡å¤çš„æ ‡ç­¾åç§°ï¼ŒæŒ‰ä½¿ç”¨æ¬¡æ•°é™åºæ’åˆ—
                db_tags = self.db.query(Tag.name).filter(Tag.name.isnot(None)).distinct().order_by(Tag.usage_count.desc()).limit(50).all()
                existing_tags = [tag.name for tag in db_tags if tag.name]
                logger.info(f"ä»æ•°æ®åº“è·å–åˆ° {len(existing_tags)} ä¸ªç°æœ‰æ ‡ç­¾")
            except Exception as e:
                logger.warning(f"è·å–æ•°æ®åº“æ ‡ç­¾å¤±è´¥: {e}")
            
            # 3. åˆå¹¶å€™é€‰æ ‡ç­¾ï¼Œå»é‡å¹¶ä¿æŒé¡ºåº
            candidate_tags = []
            seen = set()
            
            # å…ˆæ·»åŠ é¢„è®¾æ ‡ç­¾
            for tag in predefined_tags:
                if tag not in seen:
                    candidate_tags.append(tag)
                    seen.add(tag)
            
            # å†æ·»åŠ æ•°æ®åº“ä¸­çš„ç°æœ‰æ ‡ç­¾
            for tag in existing_tags:
                if tag not in seen:
                    candidate_tags.append(tag)
                    seen.add(tag)
            
            logger.info(f"æ€»å…±æœ‰ {len(candidate_tags)} ä¸ªå€™é€‰æ ‡ç­¾")
            
            # 4. å‡†å¤‡åˆ†æå†…å®¹ï¼ˆæ”¯æŒå¤šå±‚æ¬¡åˆ†æï¼‰
            analysis_content = self._prepare_content_for_tagging(title, content)
            
            # 5. æ„å»ºæç¤ºè¯ï¼Œè¦æ±‚ä»å€™é€‰æ ‡ç­¾ä¸­é€‰æ‹©
            candidate_tags_text = "ã€".join(candidate_tags)
            
            prompt = f"""è¯·ä»ä»¥ä¸‹å€™é€‰æ ‡ç­¾ä¸­é€‰æ‹©æœ€å¤š{max_tags}ä¸ªæœ€é€‚åˆçš„æ ‡ç­¾æ¥æ ‡è®°ä¸‹é¢çš„æ–‡æ¡£ã€‚

**å€™é€‰æ ‡ç­¾åˆ—è¡¨ï¼š**
{candidate_tags_text}

**æ–‡æ¡£ä¿¡æ¯ï¼š**
{analysis_content}

**è¦æ±‚ï¼š**
1. åªèƒ½ä»ä¸Šè¿°å€™é€‰æ ‡ç­¾åˆ—è¡¨ä¸­é€‰æ‹©ï¼Œä¸è¦åˆ›é€ æ–°æ ‡ç­¾
2. é€‰æ‹©æœ€ç›¸å…³çš„{max_tags}ä¸ªæ ‡ç­¾
3. æ ‡ç­¾è¦å‡†ç¡®åæ˜ æ–‡æ¡£çš„ä¸»è¦å†…å®¹å’Œç‰¹å¾
4. æ¯è¡Œè¿”å›ä¸€ä¸ªæ ‡ç­¾åç§°

**è¿”å›æ ¼å¼ï¼š**
è¯·åªè¿”å›é€‰ä¸­çš„æ ‡ç­¾ï¼Œæ¯è¡Œä¸€ä¸ªï¼š"""
            
            response = self.llm.invoke(prompt)
            tags_text = response.content.strip()
            
            # è§£ææ ‡ç­¾å¹¶éªŒè¯
            suggested_tags = [tag.strip() for tag in tags_text.split('\n') if tag.strip()]
            
            # è¿‡æ»¤ï¼šåªä¿ç•™åœ¨å€™é€‰æ ‡ç­¾ä¸­çš„æ ‡ç­¾
            valid_tags = []
            for tag in suggested_tags[:max_tags]:
                if tag in candidate_tags:
                    valid_tags.append(tag)
                else:
                    logger.warning(f"LLMè¿”å›äº†ä¸åœ¨å€™é€‰åˆ—è¡¨ä¸­çš„æ ‡ç­¾: {tag}")
            
            # å¦‚æœæœ‰æ•ˆæ ‡ç­¾å¤ªå°‘ï¼Œä»å€™é€‰æ ‡ç­¾ä¸­è¡¥å……ä¸€äº›é€šç”¨æ ‡ç­¾
            if len(valid_tags) < max_tags and len(valid_tags) < 3:
                # æ·»åŠ ä¸€äº›é€šç”¨çš„åå¤‡æ ‡ç­¾
                fallback_tags = ["ç¬”è®°", "æ–‡æ¡£", "é‡ç‚¹"]
                for fallback in fallback_tags:
                    if fallback in candidate_tags and fallback not in valid_tags and len(valid_tags) < max_tags:
                        valid_tags.append(fallback)
            
            logger.info(f"æ ‡ç­¾å»ºè®®ç”ŸæˆæˆåŠŸ: {valid_tags} (ä» {len(candidate_tags)} ä¸ªå€™é€‰æ ‡ç­¾ä¸­é€‰æ‹©)")
            return valid_tags
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ ‡ç­¾å»ºè®®å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            return []
    
    def _prepare_content_for_tagging(self, title: str, content: str) -> str:
        """ä¸ºæ ‡ç­¾ç”Ÿæˆå‡†å¤‡åˆ†æå†…å®¹"""
        if settings.enable_hierarchical_chunking:
            # å¤šå±‚æ¬¡æ¨¡å¼ï¼šæå–å…³é”®ä¿¡æ¯
            summary = self._generate_file_summary_for_linking(content, title)
            
            # æå–å¯èƒ½çš„ç« èŠ‚æ ‡é¢˜
            from .hierarchical_splitter import HierarchicalTextSplitter
            splitter = HierarchicalTextSplitter()
            structure = splitter._extract_document_structure(content)
            
            sections = []
            for item in structure[:5]:  # æœ€å¤š5ä¸ªç« èŠ‚
                sections.append(item.get('heading', ''))
            
            analysis_parts = [
                f"æ ‡é¢˜ï¼š{title}",
                f"æ–‡æ¡£æ‘˜è¦ï¼š{summary[:500]}",
            ]
            
            if sections:
                analysis_parts.append(f"ä¸»è¦ç« èŠ‚ï¼š{', '.join(sections)}")
            
            analysis_parts.append(f"å†…å®¹ç‰‡æ®µï¼š{content[:800]}")
            
            return "\n\n".join(analysis_parts)
        else:
            # ä¼ ç»Ÿæ¨¡å¼
            return f"æ ‡é¢˜ï¼š{title}\nå†…å®¹ï¼š{content[:1000]}"

    def analyze_content(self, content: str) -> Dict[str, Any]:
        """å†…å®¹åˆ†æ"""
        if not self.llm:
            logger.warning("LLMä¸å¯ç”¨ï¼Œæ— æ³•åˆ†æå†…å®¹")
            return {}
        
        try:
            prompt = f"""è¯·åˆ†æä»¥ä¸‹å†…å®¹ï¼Œæä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š
1. ä¸»è¦è¯é¢˜
2. å†…å®¹ç±»å‹ï¼ˆæŠ€æœ¯æ–‡æ¡£ã€ç¬”è®°ã€æ€»ç»“ç­‰ï¼‰
3. é‡è¦æ€§è¯„åˆ†ï¼ˆ1-10ï¼‰
4. å»ºè®®çš„å¤„ç†æ–¹å¼

å†…å®¹ï¼š
{content[:1500]}

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœã€‚"""
            
            response = self.llm.invoke(prompt)
            # è¿™é‡Œåº”è¯¥è§£æJSONå“åº”ï¼Œä¸ºç®€åŒ–ç›´æ¥è¿”å›æ–‡æœ¬
            
            analysis = {
                "raw_response": response.content,
                "analyzed": True
            }
            
            logger.info("å†…å®¹åˆ†æå®Œæˆ")
            return analysis
            
        except Exception as e:
            logger.error(f"å†…å®¹åˆ†æå¤±è´¥: {e}")
            return {}

    def generate_related_questions(self, content: str, num_questions: int = 3) -> List[str]:
        """ç”Ÿæˆç›¸å…³é—®é¢˜"""
        if not self.llm:
            logger.warning("LLMä¸å¯ç”¨ï¼Œæ— æ³•ç”Ÿæˆç›¸å…³é—®é¢˜")
            return []
        
        try:
            prompt = f"""åŸºäºä»¥ä¸‹å†…å®¹ï¼Œç”Ÿæˆ{num_questions}ä¸ªç›¸å…³çš„é—®é¢˜ï¼Œè¿™äº›é—®é¢˜åº”è¯¥èƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·æ›´æ·±å…¥åœ°ç†è§£å†…å®¹ï¼š

å†…å®¹ï¼š
{content[:1500]}

è¯·åªè¿”å›é—®é¢˜åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªé—®é¢˜ï¼š"""
            
            response = self.llm.invoke(prompt)
            questions_text = response.content.strip()
            
            # è§£æé—®é¢˜
            questions = [q.strip() for q in questions_text.split('\n') if q.strip()]
            questions = questions[:num_questions]  # é™åˆ¶æ•°é‡
            
            logger.info(f"ç›¸å…³é—®é¢˜ç”ŸæˆæˆåŠŸ: {questions}")
            return questions
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆç›¸å…³é—®é¢˜å¤±è´¥: {e}")
            return []

    def discover_smart_links(self, file_id: int, content: str, title: str) -> List[Dict[str, Any]]:
        """æ™ºèƒ½é“¾æ¥å‘ç° - æ”¯æŒå¤šå±‚æ¬¡é“¾æ¥å‘ç°"""
        if not self.llm:
            logger.warning("LLMä¸å¯ç”¨ï¼Œæ— æ³•å‘ç°æ™ºèƒ½é“¾æ¥")
            return []
        
        try:
            logger.info(f"å¼€å§‹æ™ºèƒ½é“¾æ¥å‘ç°: {title}")
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨å¤šå±‚æ¬¡æ¨¡å¼
            if settings.enable_hierarchical_chunking:
                return self._hierarchical_smart_links(file_id, content, title)
            else:
                return self._traditional_smart_links(file_id, content, title)
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½é“¾æ¥å‘ç°å¤±è´¥: {e}")
            return []
    
    def _traditional_smart_links(self, file_id: int, content: str, title: str) -> List[Dict[str, Any]]:
        """ä¼ ç»Ÿæ™ºèƒ½é“¾æ¥å‘ç°ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰"""
        try:
            # å…ˆé€šè¿‡è¯­ä¹‰æœç´¢æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ - æ™ºèƒ½é“¾æ¥ä½¿ç”¨æ›´é«˜çš„é˜ˆå€¼ç¡®ä¿é“¾æ¥è´¨é‡
            link_threshold = max(settings.semantic_search_threshold + 0.2, 0.6)  # è‡³å°‘0.6ï¼Œç¡®ä¿é“¾æ¥è´¨é‡
            related_results = self.semantic_search(content[:500], limit=5, similarity_threshold=link_threshold)
            
            if not related_results:
                logger.info("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œæ— æ³•ç”Ÿæˆæ™ºèƒ½é“¾æ¥")
                return []
            
            # æ„å»ºç›¸å…³æ–‡æ¡£ä¿¡æ¯
            files_text = ""
            for result in related_results:
                if result['file_id'] != file_id:  # æ’é™¤è‡ªå·±
                    files_text += f"æ–‡ä»¶ID: {result['file_id']}, æ ‡é¢˜: {result['title']}, è·¯å¾„: {result['file_path']}\n"
            
            if not files_text:
                logger.info("æ²¡æœ‰å…¶ä»–ç›¸å…³æ–‡æ¡£ï¼Œæ— æ³•ç”Ÿæˆæ™ºèƒ½é“¾æ¥")
                return []
            
            return self._generate_links_with_llm(file_id, content, title, files_text, related_results)
            
        except Exception as e:
            logger.error(f"ä¼ ç»Ÿæ™ºèƒ½é“¾æ¥å‘ç°å¤±è´¥: {e}")
            return []
    
    def _hierarchical_smart_links(self, file_id: int, content: str, title: str) -> List[Dict[str, Any]]:
        """å¤šå±‚æ¬¡æ™ºèƒ½é“¾æ¥å‘ç°"""
        try:
            logger.info(f"å¼€å§‹å¤šå±‚æ¬¡é“¾æ¥å‘ç°: {title}")
            
            # Step 1: ç”Ÿæˆå½“å‰æ–‡ä»¶çš„æ‘˜è¦ç”¨äºæ¯”è¾ƒ
            current_summary = self._generate_file_summary_for_linking(content, title)
            
            # Step 2: ä»æ‘˜è¦å±‚æœç´¢ç›¸å…³æ–‡ä»¶ï¼ˆæ–‡ä»¶çº§åˆ«çš„å…³è”ï¼‰
            summary_results = self._search_by_chunk_type(current_summary, "summary", 10, 0.8)
            
            # Step 3: ä»å¤§çº²å±‚æœç´¢ç›¸å…³ç« èŠ‚ï¼ˆç« èŠ‚çº§åˆ«çš„å…³è”ï¼‰
            outline_results = self._search_by_chunk_type(content[:800], "outline", 8, 0.7)
            
            # Step 4: æ™ºèƒ½é“¾æ¥åˆ†æ
            candidate_files = {}
            
            # å¤„ç†æ–‡ä»¶çº§åˆ«çš„å…³è”ï¼ˆæ‘˜è¦å±‚åŒ¹é…ï¼‰
            for result in summary_results:
                if result['file_id'] != file_id:
                    candidate_files[result['file_id']] = {
                        'file_id': result['file_id'],
                        'title': result['title'],
                        'file_path': result['file_path'],
                        'link_level': 'file',  # æ–‡ä»¶çº§åˆ«å…³è”
                        'similarity': result['similarity'],
                        'match_type': 'summary',
                        'match_content': result['chunk_text']
                    }
            
            # å¤„ç†ç« èŠ‚çº§åˆ«çš„å…³è”ï¼ˆå¤§çº²å±‚åŒ¹é…ï¼‰
            for result in outline_results:
                if result['file_id'] != file_id:
                    file_id_key = result['file_id']
                    if file_id_key in candidate_files:
                        # å¦‚æœå·²ç»æœ‰æ–‡ä»¶çº§åˆ«çš„å…³è”ï¼Œå‡çº§ä¸ºç« èŠ‚çº§åˆ«
                        candidate_files[file_id_key]['link_level'] = 'section'
                        candidate_files[file_id_key]['section_info'] = {
                            'section_path': result.get('section_path'),
                            'parent_heading': result.get('parent_heading'),
                            'section_similarity': result['similarity']
                        }
                    else:
                        # æ–°çš„ç« èŠ‚çº§åˆ«å…³è”
                        candidate_files[file_id_key] = {
                            'file_id': result['file_id'],
                            'title': result['title'],
                            'file_path': result['file_path'],
                            'link_level': 'section',
                            'similarity': result['similarity'],
                            'match_type': 'outline',
                            'match_content': result['chunk_text'],
                            'section_info': {
                                'section_path': result.get('section_path'),
                                'parent_heading': result.get('parent_heading'),
                                'section_similarity': result['similarity']
                            }
                        }
            
            if not candidate_files:
                logger.info("æœªæ‰¾åˆ°å€™é€‰å…³è”æ–‡ä»¶")
                return []
            
            # Step 5: æ„å»ºæ–‡ä»¶ä¿¡æ¯ç”¨äºLLMåˆ†æ
            files_info = []
            for file_info in candidate_files.values():
                if file_info['link_level'] == 'file':
                    files_info.append(f"æ–‡ä»¶ID: {file_info['file_id']}, æ ‡é¢˜: {file_info['title']}, è·¯å¾„: {file_info['file_path']}, å…³è”çº§åˆ«: æ–‡ä»¶çº§(æ•´ä½“ç›¸å…³), ç›¸ä¼¼åº¦: {file_info['similarity']:.2f}")
                elif file_info['link_level'] == 'section':
                    section_path = file_info.get('section_info', {}).get('section_path', 'æœªçŸ¥ç« èŠ‚')
                    files_info.append(f"æ–‡ä»¶ID: {file_info['file_id']}, æ ‡é¢˜: {file_info['title']}, è·¯å¾„: {file_info['file_path']}, å…³è”çº§åˆ«: ç« èŠ‚çº§({section_path}), ç›¸ä¼¼åº¦: {file_info['similarity']:.2f}")
            
            files_text = "\n".join(files_info)
            
            # Step 6: ä½¿ç”¨LLMç”Ÿæˆæ™ºèƒ½é“¾æ¥
            smart_links = self._generate_enhanced_links_with_llm(file_id, content, title, files_text, list(candidate_files.values()))
            
            logger.info(f"å¤šå±‚æ¬¡é“¾æ¥å‘ç°å®Œæˆ: æ‰¾åˆ° {len(smart_links)} ä¸ªæ™ºèƒ½é“¾æ¥")
            return smart_links
            
        except Exception as e:
            logger.error(f"å¤šå±‚æ¬¡æ™ºèƒ½é“¾æ¥å‘ç°å¤±è´¥: {e}")
            # é™çº§åˆ°ä¼ ç»Ÿæ–¹æ³•
            return self._traditional_smart_links(file_id, content, title)
    
    def _generate_file_summary_for_linking(self, content: str, title: str) -> str:
        """ä¸ºé“¾æ¥å‘ç°ç”Ÿæˆæ–‡ä»¶æ‘˜è¦"""
        # ç”Ÿæˆç®€æ´çš„æ–‡ä»¶æ‘˜è¦ç”¨äºæ–‡ä»¶çº§åˆ«çš„å…³è”åˆ¤æ–­
        summary_parts = [f"æ ‡é¢˜: {title}"]
        
        # æå–å‰å‡ æ®µé‡è¦å†…å®¹
        lines = content.split('\n')
        important_lines = []
        char_count = 0
        max_chars = 800
        
        for line in lines:
            line = line.strip()
            if line and char_count < max_chars:
                important_lines.append(line)
                char_count += len(line)
            if char_count >= max_chars:
                break
        
        summary_parts.extend(important_lines)
        return '\n'.join(summary_parts)
    
    def _generate_links_with_llm(self, file_id: int, content: str, title: str, files_text: str, related_results: List[Dict]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨LLMç”Ÿæˆä¼ ç»Ÿæ™ºèƒ½é“¾æ¥"""
        try:
            prompt = f"""å½“å‰æ–‡æ¡£ï¼š
æ ‡é¢˜ï¼š{title}
å†…å®¹ï¼š{content[:500]}

ç›¸å…³æ–‡æ¡£ï¼š
{files_text}

è¯·åˆ†æå½“å‰æ–‡æ¡£ä¸è¿™äº›ç›¸å…³æ–‡æ¡£ä¹‹é—´çš„å…³ç³»ç±»å‹ï¼Œå¹¶ä¸ºæ¯ä¸ªå»ºè®®çš„é“¾æ¥æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š
1. é“¾æ¥ç±»å‹ï¼ˆreference/related/follow_up/prerequisite/example/contradictionï¼‰
2. é“¾æ¥ç†ç”±ï¼ˆç®€çŸ­è¯´æ˜ä¸ºä»€ä¹ˆè¦å»ºç«‹è¿™ä¸ªé“¾æ¥ï¼‰
3. å»ºè®®çš„é“¾æ¥æ–‡æœ¬

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[
    {{
        "target_file_id": æ–‡ä»¶ID,
        "link_type": "é“¾æ¥ç±»å‹",
        "reason": "é“¾æ¥ç†ç”±",
        "suggested_text": "å»ºè®®çš„é“¾æ¥æ–‡æœ¬"
    }}
]

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼š"""
            
            response = self.llm.invoke(prompt)
            result_text = response.content.strip()
            
            # å°è¯•è§£æJSON
            import json
            try:
                smart_links = json.loads(result_text)
                logger.info(f"æ™ºèƒ½é“¾æ¥ç”ŸæˆæˆåŠŸ: {len(smart_links)} ä¸ªé“¾æ¥")
                return smart_links
            except json.JSONDecodeError as e:
                logger.error(f"è§£ææ™ºèƒ½é“¾æ¥JSONå¤±è´¥: {e}")
                return []
                
        except Exception as e:
            logger.error(f"LLMç”Ÿæˆé“¾æ¥å¤±è´¥: {e}")
            return []
    
    def _generate_enhanced_links_with_llm(self, file_id: int, content: str, title: str, files_text: str, candidate_files: List[Dict]) -> List[Dict[str, Any]]:
        """ä½¿ç”¨LLMç”Ÿæˆå¢å¼ºçš„å¤šå±‚æ¬¡æ™ºèƒ½é“¾æ¥"""
        try:
            prompt = f"""å½“å‰æ–‡æ¡£ï¼š
æ ‡é¢˜ï¼š{title}
å†…å®¹ï¼š{content[:600]}

å€™é€‰å…³è”æ–‡æ¡£ï¼ˆåŒ…å«å…³è”çº§åˆ«å’Œç›¸ä¼¼åº¦ï¼‰ï¼š
{files_text}

è¯·åŸºäºå¤šå±‚æ¬¡å…³è”åˆ†æï¼Œä¸ºæ¯ä¸ªå€™é€‰æ–‡æ¡£è¯„ä¼°æ˜¯å¦åº”è¯¥å»ºç«‹é“¾æ¥ï¼Œä»¥åŠé“¾æ¥çš„ç±»å‹å’Œå¼ºåº¦ã€‚

å…³è”çº§åˆ«è¯´æ˜ï¼š
- æ–‡ä»¶çº§ï¼šæ•´ä¸ªæ–‡æ¡£åœ¨ä¸»é¢˜æˆ–å†…å®¹ä¸Šç›¸å…³
- ç« èŠ‚çº§ï¼šç‰¹å®šç« èŠ‚æˆ–ä¸»é¢˜ç›¸å…³

è¯·ä¸ºæ¯ä¸ªå»ºè®®çš„é“¾æ¥æä¾›ï¼š
1. é“¾æ¥ç±»å‹ï¼ˆreference/related/follow_up/prerequisite/example/contradiction/complementï¼‰
2. é“¾æ¥å¼ºåº¦ï¼ˆstrong/medium/weakï¼‰
3. é“¾æ¥ç†ç”±ï¼ˆè¯¦ç»†è¯´æ˜å…³è”åŸå› å’Œå…³è”çº§åˆ«ï¼‰
4. å»ºè®®çš„é“¾æ¥æ–‡æœ¬
5. æ˜¯å¦æ¨èå»ºç«‹é“¾æ¥ï¼ˆtrue/falseï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
[
    {{
        "target_file_id": æ–‡ä»¶ID,
        "link_type": "é“¾æ¥ç±»å‹",
        "link_strength": "é“¾æ¥å¼ºåº¦",
        "reason": "é“¾æ¥ç†ç”±",
        "suggested_text": "å»ºè®®çš„é“¾æ¥æ–‡æœ¬",
        "recommended": trueæˆ–false
    }}
]

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ï¼š"""
            
            response = self.llm.invoke(prompt)
            result_text = response.content.strip()
            
            # å°è¯•è§£æJSON
            import json
            try:
                smart_links = json.loads(result_text)
                # åªè¿”å›æ¨èçš„é“¾æ¥
                recommended_links = [link for link in smart_links if link.get('recommended', False)]
                logger.info(f"å¢å¼ºæ™ºèƒ½é“¾æ¥ç”ŸæˆæˆåŠŸ: {len(recommended_links)} ä¸ªæ¨èé“¾æ¥ï¼ˆä» {len(smart_links)} ä¸ªå€™é€‰ä¸­ç­›é€‰ï¼‰")
                return recommended_links
            except json.JSONDecodeError as e:
                logger.error(f"è§£æå¢å¼ºæ™ºèƒ½é“¾æ¥JSONå¤±è´¥: {e}")
                return []
                
        except Exception as e:
            logger.error(f"LLMç”Ÿæˆå¢å¼ºé“¾æ¥å¤±è´¥: {e}")
            return []

    def _get_cached_query_embedding(self, query: str) -> List[float]:
        """è·å–ç¼“å­˜çš„æŸ¥è¯¢å‘é‡"""
        with self._cache_lock:
            query_hash = hashlib.md5(query.encode()).hexdigest()
            
            if query_hash in self._query_cache:
                logger.info(f"ä½¿ç”¨ç¼“å­˜çš„æŸ¥è¯¢å‘é‡: {query[:50]}...")
                return self._query_cache[query_hash]
            
            # ç”Ÿæˆæ–°çš„æŸ¥è¯¢å‘é‡
            embedding = self.embeddings.embed_query(query)
            
            # ç¼“å­˜ç®¡ç†ï¼šå¦‚æœç¼“å­˜è¿‡å¤§ï¼Œæ¸…ç†æœ€æ—§çš„æ¡ç›®
            if len(self._query_cache) >= self._max_cache_size:
                # ç®€å•çš„FIFOæ¸…ç†ç­–ç•¥
                oldest_key = next(iter(self._query_cache))
                del self._query_cache[oldest_key]
                logger.info(f"æŸ¥è¯¢å‘é‡ç¼“å­˜å·²æ»¡ï¼Œæ¸…ç†æœ€æ—§æ¡ç›®")
            
            self._query_cache[query_hash] = embedding
            logger.info(f"ç”Ÿæˆå¹¶ç¼“å­˜æ–°çš„æŸ¥è¯¢å‘é‡: {query[:50]}...")
            return embedding

    def _build_smart_prompt(self, question: str, context: str, messages: List[Dict] = None) -> str:
        """æ„å»ºæ™ºèƒ½æç¤ºè¯ï¼Œæ ¹æ®ä¸Šä¸‹æ–‡å†…å®¹å†³å®šç­–ç•¥ï¼Œé›†æˆç”¨æˆ·è®°å¿†"""
        # è·å–ç”¨æˆ·è®°å¿†ä½œä¸ºèƒŒæ™¯ä¿¡æ¯
        memory_context = ""
        try:
            memory_context = self.memory_service.format_memories_for_prompt(limit=8)
            if memory_context.strip():
                logger.info(f"ğŸ§  è®°å¿†æœåŠ¡æä¾›èƒŒæ™¯ä¿¡æ¯: {len(memory_context)} å­—ç¬¦")
                logger.info(f"ğŸ§  è®°å¿†å†…å®¹é¢„è§ˆ: {memory_context[:200]}...")
            else:
                logger.info("ğŸ§  è®°å¿†æœåŠ¡: æœªæ‰¾åˆ°ç›¸å…³è®°å¿†")
        except Exception as e:
            logger.warning(f"è·å–ç”¨æˆ·è®°å¿†å¤±è´¥: {e}")
        
        if messages and len(messages) > 1:
            # æœ‰å¯¹è¯å†å²çš„æƒ…å†µ
            if context.strip():
                return f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œæ‹¥æœ‰ç”¨æˆ·çš„å†å²è®°å¿†ï¼Œå¯ä»¥åŸºäºç”¨æˆ·ç¬”è®°å†…å®¹å›ç­”é—®é¢˜ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å·¥å…·è·å–å®æ—¶ä¿¡æ¯ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

{memory_context}ç›¸å…³æ–‡æ¡£å†…å®¹ï¼š
{context}

è¯·æ ¹æ®ä¸Šè¿°ä¿¡æ¯å’Œå¯¹è¯å†å²å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¦æ±‚ï¼š
1. ä¼˜å…ˆç»“åˆç”¨æˆ·è®°å¿†ä¿¡æ¯ï¼Œæä¾›ä¸ªæ€§åŒ–å›ç­”
2. åŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜
3. å¦‚æœæ–‡æ¡£å†…å®¹ä¸è¶³ï¼Œå¯ä»¥ä½¿ç”¨å¯ç”¨çš„å·¥å…·è·å–é¢å¤–ä¿¡æ¯
4. å›ç­”è¦å‡†ç¡®ã€æœ‰ç”¨ï¼Œç®€æ´æ˜äº†
5. å¦‚æœå¼•ç”¨æ–‡æ¡£å†…å®¹ï¼Œè¯·è¯´æ˜æ¥æº
6. ä¿æŒå¯¹è¯çš„è¿è´¯æ€§å’Œä¸ªæ€§åŒ–
7. æ ¹æ®ç”¨æˆ·çš„åå¥½å’Œä¹ æƒ¯è°ƒæ•´å›ç­”é£æ ¼"""
            else:
                return f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å›ç­”å„ç§é—®é¢˜å¹¶ä½¿ç”¨å·¥å…·è·å–å®æ—¶ä¿¡æ¯ã€‚å½“å‰æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„ç¬”è®°å†…å®¹ï¼Œä½†ä½ å¯ä»¥ä½¿ç”¨å¯ç”¨çš„å·¥å…·æ¥å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

{memory_context}è¯·æ ¹æ®å¯¹è¯å†å²å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¦æ±‚ï¼š
1. å¦‚æœé—®é¢˜éœ€è¦å®æ—¶ä¿¡æ¯ï¼ˆå¦‚å¤©æ°”ã€åœ°å›¾ç­‰ï¼‰ï¼Œè¯·ä½¿ç”¨ç›¸åº”çš„å·¥å…·
2. å›ç­”è¦å‡†ç¡®ã€æœ‰ç”¨ï¼Œç®€æ´æ˜äº†
3. å¦‚æœä½¿ç”¨å·¥å…·è·å–ä¿¡æ¯ï¼Œè¯·æ•´åˆå·¥å…·ç»“æœæä¾›å®Œæ•´å›ç­”
4. è¯·ç»“åˆä¹‹å‰çš„å¯¹è¯å†å²ï¼Œä¿æŒå¯¹è¯çš„è¿è´¯æ€§
5. å¦‚æœæ— æ³•é€šè¿‡å·¥å…·è·å–æ‰€éœ€ä¿¡æ¯ï¼Œè¯·è¯šå®è¯´æ˜
6. æ ¹æ®ç”¨æˆ·èƒŒæ™¯ä¿¡æ¯æä¾›ä¸ªæ€§åŒ–å›ç­”"""
        else:
            # æ²¡æœ‰å¯¹è¯å†å²çš„æƒ…å†µ
            if context.strip():
                return f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥åŸºäºç”¨æˆ·ç¬”è®°å†…å®¹å›ç­”é—®é¢˜ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨å·¥å…·è·å–å®æ—¶ä¿¡æ¯ã€‚è¯·æ ¹æ®ä»¥ä¸‹ç›¸å…³æ–‡æ¡£å†…å®¹æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{question}

{memory_context}ç›¸å…³æ–‡æ¡£å†…å®¹ï¼š
{context}

è¯·æ ¹æ®ä¸Šè¿°æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¦æ±‚ï¼š
1. ä¼˜å…ˆåŸºäºæä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”
2. å¦‚æœæ–‡æ¡£å†…å®¹ä¸è¶³ä»¥å®Œå…¨å›ç­”é—®é¢˜ï¼Œå¯ä»¥ä½¿ç”¨å¯ç”¨çš„å·¥å…·è·å–é¢å¤–ä¿¡æ¯
3. å›ç­”è¦å‡†ç¡®ã€æœ‰ç”¨ï¼Œç®€æ´æ˜äº†
4. å¦‚æœå¼•ç”¨æ–‡æ¡£å†…å®¹ï¼Œè¯·è¯´æ˜æ¥æº
5. å¦‚æœä½¿ç”¨å·¥å…·è·å–ä¿¡æ¯ï¼Œè¯·æ•´åˆå·¥å…·ç»“æœæä¾›å®Œæ•´å›ç­”
6. æ ¹æ®ç”¨æˆ·èƒŒæ™¯ä¿¡æ¯æä¾›ä¸ªæ€§åŒ–å›ç­”

å›ç­”ï¼š"""
            else:
                return f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥å›ç­”å„ç§é—®é¢˜å¹¶ä½¿ç”¨å·¥å…·è·å–å®æ—¶ä¿¡æ¯ã€‚å½“å‰æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„ç¬”è®°å†…å®¹ï¼Œä½†ä½ å¯ä»¥ä½¿ç”¨å¯ç”¨çš„å·¥å…·æ¥å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{question}

{memory_context}è¯·å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚è¦æ±‚ï¼š
1. å¦‚æœé—®é¢˜éœ€è¦å®æ—¶ä¿¡æ¯ï¼ˆå¦‚å¤©æ°”ã€åœ°å›¾ç­‰ï¼‰ï¼Œè¯·ä½¿ç”¨ç›¸åº”çš„å·¥å…·
2. å›ç­”è¦å‡†ç¡®ã€æœ‰ç”¨ï¼Œç®€æ´æ˜äº†
3. å¦‚æœä½¿ç”¨å·¥å…·è·å–ä¿¡æ¯ï¼Œè¯·æ•´åˆå·¥å…·ç»“æœæä¾›å®Œæ•´å›ç­”
4. å¦‚æœæ— æ³•é€šè¿‡å·¥å…·è·å–æ‰€éœ€ä¿¡æ¯ï¼Œè¯·è¯šå®è¯´æ˜

å›ç­”ï¼š"""

    def _extract_memories_from_conversation_async(self, question: str, answer: str, source: str) -> None:
        """å¼‚æ­¥ä»å¯¹è¯ä¸­æå–è®°å¿†ä¿¡æ¯"""
        def _extract_memories():
            try:
                # å¤„ç†å¯¹è¯å¹¶æ›´æ–°è®°å¿†
                result = self.memory_service.process_conversation(question, answer)
                
                if result.get("status") == "success":
                    logger.info(f"ä»å¯¹è¯ä¸­è‡ªåŠ¨æ›´æ–°è®°å¿†: {result.get('old_count')} -> {result.get('new_count')} æ¡è®°å¿†")
                else:
                    logger.warning(f"å¯¹è¯è®°å¿†å¤„ç†å¤±è´¥: {result.get('message', 'æœªçŸ¥é”™è¯¯')}")
                
            except Exception as e:
                logger.error(f"æå–å¯¹è¯è®°å¿†å¤±è´¥: {e}")
        
        # å¼‚æ­¥æ‰§è¡Œè®°å¿†æå–ï¼Œä¸é˜»å¡ä¸»çº¿ç¨‹
        try:
            executor = ThreadPoolExecutor(max_workers=1)
            executor.submit(_extract_memories)
            logger.debug("è®°å¿†æå–ä»»åŠ¡å·²æäº¤åˆ°åå°æ‰§è¡Œ")
        except Exception as e:
            logger.error(f"æäº¤è®°å¿†æå–ä»»åŠ¡å¤±è´¥: {e}")

    def chat_with_context(self, question: str, max_context_length: int = 3000, search_limit: int = 5, enable_tools: bool = True, messages: List[Dict] = None) -> Dict[str, Any]:
        """åŸºäºçŸ¥è¯†åº“å†…å®¹çš„æ™ºèƒ½é—®ç­”"""
        if not self.is_available():
            logger.warning("AI service not available")
            return {"error": "AI service not available"}
        
        try:
            start_time = time.time()
            logger.info(f"Starting chat with context: {question}")
            
            # è·å–ç›¸å…³æ–‡æ¡£
            context_results = self._hierarchical_context_search(question, search_limit)
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context = self._build_context_from_results(context_results, max_context_length)
            
            # æ„å»ºæç¤ºè¯
            prompt = self._build_smart_prompt(question, context, messages)
            
            # è·å–LLMå›ç­”
            response = self.llm.invoke([HumanMessage(content=prompt)])
            
            # å¼‚æ­¥æå–è®°å¿†ï¼ˆä¸é˜»å¡å“åº”ï¼‰
            self._extract_memories_from_conversation_async(question, response.content, "chat_with_context")
            
            total_time = time.time() - start_time
            logger.info(f"Chat completed in {total_time:.3f}s")
            
            return {
                "answer": response.content,
                "related_documents": context_results,
                "search_query": question,
                "context_length": len(context),
                "processing_time": round(total_time, 3),
                "tools_used": 0
            }
            
        except Exception as e:
            logger.error(f"Chat failed: {e}")
            return {"error": str(e)}

    def direct_chat(self, question: str, messages: List[Dict] = None) -> Dict[str, Any]:
        """Non-streaming direct chat for quick responses"""
        if not self.is_available():
            logger.warning("AI service not available")
            return {"error": "AI service not available"}
        
        try:
            start_time = time.time()
            logger.info(f"Starting direct chat: {question}")
            
            # Build message history  
            if messages and len(messages) > 1:
                conversation_history = []
                for msg in messages[:-1]:
                    if msg.get('role') == 'user':
                        conversation_history.append(HumanMessage(content=msg.get('content', '')))
                    elif msg.get('role') == 'assistant':
                        conversation_history.append(AIMessage(content=msg.get('content', '')))
                
                full_messages = conversation_history + [HumanMessage(content=question)]
            else:
                full_messages = [HumanMessage(content=question)]
            
            # Get response from LLM
            response = self.llm.invoke(full_messages)
            
            # å¼‚æ­¥æå–è®°å¿†ï¼ˆä¸é˜»å¡å“åº”ï¼‰
            self._extract_memories_from_conversation_async(question, response.content, "direct_chat")
            
            total_time = time.time() - start_time
            logger.info(f"Direct chat completed in {total_time:.3f}s")
            
            return {
                "answer": response.content,
                "related_documents": [],
                "search_query": question,
                "context_length": 0,
                "processing_time": round(total_time, 3),
                "tools_used": 0
            }
            
        except Exception as e:
            logger.error(f"Direct chat failed: {e}")
            return {"error": str(e)}

    def _hierarchical_context_search(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """å±‚æ¬¡åŒ–ä¸Šä¸‹æ–‡æœç´¢ - åŸºäºè¯­ä¹‰æœç´¢çš„å°è£…"""
        try:
            # ä½¿ç”¨ç°æœ‰çš„å±‚æ¬¡åŒ–è¯­ä¹‰æœç´¢
            similarity_threshold = settings.semantic_search_threshold
            return self._hierarchical_semantic_search(query, limit, similarity_threshold)
        except Exception as e:
            logger.error(f"å±‚æ¬¡åŒ–ä¸Šä¸‹æ–‡æœç´¢å¤±è´¥: {e}")
            return []

    def _build_context_from_results(self, search_results: List[Dict[str, Any]], max_length: int = 3000) -> str:
        """ä»æœç´¢ç»“æœæ„å»ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
        if not search_results:
            logger.info("ğŸ”§ æ„å»ºä¸Šä¸‹æ–‡: æ— æœç´¢ç»“æœï¼Œè¿”å›ç©ºä¸Šä¸‹æ–‡")
            return ""
        
        logger.info(f"ğŸ”§ å¼€å§‹æ„å»ºä¸Šä¸‹æ–‡: {len(search_results)} ä¸ªæœç´¢ç»“æœ, æœ€å¤§é•¿åº¦: {max_length}")
        
        context_parts = []
        current_length = 0
        included_count = 0
        
        for i, result in enumerate(search_results, 1):
            content = result.get('chunk_text', result.get('content', ''))
            file_path = result.get('file_path', 'Unknown')
            chunk_type = result.get('chunk_type', 'content')
            
            # æ ¼å¼åŒ–ç»“æœç‰‡æ®µ
            if chunk_type == "summary":
                formatted_content = f"[æ‘˜è¦ - {file_path}]\n{content}\n"
            elif chunk_type == "outline":
                formatted_content = f"[å¤§çº² - {file_path}]\n{content}\n"
            else:
                formatted_content = f"[å†…å®¹ - {file_path}]\n{content}\n"
            
            # æ£€æŸ¥é•¿åº¦é™åˆ¶
            if current_length + len(formatted_content) > max_length:
                logger.info(f"   ç‰‡æ®µ {i}: é•¿åº¦è¶…é™ ({current_length + len(formatted_content)} > {max_length}), åœæ­¢æ·»åŠ ")
                break
                
            context_parts.append(formatted_content)
            current_length += len(formatted_content)
            included_count += 1
            
            logger.info(f"   ç‰‡æ®µ {i}: [{chunk_type}] {file_path} - {len(formatted_content)} å­—ç¬¦ (ç´¯è®¡: {current_length})")
            logger.info(f"     å†…å®¹: {content[:100]}..." if len(content) > 100 else f"     å†…å®¹: {content}")
        
        final_context = "\n".join(context_parts)
        logger.info(f"ğŸ¯ ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆ: åŒ…å« {included_count}/{len(search_results)} ä¸ªç‰‡æ®µ, æ€»é•¿åº¦: {len(final_context)} å­—ç¬¦")
        logger.info(f"ğŸ“„ æœ€ç»ˆä¸Šä¸‹æ–‡é¢„è§ˆ:\n{final_context[:300]}..." if len(final_context) > 300 else f"ğŸ“„ æœ€ç»ˆä¸Šä¸‹æ–‡:\n{final_context}")
        
        return final_context

    def create_memory_from_chat(self, content: str, memory_type: str = "fact", 
                              category: str = "personal", importance_score: float = 0.5) -> bool:
        """æ‰‹åŠ¨åˆ›å»ºèŠå¤©è®°å¿†"""
        try:
            success = self.memory_service.add_manual_memory(content, memory_type, importance_score)
            logger.info(f"æ‰‹åŠ¨åˆ›å»ºèŠå¤©è®°å¿†æˆåŠŸ")
            return success
            
        except Exception as e:
            logger.error(f"åˆ›å»ºèŠå¤©è®°å¿†å¤±è´¥: {e}")
            return False

    async def direct_chat_streaming(self, question: str, messages: List[Dict] = None) -> AsyncGenerator[Dict[str, Any], None]:
        """ç›´æ¥èŠå¤© - æµå¼å“åº”ç‰ˆæœ¬"""
        if not self.is_available():
            logger.warning("AI service not available")
            yield {"error": "AI service not available"}
            return

        try:
            # æ„å»ºæç¤ºè¯
            prompt = self._build_smart_prompt(question, "", messages)
            
            # æ„å»ºæ¶ˆæ¯å†å²
            chat_history = []
            if messages:
                for msg in messages:
                    chat_history.append({"role": msg["role"], "content": msg["content"]})
            
            # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
            chat_history.insert(0, {"role": "system", "content": prompt})
            
            # æ·»åŠ ç”¨æˆ·é—®é¢˜
            chat_history.append({"role": "user", "content": question})
            
            # è°ƒç”¨LangChainæµå¼èŠå¤©
            from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
            
            langchain_messages = []
            for msg in chat_history:
                if msg["role"] == "system":
                    langchain_messages.append(SystemMessage(content=msg["content"]))
                elif msg["role"] == "user":
                    langchain_messages.append(HumanMessage(content=msg["content"]))
                elif msg["role"] == "assistant":
                    langchain_messages.append(AIMessage(content=msg["content"]))
            
            # ä½¿ç”¨LangChainçš„streamingæ–¹å¼
            collected_messages = []
            for chunk in self.streaming_llm.stream(langchain_messages):
                if hasattr(chunk, 'content') and chunk.content:
                    content = chunk.content
                    collected_messages.append(content)
                    yield {"chunk": content}
            
            # å¼‚æ­¥æå–è®°å¿†ï¼ˆä¸é˜»å¡å“åº”ï¼‰
            full_response = "".join(collected_messages)
            self._extract_memories_from_conversation_async(question, full_response, "direct_chat")
            
        except Exception as e:
            logger.error(f"ç›´æ¥èŠå¤©å‡ºé”™: {e}")
            yield {"error": f"å¯¹è¯å‡ºé”™: {str(e)}"}

    async def streaming_chat_with_context(self, question: str, max_context_length: int = 3000, search_limit: int = 5, enable_tools: bool = True, messages: List[Dict] = None) -> AsyncGenerator[Dict[str, Any], None]:
        """åŸºäºçŸ¥è¯†åº“å†…å®¹çš„æ™ºèƒ½é—®ç­” - æµå¼å“åº”ç‰ˆæœ¬"""
        if not self.is_available():
            logger.warning("AI service not available")
            yield {"error": "AI service not available"}
            return

        try:
            start_time = time.time()
            logger.info(f"Starting streaming chat with context: {question}")
            
            # è·å–ç›¸å…³æ–‡æ¡£
            context_results = self._hierarchical_context_search(question, search_limit)
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context = self._build_context_from_results(context_results, max_context_length)
            
            # æ„å»ºæç¤ºè¯
            prompt = self._build_smart_prompt(question, context, messages)
            
            # æ„å»ºæ¶ˆæ¯å†å²
            chat_history = []
            if messages:
                for msg in messages:
                    chat_history.append({"role": msg["role"], "content": msg["content"]})
            
            # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
            chat_history.insert(0, {"role": "system", "content": prompt})
            
            # æ·»åŠ ç”¨æˆ·é—®é¢˜
            chat_history.append({"role": "user", "content": question})
            
            # è°ƒç”¨LangChainæµå¼èŠå¤©
            from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
            
            langchain_messages = []
            for msg in chat_history:
                if msg["role"] == "system":
                    langchain_messages.append(SystemMessage(content=msg["content"]))
                elif msg["role"] == "user":
                    langchain_messages.append(HumanMessage(content=msg["content"]))
                elif msg["role"] == "assistant":
                    langchain_messages.append(AIMessage(content=msg["content"]))
            
            # ä½¿ç”¨LangChainçš„streamingæ–¹å¼
            collected_messages = []
            for chunk in self.streaming_llm.stream(langchain_messages):
                if hasattr(chunk, 'content') and chunk.content:
                    content = chunk.content
                    collected_messages.append(content)
                    yield {"chunk": content}
            
            # å¼‚æ­¥æå–è®°å¿†ï¼ˆä¸é˜»å¡å“åº”ï¼‰
            full_response = "".join(collected_messages)
            self._extract_memories_from_conversation_async(question, full_response, "chat_with_context")
            
            total_time = time.time() - start_time
            logger.info(f"Streaming chat completed in {total_time:.3f}s")
            
            # å‘é€æœ€ç»ˆä¿¡æ¯
            yield {
                "related_documents": context_results,
                "search_query": question,
                "context_length": len(context),
                "processing_time": round(total_time, 3),
                "tools_used": 0,
                "finished": True
            }
            
        except Exception as e:
            logger.error(f"Streaming chat failed: {e}")
            yield {"error": str(e)}
    
    def get_document_summary_and_outline(self, file_id: int) -> Optional[Dict[str, Any]]:
        """è·å–æ–‡æ¡£çš„æ€»ç»“å’Œæçº²"""
        try:
            # ä»æ•°æ®åº“è·å–æ–‡ä»¶ä¿¡æ¯
            file = self.db.query(File).filter(
                File.id == file_id,
                File.is_deleted == False
            ).first()
            
            if not file:
                logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨æˆ–å·²åˆ é™¤: file_id={file_id}")
                return None
            
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨å±‚æ¬¡åŒ–åˆ†å—
            if settings.enable_hierarchical_chunking:
                # å°è¯•ä»åµŒå…¥æ•°æ®ä¸­è·å–æ‘˜è¦å’Œæçº²
                embeddings = self.db.query(Embedding).filter(
                    Embedding.file_id == file_id
                ).all()
                
                summary = None
                outline = []
                
                for embedding in embeddings:
                    if embedding.chunk_type == "summary":
                        summary = embedding.chunk_text
                    elif embedding.chunk_type == "outline":
                        outline.append(embedding.chunk_text)
                
                if summary and outline:
                    return {
                        "summary": summary,
                        "outline": outline,
                        "source": "cached"
                    }
            
            # å¦‚æœæ²¡æœ‰ç¼“å­˜çš„æ‘˜è¦å’Œæçº²ï¼ŒåŠ¨æ€ç”Ÿæˆ
            if not self.is_available():
                logger.warning("AIæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•ç”Ÿæˆæ–‡æ¡£æ‘˜è¦å’Œæçº²")
                return None
            
            # ç”Ÿæˆæ‘˜è¦
            summary = self.generate_summary(file.content, max_length=300)
            
            # ç”Ÿæˆæçº²
            outline_items = self.generate_outline(file.content, max_items=8)
            
            if summary and outline_items:
                return {
                    "summary": summary,
                    "outline": outline_items,
                    "source": "generated"
                }
            
            return None
            
        except Exception as e:
            logger.error(f"è·å–æ–‡æ¡£æ‘˜è¦å’Œæçº²å¤±è´¥: {e}")
            return None
