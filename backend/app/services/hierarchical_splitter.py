"""
æ™ºèƒ½å¤šå±‚æ¬¡åˆ†å—å™¨ - åŸºäºLLMçš„æ–‡æ¡£åˆ†æå’Œåˆ†å—ç­–ç•¥
æ”¯æŒï¼š1.LLMç”Ÿæˆæ‘˜è¦å±‚ 2.LLMæå–å¤§çº²å±‚ 3.æ™ºèƒ½å†…å®¹å±‚
å¯¹è¶…é•¿æ–‡æ¡£ä½¿ç”¨"åˆ†è€Œæ²»ä¹‹"(Divide and Conquer)ç­–ç•¥
"""
import logging
import re
from typing import List, Dict, Any, Optional, Tuple
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
import hashlib
import math

from ..config import settings

logger = logging.getLogger(__name__)

class IntelligentHierarchicalSplitter:
    """åŸºäºLLMçš„æ™ºèƒ½å¤šå±‚æ¬¡æ–‡æœ¬åˆ†å—å™¨"""
    
    def __init__(self, llm=None):
        self.llm = llm  # LLMå®ä¾‹ï¼Œä»AIServiceä¼ å…¥
        self.summary_max_length = settings.hierarchical_summary_max_length
        self.outline_max_depth = settings.hierarchical_outline_max_depth
        self.content_target_size = settings.hierarchical_content_target_size
        self.content_max_size = settings.hierarchical_content_max_size
        self.content_overlap = settings.hierarchical_content_overlap
        
        # LLMå¤„ç†ç›¸å…³é…ç½®
        self.llm_context_window = settings.llm_context_window
        self.chunk_for_llm = settings.chunk_for_llm_processing
        self.max_refine_chunks = settings.max_chunks_for_refine
        
        # ç”¨äºé¢„åˆ†å—çš„æ–‡æœ¬åˆ†å‰²å™¨
        self.pre_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_for_llm,
            chunk_overlap=200,
            length_function=len,
        )
        
        # ç”¨äºæœ€ç»ˆå†…å®¹åˆ†å—çš„åˆ†å‰²å™¨
        self.content_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.content_target_size,
            chunk_overlap=self.content_overlap,
            length_function=len,
        )
    
    def split_document(self, content: str, title: str, file_id: int, progress_callback=None) -> Dict[str, List[Document]]:
        """
        æ™ºèƒ½å¤šå±‚æ¬¡æ–‡æ¡£åˆ†å—
        
        Args:
            content: æ–‡æ¡£å†…å®¹
            title: æ–‡æ¡£æ ‡é¢˜
            file_id: æ–‡ä»¶ID
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            åŒ…å«ä¸‰ä¸ªå±‚æ¬¡æ–‡æ¡£çš„å­—å…¸
        """
        try:
            logger.info(f"å¼€å§‹æ™ºèƒ½å¤šå±‚æ¬¡åˆ†å—ï¼Œæ–‡ä»¶: {title}, é•¿åº¦: {len(content)} å­—ç¬¦")
            
            if not self.llm:
                logger.warning("LLMä¸å¯ç”¨ï¼Œé™çº§åˆ°ç®€å•åˆ†å—")
                if progress_callback:
                    progress_callback("é™çº§å¤„ç†", "LLMä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•åˆ†å—")
                return self._fallback_to_simple_chunking(content, title, file_id)
            
            # åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†è€Œæ²»ä¹‹ç­–ç•¥
            needs_divide_conquer = len(content) > self.llm_context_window * 0.8  # é¢„ç•™20%ç©ºé—´ç»™prompt
            
            if needs_divide_conquer:
                logger.info(f"æ–‡æ¡£é•¿åº¦ {len(content)} è¶…è¿‡LLMçª—å£ï¼Œä½¿ç”¨åˆ†è€Œæ²»ä¹‹ç­–ç•¥")
                
                if progress_callback:
                    progress_callback("æ‘˜è¦ç”Ÿæˆ", "ä½¿ç”¨åˆ†è€Œæ²»ä¹‹ç­–ç•¥ç”Ÿæˆæ‘˜è¦")
                summary_docs = self._create_summary_with_divide_conquer(content, title, file_id, progress_callback)
                
                if progress_callback:
                    progress_callback("å¤§çº²æå–", "ä½¿ç”¨åˆ†è€Œæ²»ä¹‹ç­–ç•¥æå–å¤§çº²")
                outline_docs = self._create_outline_with_divide_conquer(content, title, file_id, progress_callback)
            else:
                logger.info(f"æ–‡æ¡£é•¿åº¦ {len(content)} é€‚ä¸­ï¼Œç›´æ¥ä½¿ç”¨LLMåˆ†æ")
                
                if progress_callback:
                    progress_callback("æ‘˜è¦ç”Ÿæˆ", "ç›´æ¥ä½¿ç”¨LLMç”Ÿæˆæ‘˜è¦")
                summary_docs = self._create_summary_direct(content, title, file_id, progress_callback)
                
                if progress_callback:
                    progress_callback("å¤§çº²æå–", "ç›´æ¥ä½¿ç”¨LLMæå–å¤§çº²")
                outline_docs = self._create_outline_direct(content, title, file_id, progress_callback)
            
            # Level 3: åŸºäºå¤§çº²çš„æ™ºèƒ½å†…å®¹åˆ†å—
            if progress_callback:
                progress_callback("æ™ºèƒ½åˆ†å—", "åŸºäºå¤§çº²è¿›è¡Œæ™ºèƒ½å†…å®¹åˆ†å—")
            
            logger.info("ğŸ§  å¼€å§‹ç¬¬ä¸‰å±‚ï¼šåŸºäºå¤§çº²çš„æ™ºèƒ½å†…å®¹åˆ†å—")
            content_docs = self._create_intelligent_content_layer(content, title, file_id, outline_docs, progress_callback)
            logger.info(f"âœ… ç¬¬ä¸‰å±‚å®Œæˆï¼Œå†…å®¹å±‚ç”Ÿæˆ: {len(content_docs)} ä¸ªæ–‡æ¡£")
            
            # ç»„è£…æœ€ç»ˆç»“æœ
            result = {
                'summary': summary_docs,
                'outline': outline_docs,
                'content': content_docs
            }
            
            # æœ€ç»ˆç»Ÿè®¡å’ŒéªŒè¯
            total_docs = len(summary_docs) + len(outline_docs) + len(content_docs)
            logger.info(f"ğŸ“Š æ™ºèƒ½åˆ†å—æœ€ç»ˆç»Ÿè®¡:")
            logger.info(f"  ğŸ“ æ‘˜è¦å±‚: {len(summary_docs)} ä¸ªæ–‡æ¡£")
            logger.info(f"  ğŸ“‹ å¤§çº²å±‚: {len(outline_docs)} ä¸ªæ–‡æ¡£") 
            logger.info(f"  ğŸ“„ å†…å®¹å±‚: {len(content_docs)} ä¸ªæ–‡æ¡£")
            logger.info(f"  ğŸ“Š æ€»è®¡: {total_docs} ä¸ªæ–‡æ¡£")
            
            # éªŒè¯ç»“æœçš„å®Œæ•´æ€§
            if total_docs == 0:
                logger.error("âŒ æ™ºèƒ½åˆ†å—æœ€ç»ˆç»“æœä¸ºç©ºï¼Œè¿™ä¸åº”è¯¥å‘ç”Ÿ")
                raise Exception("æ™ºèƒ½åˆ†å—ç»“æœä¸ºç©º")
            
            if len(summary_docs) == 0:
                logger.warning("âš ï¸ æ²¡æœ‰ç”Ÿæˆæ‘˜è¦æ–‡æ¡£")
            
            if len(content_docs) == 0:
                logger.warning("âš ï¸ æ²¡æœ‰ç”Ÿæˆå†…å®¹æ–‡æ¡£")
            
            logger.info(f"ğŸ‰ æ™ºèƒ½åˆ†å—å®Œæˆ: æ‘˜è¦={len(summary_docs)}, å¤§çº²={len(outline_docs)}, å†…å®¹={len(content_docs)}")
            return result
            
        except Exception as e:
            logger.error(f"æ™ºèƒ½åˆ†å—å¤±è´¥: {e}")
            # é™çº§åˆ°ç®€å•åˆ†å—
            if progress_callback:
                progress_callback("é”™è¯¯é™çº§", f"æ™ºèƒ½åˆ†å—å¤±è´¥: {str(e)}")
            return self._fallback_to_simple_chunking(content, title, file_id)
    
    def _create_summary_direct(self, content: str, title: str, file_id: int, progress_callback=None) -> List[Document]:
        """ç›´æ¥ä½¿ç”¨LLMç”Ÿæˆæ‘˜è¦ï¼ˆæ–‡æ¡£é•¿åº¦é€‚ä¸­æ—¶ï¼‰"""
        try:
            prompt = f"""è¯·ä¸ºä»¥ä¸‹æ–‡æ¡£ç”Ÿæˆä¸€ä¸ªé«˜è´¨é‡çš„æ‘˜è¦ï¼Œè¦æ±‚ï¼š
1. æ‘˜è¦åº”è¯¥å‡†ç¡®æ¦‚æ‹¬æ–‡æ¡£çš„ä¸»è¦å†…å®¹å’Œæ ¸å¿ƒè§‚ç‚¹
2. é•¿åº¦æ§åˆ¶åœ¨{self.summary_max_length}å­—ç¬¦ä»¥å†…
3. ä¿æŒé€»è¾‘æ¸…æ™°ã€ä¿¡æ¯å®Œæ•´
4. çªå‡ºæ–‡æ¡£çš„é‡ç‚¹å’Œç‰¹è‰²

æ–‡æ¡£æ ‡é¢˜ï¼š{title}
æ–‡æ¡£å†…å®¹ï¼š
{content}

æ‘˜è¦ï¼š"""
            
            response = self.llm.invoke(prompt)
            summary = response.content.strip()
            
            doc = Document(
                page_content=summary,
                metadata={
                    "file_id": file_id,
                    "chunk_type": "summary",
                    "chunk_level": 1,
                    "chunk_index": 0,
                    "title": title,
                    "chunk_hash": hashlib.sha256(summary.encode()).hexdigest(),
                    "parent_heading": None,
                    "section_path": "å…¨æ–‡æ‘˜è¦",
                    "generation_method": "direct_llm",
                    "vector_model": "hierarchical_summary"
                }
            )
            
            if progress_callback:
                progress_callback("æ‘˜è¦å®Œæˆ", f"æ‘˜è¦ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(summary)}å­—ç¬¦")
            
            logger.info(f"LLMç›´æ¥ç”Ÿæˆæ‘˜è¦æˆåŠŸï¼Œé•¿åº¦: {len(summary)}")
            return [doc]
            
        except Exception as e:
            logger.error(f"LLMç›´æ¥ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
            return []
    
    def _create_summary_with_divide_conquer(self, content: str, title: str, file_id: int, progress_callback=None) -> List[Document]:
        """ä½¿ç”¨åˆ†è€Œæ²»ä¹‹ç­–ç•¥ç”Ÿæˆæ‘˜è¦ï¼ˆè¶…é•¿æ–‡æ¡£ï¼‰"""
        try:
            logger.info("å¼€å§‹åˆ†è€Œæ²»ä¹‹æ‘˜è¦ç”Ÿæˆ")
            
            # 1. å°†æ–‡æ¡£åˆ†å—
            chunks = self.pre_splitter.split_text(content)
            logger.info(f"æ–‡æ¡£åˆ†ä¸º {len(chunks)} ä¸ªå—è¿›è¡Œå¤„ç†")
            
            if len(chunks) == 1:
                # å¦‚æœåªæœ‰ä¸€ä¸ªå—ï¼Œç›´æ¥å¤„ç†
                return self._create_summary_direct(content, title, file_id, progress_callback)
            
            # 2. ä½¿ç”¨Refineç­–ç•¥è¿­ä»£ç”Ÿæˆæ‘˜è¦
            current_summary = None
            
            for i, chunk in enumerate(chunks[:self.max_refine_chunks]):  # é™åˆ¶å¤„ç†å—æ•°
                if progress_callback:
                    progress_callback("åˆ†å—æ‘˜è¦", f"å¤„ç†ç¬¬ {i+1}/{min(len(chunks), self.max_refine_chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
                
                if current_summary is None:
                    # ç¬¬ä¸€ä¸ªå—ï¼šç”Ÿæˆåˆå§‹æ‘˜è¦
                    prompt = f"""è¯·ä¸ºä»¥ä¸‹æ–‡æ¡£ç‰‡æ®µç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„æ‘˜è¦ï¼Œè¿™æ˜¯ä¸€ä¸ªé•¿æ–‡æ¡£çš„ç¬¬ä¸€éƒ¨åˆ†ï¼š

æ–‡æ¡£æ ‡é¢˜ï¼š{title}
æ–‡æ¡£ç‰‡æ®µ (ç¬¬{i+1}éƒ¨åˆ†)ï¼š
{chunk}

æ‘˜è¦ï¼š"""
                else:
                    # åç»­å—ï¼šåŸºäºå·²æœ‰æ‘˜è¦è¿›è¡Œç²¾ç‚¼
                    prompt = f"""ä½ å·²æœ‰çš„æ‘˜è¦æ˜¯ï¼š
{current_summary}

ç°åœ¨è¯·é˜…è¯»ä»¥ä¸‹æ–°çš„æ–‡æ¡£ç‰‡æ®µï¼Œå¹¶å°†å…¶ä¿¡æ¯èå…¥åˆ°å·²æœ‰æ‘˜è¦ä¸­ï¼Œç”Ÿæˆä¸€ä¸ªæ›´å®Œæ•´ã€æ›´å‡†ç¡®çš„æ‘˜è¦ï¼š

æ–°çš„æ–‡æ¡£ç‰‡æ®µ (ç¬¬{i+1}éƒ¨åˆ†)ï¼š
{chunk}

è¦æ±‚ï¼š
1. ä¿ç•™åŸæ‘˜è¦ä¸­çš„é‡è¦ä¿¡æ¯
2. èå…¥æ–°ç‰‡æ®µçš„å…³é”®å†…å®¹
3. ç¡®ä¿æ‘˜è¦é€»è¾‘è¿è´¯ã€ä¿¡æ¯å®Œæ•´
4. æ§åˆ¶é•¿åº¦åœ¨{self.summary_max_length}å­—ç¬¦ä»¥å†…

æ›´æ–°åçš„æ‘˜è¦ï¼š"""
                
                response = self.llm.invoke(prompt)
                current_summary = response.content.strip()
                logger.info(f"å¤„ç†ç¬¬ {i+1} å—ï¼Œå½“å‰æ‘˜è¦é•¿åº¦: {len(current_summary)}")
            
            if current_summary:
                doc = Document(
                    page_content=current_summary,
                    metadata={
                        "file_id": file_id,
                        "chunk_type": "summary",
                        "chunk_level": 1,
                        "chunk_index": 0,
                        "title": title,
                        "chunk_hash": hashlib.sha256(current_summary.encode()).hexdigest(),
                        "parent_heading": None,
                        "section_path": "å…¨æ–‡æ‘˜è¦",
                        "generation_method": "divide_conquer_refine",
                        "processed_chunks": min(len(chunks), self.max_refine_chunks),
                        "vector_model": "hierarchical_summary"
                    }
                )
                
                logger.info(f"åˆ†è€Œæ²»ä¹‹æ‘˜è¦ç”ŸæˆæˆåŠŸï¼Œå¤„ç†äº† {min(len(chunks), self.max_refine_chunks)} ä¸ªå—")
                return [doc]
            
            return []
            
        except Exception as e:
            logger.error(f"åˆ†è€Œæ²»ä¹‹æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return []
    
    def _create_outline_direct(self, content: str, title: str, file_id: int, progress_callback=None) -> List[Document]:
        """ç›´æ¥ä½¿ç”¨LLMæå–å¤§çº²ï¼ˆæ–‡æ¡£é•¿åº¦é€‚ä¸­æ—¶ï¼‰"""
        try:
            prompt = f"""è¯·ä¸ºä»¥ä¸‹æ–‡æ¡£æå–è¯¦ç»†çš„å¤§çº²ç»“æ„ï¼Œè¦æ±‚ï¼š
1. è¯†åˆ«æ–‡æ¡£çš„å±‚æ¬¡ç»“æ„å’Œç« èŠ‚åˆ’åˆ†
2. æå–æ¯ä¸ªç« èŠ‚çš„æ ‡é¢˜å’Œä¸»è¦å†…å®¹ç‚¹
3. ä¿æŒé€»è¾‘å±‚æ¬¡æ¸…æ™°
4. å¦‚æœæ–‡æ¡£æ²¡æœ‰æ˜æ˜¾çš„ç« èŠ‚ç»“æ„ï¼Œè¯·æ ¹æ®å†…å®¹ä¸»é¢˜è‡ªè¡Œç»„ç»‡åˆç†çš„å¤§çº²
5. æ¯ä¸ªå¤§çº²é¡¹ç›®åº”è¯¥åŒ…å«è¶³å¤Ÿçš„ä¿¡æ¯ä»¥ä¾¿åç»­æ£€ç´¢

æ–‡æ¡£æ ‡é¢˜ï¼š{title}
æ–‡æ¡£å†…å®¹ï¼š
{content}

è¯·ä»¥ä»¥ä¸‹æ ¼å¼è¿”å›å¤§çº²ï¼Œæ¯è¡Œä¸€ä¸ªå¤§çº²é¡¹ç›®ï¼š
1. [ä¸€çº§æ ‡é¢˜]
   1.1 [äºŒçº§æ ‡é¢˜]
   1.2 [äºŒçº§æ ‡é¢˜]
2. [ä¸€çº§æ ‡é¢˜]
   2.1 [äºŒçº§æ ‡é¢˜]

å¤§çº²ï¼š"""
            
            response = self.llm.invoke(prompt)
            outline_text = response.content.strip()
            
            # è§£æå¤§çº²ä¸ºç‹¬ç«‹çš„æ–‡æ¡£
            outline_docs = self._parse_outline_to_documents(outline_text, title, file_id)
            
            if progress_callback:
                progress_callback("å¤§çº²å®Œæˆ", f"å¤§çº²æå–æˆåŠŸï¼Œç”Ÿæˆ {len(outline_docs)} ä¸ªå¤§çº²é¡¹ç›®")
            
            logger.info(f"LLMç›´æ¥æå–å¤§çº²æˆåŠŸï¼Œç”Ÿæˆ {len(outline_docs)} ä¸ªå¤§çº²é¡¹ç›®")
            return outline_docs
            
        except Exception as e:
            logger.error(f"LLMç›´æ¥æå–å¤§çº²å¤±è´¥: {e}")
            return []
    
    def _create_outline_with_divide_conquer(self, content: str, title: str, file_id: int, progress_callback=None) -> List[Document]:
        """ä½¿ç”¨åˆ†è€Œæ²»ä¹‹ç­–ç•¥æå–å¤§çº²ï¼ˆè¶…é•¿æ–‡æ¡£ï¼‰"""
        try:
            logger.info("å¼€å§‹åˆ†è€Œæ²»ä¹‹å¤§çº²æå–")
            
            # 1. å°†æ–‡æ¡£åˆ†å—
            chunks = self.pre_splitter.split_text(content)
            logger.info(f"æ–‡æ¡£åˆ†ä¸º {len(chunks)} ä¸ªå—è¿›è¡Œå¤§çº²æå–")
            
            if len(chunks) == 1:
                return self._create_outline_direct(content, title, file_id, progress_callback)
            
            # 2. ä½¿ç”¨Refineç­–ç•¥è¿­ä»£æ„å»ºå¤§çº²
            current_outline = None
            
            for i, chunk in enumerate(chunks[:self.max_refine_chunks]):
                if progress_callback:
                    progress_callback("åˆ†å—å¤§çº²", f"åˆ†æç¬¬ {i+1}/{min(len(chunks), self.max_refine_chunks)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
                
                if current_outline is None:
                    # ç¬¬ä¸€ä¸ªå—ï¼šç”Ÿæˆåˆå§‹å¤§çº²
                    prompt = f"""è¯·ä¸ºä»¥ä¸‹æ–‡æ¡£ç‰‡æ®µæå–å¤§çº²ç»“æ„ï¼Œè¿™æ˜¯ä¸€ä¸ªé•¿æ–‡æ¡£çš„ç¬¬ä¸€éƒ¨åˆ†ï¼š

æ–‡æ¡£æ ‡é¢˜ï¼š{title}
æ–‡æ¡£ç‰‡æ®µ (ç¬¬{i+1}éƒ¨åˆ†)ï¼š
{chunk}

è¯·æå–è¿™éƒ¨åˆ†çš„å¤§çº²ç»“æ„ï¼š"""
                else:
                    # åç»­å—ï¼šåŸºäºå·²æœ‰å¤§çº²è¿›è¡Œæ‰©å±•
                    prompt = f"""ä½ å·²æœ‰çš„å¤§çº²æ˜¯ï¼š
{current_outline}

ç°åœ¨è¯·é˜…è¯»ä»¥ä¸‹æ–°çš„æ–‡æ¡£ç‰‡æ®µï¼Œå¹¶å°†å…¶ç»“æ„ä¿¡æ¯èå…¥åˆ°å·²æœ‰å¤§çº²ä¸­ï¼š

æ–°çš„æ–‡æ¡£ç‰‡æ®µ (ç¬¬{i+1}éƒ¨åˆ†)ï¼š
{chunk}

è¦æ±‚ï¼š
1. ä¿ç•™åŸå¤§çº²çš„ç»“æ„
2. æ·»åŠ æ–°ç‰‡æ®µä¸­çš„ç« èŠ‚å’Œè¦ç‚¹
3. ç¡®ä¿å¤§çº²é€»è¾‘è¿è´¯ã€å±‚æ¬¡æ¸…æ™°
4. åˆå¹¶ç›¸ä¼¼çš„ç« èŠ‚ï¼Œé¿å…é‡å¤

æ›´æ–°åçš„å¤§çº²ï¼š"""
                
                response = self.llm.invoke(prompt)
                current_outline = response.content.strip()
                logger.info(f"å¤„ç†ç¬¬ {i+1} å—ï¼Œå½“å‰å¤§çº²é•¿åº¦: {len(current_outline)}")
            
            if current_outline:
                # è§£ææœ€ç»ˆå¤§çº²ä¸ºæ–‡æ¡£
                outline_docs = self._parse_outline_to_documents(current_outline, title, file_id, generation_method="divide_conquer_refine")
                
                if progress_callback:
                    progress_callback("å¤§çº²å®Œæˆ", f"åˆ†è€Œæ²»ä¹‹å¤§çº²æå–æˆåŠŸï¼Œç”Ÿæˆ {len(outline_docs)} ä¸ªå¤§çº²é¡¹ç›®")
                
                logger.info(f"åˆ†è€Œæ²»ä¹‹å¤§çº²æå–æˆåŠŸï¼Œç”Ÿæˆ {len(outline_docs)} ä¸ªå¤§çº²é¡¹ç›®")
                return outline_docs
            
            return []
            
        except Exception as e:
            logger.error(f"åˆ†è€Œæ²»ä¹‹å¤§çº²æå–å¤±è´¥: {e}")
            return []
    
    def _parse_outline_to_documents(self, outline_text: str, title: str, file_id: int, generation_method: str = "direct_llm") -> List[Document]:
        """å°†å¤§çº²æ–‡æœ¬è§£æä¸ºç‹¬ç«‹çš„æ–‡æ¡£"""
        try:
            lines = outline_text.split('\n')
            outline_docs = []
            current_level_1 = None
            
            for i, line in enumerate(lines):
                line = line.strip()
                if not line:
                    continue
                
                # ç®€å•çš„å¤§çº²è§£æé€»è¾‘
                if re.match(r'^\d+\.', line):  # ä¸€çº§æ ‡é¢˜ (1. 2. 3.)
                    current_level_1 = line
                    doc = Document(
                        page_content=line,
                        metadata={
                            "file_id": file_id,
                            "chunk_type": "outline",
                            "chunk_level": 2,
                            "chunk_index": i,
                            "title": title,
                            "chunk_hash": hashlib.sha256(line.encode()).hexdigest(),
                            "parent_heading": None,
                            "section_path": line,
                            "outline_level": 1,
                            "generation_method": generation_method,
                            "vector_model": "hierarchical_outline"
                        }
                    )
                    outline_docs.append(doc)
                    
                elif re.match(r'^\s+\d+\.\d+', line):  # äºŒçº§æ ‡é¢˜ (1.1 1.2)
                    doc = Document(
                        page_content=line,
                        metadata={
                            "file_id": file_id,
                            "chunk_type": "outline",
                            "chunk_level": 2,
                            "chunk_index": i,
                            "title": title,
                            "chunk_hash": hashlib.sha256(line.encode()).hexdigest(),
                            "parent_heading": current_level_1,
                            "section_path": f"{current_level_1} / {line.strip()}" if current_level_1 else line.strip(),
                            "outline_level": 2,
                            "generation_method": generation_method,
                            "vector_model": "hierarchical_outline"
                        }
                    )
                    outline_docs.append(doc)
                
                elif line and not re.match(r'^\s*$', line):  # å…¶ä»–éç©ºè¡Œ
                    doc = Document(
                        page_content=line,
                        metadata={
                            "file_id": file_id,
                            "chunk_type": "outline",
                            "chunk_level": 2,
                            "chunk_index": i,
                            "title": title,
                            "chunk_hash": hashlib.sha256(line.encode()).hexdigest(),
                            "parent_heading": current_level_1,
                            "section_path": f"{current_level_1} / {line.strip()}" if current_level_1 else line.strip(),
                            "outline_level": 3,
                            "generation_method": generation_method,
                            "vector_model": "hierarchical_outline"
                        }
                    )
                    outline_docs.append(doc)
            
            return outline_docs
            
        except Exception as e:
            logger.error(f"è§£æå¤§çº²å¤±è´¥: {e}")
            return []
    
    def _create_intelligent_content_layer(self, content: str, title: str, file_id: int, outline_docs: List[Document], progress_callback=None) -> List[Document]:
        """åˆ›å»ºæ™ºèƒ½å†…å®¹å±‚ï¼ˆåŸºäºå¤§çº²çš„è¯­ä¹‰åˆ†å—ï¼‰"""
        import time
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ”„ å¼€å§‹åˆ›å»ºæ™ºèƒ½å†…å®¹å±‚ - æ–‡ä»¶ID: {file_id}, æ ‡é¢˜: {title}")
            logger.info(f"ğŸ“„ å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            logger.info(f"ğŸ“‹ å¤§çº²æ–‡æ¡£æ•°é‡: {len(outline_docs) if outline_docs else 0}")
            
            # éªŒè¯è¾“å…¥å‚æ•°
            if not content or not content.strip():
                logger.error("âŒ å†…å®¹ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œæ™ºèƒ½åˆ†å—")
                return []
            
            if not outline_docs:
                # æ²¡æœ‰å¤§çº²æ—¶ï¼Œä½¿ç”¨é€’å½’å­—ç¬¦åˆ†å—
                logger.info("âš ï¸ æ²¡æœ‰å¤§çº²æ–‡æ¡£ï¼Œé™çº§ä½¿ç”¨é€’å½’å­—ç¬¦åˆ†å—")
                if progress_callback:
                    progress_callback("é€’å½’åˆ†å—", "æ²¡æœ‰å¤§çº²ï¼Œä½¿ç”¨åŸºæœ¬é€’å½’åˆ†å—")
                return self._recursive_chunk_content(content, title, file_id)
            
            # åŸºäºå¤§çº²è¿›è¡Œæ™ºèƒ½åˆ†å—
            logger.info(f"ğŸ§  åŸºäº {len(outline_docs)} ä¸ªå¤§çº²é¡¹ç›®è¿›è¡Œæ™ºèƒ½åˆ†å—")
            
            # è¾“å‡ºå¤§çº²æ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
            for i, outline_doc in enumerate(outline_docs[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                logger.info(f"  ğŸ“ å¤§çº² {i+1}: {outline_doc.page_content[:50]}...")
                logger.info(f"      ç« èŠ‚è·¯å¾„: {outline_doc.metadata.get('section_path', 'N/A')}")
            
            if progress_callback:
                progress_callback("æ™ºèƒ½åˆ†å—", f"åŸºäº {len(outline_docs)} ä¸ªå¤§çº²é¡¹ç›®è¿›è¡Œæ™ºèƒ½åˆ†å—")
            
            # ä½¿ç”¨å†…å®¹åˆ†å‰²å™¨è¿›è¡Œåˆ†å—
            logger.info("ğŸ”ª å¼€å§‹ä½¿ç”¨å†…å®¹åˆ†å‰²å™¨è¿›è¡Œåˆ†å—...")
            try:
                chunks = self.content_splitter.split_text(content)
                logger.info(f"âœ… åˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªå†…å®¹å—")
                
                # éªŒè¯åˆ†å—ç»“æœ
                if not chunks:
                    logger.error("âŒ åˆ†å—ç»“æœä¸ºç©ºï¼Œé™çº§åˆ°é€’å½’åˆ†å—")
                    return self._recursive_chunk_content(content, title, file_id)
                
                # ç»Ÿè®¡åˆ†å—ä¿¡æ¯
                total_chars = sum(len(chunk) for chunk in chunks)
                avg_length = total_chars / len(chunks) if chunks else 0
                logger.info(f"ğŸ“Š åˆ†å—ç»Ÿè®¡ - æ€»å­—ç¬¦æ•°: {total_chars}, å¹³å‡é•¿åº¦: {avg_length:.0f}, å—æ•°: {len(chunks)}")
                
            except Exception as e:
                logger.error(f"âŒ å†…å®¹åˆ†å‰²å™¨å¤±è´¥: {e}")
                logger.error(f"ğŸ“‹ å†…å®¹åˆ†å‰²å™¨é…ç½®: chunk_size={self.content_splitter.chunk_size}, overlap={self.content_splitter.chunk_overlap}")
                return self._recursive_chunk_content(content, title, file_id)
            
            # åˆ›å»ºå†…å®¹æ–‡æ¡£
            logger.info("ğŸ—ï¸ å¼€å§‹åˆ›å»ºå†…å®¹æ–‡æ¡£å¹¶åŒ¹é…å¤§çº²...")
            content_docs = []
            matched_outlines = 0
            
            for i, chunk in enumerate(chunks):
                try:
                    logger.info(f"ğŸ” å¤„ç†ç¬¬ {i+1}/{len(chunks)} ä¸ªå†…å®¹å— (é•¿åº¦: {len(chunk)} å­—ç¬¦)")
                    
                    # éªŒè¯å†…å®¹å—
                    if not chunk or not chunk.strip():
                        logger.warning(f"âš ï¸ ç¬¬ {i+1} ä¸ªå†…å®¹å—ä¸ºç©ºï¼Œè·³è¿‡")
                        continue
                    
                    # ä¸ºæ¯ä¸ªå†…å®¹å—æ‰¾åˆ°æœ€ç›¸å…³çš„å¤§çº²é¡¹ç›®
                    best_outline = self._find_best_outline_for_chunk(chunk, outline_docs)
                    
                    if best_outline:
                        matched_outlines += 1
                        logger.info(f"âœ… ä¸ºå†…å®¹å— {i+1} åŒ¹é…åˆ°å¤§çº²: {best_outline.get('section_path', 'N/A')}")
                    else:
                        logger.info(f"âš ï¸ å†…å®¹å— {i+1} æœªåŒ¹é…åˆ°ç›¸å…³å¤§çº²")
                    
                    # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
                    doc = Document(
                        page_content=chunk,
                        metadata={
                            "file_id": file_id,
                            "chunk_type": "content",
                            "chunk_level": 3,
                            "chunk_index": i,
                            "title": title,
                            "chunk_hash": hashlib.sha256(chunk.encode()).hexdigest(),
                            "parent_heading": best_outline.get('section_path') if best_outline else None,
                            "section_path": f"å†…å®¹å—-{i+1}",
                            "related_outline": best_outline.get('content') if best_outline else None,
                            "vector_model": "hierarchical_intelligent"
                        }
                    )
                    content_docs.append(doc)
                    
                    # æ¯10ä¸ªå—è¾“å‡ºä¸€æ¬¡è¿›åº¦
                    if (i + 1) % 10 == 0:
                        logger.info(f"ğŸ“ˆ è¿›åº¦: {i+1}/{len(chunks)} ä¸ªå†…å®¹å—å·²å¤„ç†")
                
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†ç¬¬ {i+1} ä¸ªå†…å®¹å—æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    import traceback
                    logger.error(f"ğŸ“‹ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                    continue
            
            # ç»Ÿè®¡ç»“æœ
            processing_time = time.time() - start_time
            logger.info(f"ğŸ“Š æ™ºèƒ½å†…å®¹åˆ†å—ç»Ÿè®¡:")
            logger.info(f"  âœ… æˆåŠŸåˆ›å»º: {len(content_docs)} ä¸ªå†…å®¹æ–‡æ¡£")
            logger.info(f"  ğŸ¯ å¤§çº²åŒ¹é…: {matched_outlines}/{len(content_docs)} ä¸ªå†…å®¹å—")
            logger.info(f"  â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
            logger.info(f"  ğŸ“Š åŒ¹é…ç‡: {matched_outlines/len(content_docs)*100:.1f}%")
            
            if progress_callback:
                progress_callback("åˆ†å—å®Œæˆ", f"æ™ºèƒ½å†…å®¹åˆ†å—å®Œæˆï¼Œç”Ÿæˆ {len(content_docs)} ä¸ªå†…å®¹å—")
            
            # éªŒè¯æœ€ç»ˆç»“æœ
            if not content_docs:
                logger.error("âŒ æ™ºèƒ½å†…å®¹åˆ†å—ç»“æœä¸ºç©ºï¼Œé™çº§åˆ°é€’å½’åˆ†å—")
                return self._recursive_chunk_content(content, title, file_id)
            
            logger.info(f"ğŸ‰ æ™ºèƒ½å†…å®¹åˆ†å—å®Œæˆï¼Œç”Ÿæˆ {len(content_docs)} ä¸ªå†…å®¹å—")
            return content_docs
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"âŒ æ™ºèƒ½å†…å®¹åˆ†å—å¤±è´¥ (è€—æ—¶: {processing_time:.2f}s): {e}")
            import traceback
            logger.error(f"ğŸ“‹ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            
            if progress_callback:
                progress_callback("é™çº§å¤„ç†", f"æ™ºèƒ½åˆ†å—å¤±è´¥: {str(e)}")
            
            # é™çº§åˆ°é€’å½’åˆ†å—
            logger.info("ğŸ”„ é™çº§åˆ°é€’å½’åˆ†å—å¤„ç†...")
            return self._recursive_chunk_content(content, title, file_id)
    
    def _find_best_outline_for_chunk(self, chunk: str, outline_docs: List[Document]) -> Optional[Dict[str, str]]:
        """ä¸ºå†…å®¹å—æ‰¾åˆ°æœ€ç›¸å…³çš„å¤§çº²é¡¹ç›®"""
        try:
            if not chunk or not chunk.strip():
                logger.warning("âš ï¸ è¾“å…¥çš„å†…å®¹å—ä¸ºç©ºï¼Œæ— æ³•åŒ¹é…å¤§çº²")
                return None
            
            if not outline_docs:
                logger.warning("âš ï¸ æ²¡æœ‰å¤§çº²æ–‡æ¡£å¯ä¾›åŒ¹é…")
                return None
            
            # æ”¹è¿›çš„åŒ¹é…ç®—æ³•ï¼šå¤šç»´åº¦åŒ¹é…
            best_match = None
            best_score = 0
            match_details = []
            
            # é¢„å¤„ç†å†…å®¹å—
            chunk_clean = self._clean_text_for_matching(chunk)
            chunk_keywords = self._extract_keywords(chunk_clean)
            
            logger.debug(f"ğŸ” å¼€å§‹åŒ¹é…å¤§çº²ï¼Œå†…å®¹å—é•¿åº¦: {len(chunk)} å­—ç¬¦")
            logger.debug(f"ğŸ”¤ æå–å…³é”®è¯: {list(chunk_keywords)[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ªå…³é”®è¯
            
            for i, outline_doc in enumerate(outline_docs):
                try:
                    outline_content = outline_doc.page_content
                    if not outline_content or not outline_content.strip():
                        logger.debug(f"âš ï¸ å¤§çº² {i+1} å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                        continue
                    
                    # é¢„å¤„ç†å¤§çº²å†…å®¹
                    outline_clean = self._clean_text_for_matching(outline_content)
                    outline_keywords = self._extract_keywords(outline_clean)
                    
                    # è®¡ç®—å¤šç»´åº¦åŒ¹é…å¾—åˆ†
                    score = self._calculate_match_score(chunk_clean, outline_clean, chunk_keywords, outline_keywords)
                    
                    match_details.append({
                        'outline_index': i,
                        'outline_content': outline_content[:50] + '...' if len(outline_content) > 50 else outline_content,
                        'section_path': outline_doc.metadata.get('section_path', 'N/A'),
                        'score': score,
                        'keywords_intersection': len(chunk_keywords & outline_keywords)
                    })
                    
                    if score > best_score:
                        best_score = score
                        best_match = {
                            'content': outline_content,
                            'section_path': outline_doc.metadata.get('section_path', ''),
                            'score': score,
                            'keywords_intersection': len(chunk_keywords & outline_keywords)
                        }
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ å¤„ç†å¤§çº² {i+1} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    continue
            
            # è¾“å‡ºåŒ¹é…è¯¦æƒ… (è°ƒè¯•æ¨¡å¼)
            if logger.isEnabledFor(logging.DEBUG):
                logger.debug("ğŸ” å¤§çº²åŒ¹é…è¯¦æƒ…:")
                sorted_details = sorted(match_details, key=lambda x: x['score'], reverse=True)
                for detail in sorted_details[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªæœ€ä½³åŒ¹é…
                    logger.debug(f"  ğŸ“ å¤§çº²: {detail['outline_content']}")
                    logger.debug(f"      è·¯å¾„: {detail['section_path']}")
                    logger.debug(f"      å¾—åˆ†: {detail['score']:.3f}, å…³é”®è¯äº¤é›†: {detail['keywords_intersection']}")
            
            # è®¾ç½®åŒ¹é…é˜ˆå€¼
            min_score_threshold = 0.1  # æœ€ä½åŒ¹é…å¾—åˆ†
            
            if best_match and best_score >= min_score_threshold:
                logger.debug(f"âœ… æ‰¾åˆ°æœ€ä½³åŒ¹é… - ç« èŠ‚: {best_match['section_path']}")
                logger.debug(f"    ç»¼åˆå¾—åˆ†: {best_match['score']:.3f}, å…³é”®è¯äº¤é›†: {best_match['keywords_intersection']}")
                return best_match
            else:
                logger.debug(f"âŒ æœªæ‰¾åˆ°æ»¡è¶³é˜ˆå€¼({min_score_threshold})çš„åŒ¹é…ï¼Œæœ€é«˜å¾—åˆ†: {best_score:.3f}")
                return None
            
        except Exception as e:
            logger.error(f"âŒ å¤§çº²åŒ¹é…è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            logger.error(f"ğŸ“‹ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return None
    
    def _clean_text_for_matching(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ç”¨äºåŒ¹é…"""
        import re
        # ç§»é™¤markdownæ ‡è®°ã€ç‰¹æ®Šç¬¦å·å’Œå¤šä½™ç©ºæ ¼
        text = re.sub(r'[#*\-\[\](){}ã€Œã€ã€Šã€‹\'""]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def _extract_keywords(self, text: str) -> set:
        """æå–å…³é”®è¯ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰"""
        import re
        # æå–ä¸­æ–‡è¯æ±‡ï¼ˆ2-4ä¸ªå­—ç¬¦ï¼‰å’Œè‹±æ–‡å•è¯
        chinese_words = re.findall(r'[\u4e00-\u9fff]{2,4}', text)
        english_words = re.findall(r'[a-zA-Z]{2,}', text.lower())
        
        # è¿‡æ»¤å¸¸ç”¨è¯
        stop_words = {
            # ä¸­æ–‡å¸¸ç”¨è¯
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'å’Œ', 'ä¸', 'æˆ–', 'ç­‰', 'åŠ', 'ä»¥', 'ä¸º', 'æœ‰', 'æ— ', 'å¯', 'èƒ½', 'è¦', 'ç”¨',
            'è¿™', 'é‚£', 'å¯¹', 'ä¸­', 'ä¸', 'ä¹Ÿ', 'å°±', 'éƒ½', 'è€Œ', 'ç„¶', 'ä½†', 'å› ', 'æ‰€', 'ä¼š', 'åˆ°', 'è¯´', 'å¾ˆ',
            'å…¶', 'å¦‚', 'ç”±', 'æ—¶', 'ä¸Š', 'ä¸‹', 'å†…', 'å¤–', 'å‰', 'å', 'å·¦', 'å³', 'å¤§', 'å°', 'å¤š', 'å°‘',
            # è‹±æ–‡å¸¸ç”¨è¯
            'the', 'of', 'and', 'in', 'to', 'for', 'with', 'on', 'at', 'by', 'from', 'is', 'are', 'was', 'were',
            'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may',
            'might', 'must', 'can', 'this', 'that', 'these', 'those', 'a', 'an', 'or', 'but', 'if', 'then'
        }
        
        # åˆå¹¶å…³é”®è¯å¹¶è¿‡æ»¤
        keywords = set()
        for word in chinese_words + english_words:
            if word.lower() not in stop_words and len(word) >= 2:
                keywords.add(word.lower())
        
        return keywords
    
    def _calculate_match_score(self, chunk_text: str, outline_text: str, chunk_keywords: set, outline_keywords: set) -> float:
        """è®¡ç®—åŒ¹é…å¾—åˆ†"""
        import re
        scores = []
        
        # 1. å…³é”®è¯é‡å å¾—åˆ†
        if chunk_keywords and outline_keywords:
            intersection = chunk_keywords & outline_keywords
            union = chunk_keywords | outline_keywords
            jaccard_score = len(intersection) / len(union) if union else 0
            scores.append(jaccard_score * 0.4)  # æƒé‡0.4
        
        # 2. æ–‡æœ¬åŒ…å«å…³ç³»å¾—åˆ†
        contains_score = 0
        outline_lower = outline_text.lower()
        chunk_lower = chunk_text.lower()
        
        # æ£€æŸ¥å¤§çº²å…³é”®è¯åœ¨å†…å®¹ä¸­çš„å‡ºç°æƒ…å†µ
        outline_important_words = [word for word in outline_keywords if len(word) >= 2]
        if outline_important_words:
            found_words = sum(1 for word in outline_important_words if word in chunk_lower)
            contains_score = found_words / len(outline_important_words)
            scores.append(contains_score * 0.3)  # æƒé‡0.3
        
        # 3. é•¿åº¦ç›¸ä¼¼æ€§å¾—åˆ†ï¼ˆé¿å…æç«¯é•¿åº¦å·®å¼‚ï¼‰
        len_chunk = len(chunk_text)
        len_outline = len(outline_text)
        if len_chunk > 0 and len_outline > 0:
            len_ratio = min(len_chunk, len_outline) / max(len_chunk, len_outline)
            # å¯¹äºå¤§çº²é€šå¸¸è¾ƒçŸ­ï¼Œè°ƒæ•´é•¿åº¦ç›¸ä¼¼æ€§è®¡ç®—
            if len_outline < len_chunk * 0.1:  # å¤§çº²å¾ˆçŸ­
                length_score = 0.5  # ç»™ä¸€ä¸ªä¸­ç­‰åˆ†æ•°
            else:
                length_score = len_ratio
            scores.append(length_score * 0.2)  # æƒé‡0.2
        
        # 4. ç‰¹æ®ŠåŒ¹é…ï¼šåŒ»å­¦æœ¯è¯­å’Œæ•°å­—æ ‡è¯†
        special_score = 0
        # æŸ¥æ‰¾åŒ»å­¦ç›¸å…³æœ¯è¯­
        medical_terms = re.findall(r'[\u4e00-\u9fff]{2,}(?:ç—‡|ç—…|æ²»|ç–—|æ–¹|è¯|æ±¤|æ•£|ä¸¸|è†)', chunk_text + outline_text)
        if medical_terms:
            special_score = 0.1  # åŒ»å­¦æ–‡æ¡£é¢å¤–åŠ åˆ†
        
        # æŸ¥æ‰¾ç« èŠ‚æ ‡è¯†
        if re.search(r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+[ã€ï¼.]', outline_text):
            special_score += 0.1  # ç« èŠ‚æ ‡è¯†åŠ åˆ†
        
        scores.append(special_score * 0.1)  # æƒé‡0.1
        
        # ç»¼åˆå¾—åˆ†
        final_score = sum(scores)
        return final_score
    
    def _recursive_chunk_content(self, content: str, title: str, file_id: int) -> List[Document]:
        """é€’å½’åˆ†å—å†…å®¹ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰"""
        import time
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ”„ å¼€å§‹é€’å½’åˆ†å—å†…å®¹ - æ–‡ä»¶ID: {file_id}, æ ‡é¢˜: {title}")
            logger.info(f"ğŸ“„ å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            
            # éªŒè¯è¾“å…¥å‚æ•°
            if not content or not content.strip():
                logger.error("âŒ å†…å®¹ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œé€’å½’åˆ†å—")
                return []
            
            # è¿›è¡Œåˆ†å—
            logger.info("ğŸ”ª å¼€å§‹ä½¿ç”¨é€’å½’å­—ç¬¦åˆ†å‰²å™¨è¿›è¡Œåˆ†å—...")
            logger.info(f"ğŸ“‹ åˆ†å‰²å™¨é…ç½®: chunk_size={self.content_splitter.chunk_size}, overlap={self.content_splitter.chunk_overlap}")
            
            try:
                chunks = self.content_splitter.split_text(content)
                logger.info(f"âœ… é€’å½’åˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªå†…å®¹å—")
                
                # éªŒè¯åˆ†å—ç»“æœ
                if not chunks:
                    logger.error("âŒ é€’å½’åˆ†å—ç»“æœä¸ºç©º")
                    return []
                
                # ç»Ÿè®¡åˆ†å—ä¿¡æ¯
                total_chars = sum(len(chunk) for chunk in chunks)
                avg_length = total_chars / len(chunks) if chunks else 0
                min_length = min(len(chunk) for chunk in chunks) if chunks else 0
                max_length = max(len(chunk) for chunk in chunks) if chunks else 0
                
                logger.info(f"ğŸ“Š é€’å½’åˆ†å—ç»Ÿè®¡:")
                logger.info(f"  ğŸ“ æ€»å­—ç¬¦æ•°: {total_chars}")
                logger.info(f"  ğŸ“Š å¹³å‡é•¿åº¦: {avg_length:.0f} å­—ç¬¦")
                logger.info(f"  ğŸ“‰ æœ€å°é•¿åº¦: {min_length} å­—ç¬¦")
                logger.info(f"  ğŸ“ˆ æœ€å¤§é•¿åº¦: {max_length} å­—ç¬¦")
                logger.info(f"  ğŸ”¢ åˆ†å—æ•°é‡: {len(chunks)}")
                
            except Exception as e:
                logger.error(f"âŒ é€’å½’åˆ†å—è¿‡ç¨‹å¤±è´¥: {e}")
                import traceback
                logger.error(f"ğŸ“‹ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                return []
            
            # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
            logger.info("ğŸ—ï¸ å¼€å§‹åˆ›å»ºé€’å½’åˆ†å—æ–‡æ¡£...")
            content_docs = []
            
            for i, chunk in enumerate(chunks):
                try:
                    # éªŒè¯å†…å®¹å—
                    if not chunk or not chunk.strip():
                        logger.warning(f"âš ï¸ ç¬¬ {i+1} ä¸ªå†…å®¹å—ä¸ºç©ºï¼Œè·³è¿‡")
                        continue
                    
                    # åˆ›å»ºæ–‡æ¡£
                    doc = Document(
                        page_content=chunk,
                        metadata={
                            "file_id": file_id,
                            "chunk_type": "content",
                            "chunk_level": 3,
                            "chunk_index": i,
                            "title": title,
                            "chunk_hash": hashlib.sha256(chunk.encode()).hexdigest(),
                            "parent_heading": None,
                            "section_path": f"å†…å®¹å—-{i+1}",
                            "vector_model": "recursive_fallback"
                        }
                    )
                    content_docs.append(doc)
                    
                    # æ¯20ä¸ªå—è¾“å‡ºä¸€æ¬¡è¿›åº¦
                    if (i + 1) % 20 == 0:
                        logger.info(f"ğŸ“ˆ é€’å½’åˆ†å—è¿›åº¦: {i+1}/{len(chunks)} ä¸ªå†…å®¹å—å·²å¤„ç†")
                    
                except Exception as e:
                    logger.error(f"âŒ åˆ›å»ºç¬¬ {i+1} ä¸ªé€’å½’åˆ†å—æ–‡æ¡£æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    import traceback
                    logger.error(f"ğŸ“‹ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                    continue
            
            # æœ€ç»ˆç»Ÿè®¡
            processing_time = time.time() - start_time
            logger.info(f"ğŸ“Š é€’å½’åˆ†å—æœ€ç»ˆç»Ÿè®¡:")
            logger.info(f"  âœ… æˆåŠŸåˆ›å»º: {len(content_docs)} ä¸ªå†…å®¹æ–‡æ¡£")
            logger.info(f"  â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
            logger.info(f"  ğŸ“Š æˆåŠŸç‡: {len(content_docs)/len(chunks)*100:.1f}%")
            
            # éªŒè¯æœ€ç»ˆç»“æœ
            if not content_docs:
                logger.error("âŒ é€’å½’åˆ†å—æœ€ç»ˆç»“æœä¸ºç©º")
                return []
            
            logger.info(f"ğŸ‰ é€’å½’åˆ†å—å®Œæˆï¼Œç”Ÿæˆ {len(content_docs)} ä¸ªå†…å®¹å—")
            return content_docs
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"âŒ é€’å½’åˆ†å—è¿‡ç¨‹å¤±è´¥ (è€—æ—¶: {processing_time:.2f}s): {e}")
            import traceback
            logger.error(f"ğŸ“‹ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            return []
    
    def _fallback_to_simple_chunking(self, content: str, title: str, file_id: int) -> Dict[str, List[Document]]:
        """é™çº§åˆ°ç®€å•åˆ†å—ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰"""
        import time
        start_time = time.time()
        
        try:
            logger.warning("âš ï¸ å¼€å§‹é™çº§åˆ°ç®€å•åˆ†å—æ¨¡å¼")
            logger.warning(f"ğŸ“„ æ–‡ä»¶ID: {file_id}, æ ‡é¢˜: {title}")
            logger.warning(f"ğŸ“ å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            
            # éªŒè¯è¾“å…¥å‚æ•°
            if not content or not content.strip():
                logger.error("âŒ å†…å®¹ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œç®€å•åˆ†å—")
                return {
                    'summary': [],
                    'outline': [],
                    'content': []
                }
            
            # åˆ›å»ºç®€å•æ‘˜è¦å—
            logger.info("ğŸ“ å¼€å§‹åˆ›å»ºç®€å•æ‘˜è¦å—...")
            try:
                preview_length = min(500, len(content))
                simple_summary = f"æ ‡é¢˜ï¼š{title}\nå†…å®¹é¢„è§ˆï¼š{content[:preview_length]}..."
                
                if len(content) <= preview_length:
                    simple_summary = f"æ ‡é¢˜ï¼š{title}\nå®Œæ•´å†…å®¹ï¼š{content}"
                
                summary_doc = Document(
                    page_content=simple_summary,
                    metadata={
                        "file_id": file_id,
                        "chunk_type": "summary",
                        "chunk_level": 1,
                        "chunk_index": 0,
                        "title": title,
                        "chunk_hash": hashlib.sha256(simple_summary.encode()).hexdigest(),
                        "parent_heading": None,
                        "section_path": "ç®€å•æ‘˜è¦",
                        "generation_method": "fallback",
                        "vector_model": "simple_fallback"
                    }
                )
                
                logger.info(f"âœ… ç®€å•æ‘˜è¦å—åˆ›å»ºæˆåŠŸï¼Œé•¿åº¦: {len(simple_summary)} å­—ç¬¦")
                
            except Exception as e:
                logger.error(f"âŒ åˆ›å»ºç®€å•æ‘˜è¦å—å¤±è´¥: {e}")
                import traceback
                logger.error(f"ğŸ“‹ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                summary_doc = None
            
            # åˆ›å»ºå†…å®¹å—
            logger.info("ğŸ”„ å¼€å§‹åˆ›å»ºå†…å®¹å—...")
            try:
                content_docs = self._recursive_chunk_content(content, title, file_id)
                logger.info(f"âœ… å†…å®¹å—åˆ›å»ºå®Œæˆï¼Œå…± {len(content_docs)} ä¸ª")
                
            except Exception as e:
                logger.error(f"âŒ åˆ›å»ºå†…å®¹å—å¤±è´¥: {e}")
                import traceback
                logger.error(f"ğŸ“‹ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                content_docs = []
            
            # ç»„è£…ç»“æœ
            result = {
                'summary': [summary_doc] if summary_doc else [],
                'outline': [],  # ç®€å•åˆ†å—æ¨¡å¼ä¸æä¾›å¤§çº²
                'content': content_docs
            }
            
            # ç»Ÿè®¡ç»“æœ
            processing_time = time.time() - start_time
            total_docs = len(result['summary']) + len(result['outline']) + len(result['content'])
            
            logger.info(f"ğŸ“Š ç®€å•åˆ†å—æœ€ç»ˆç»Ÿè®¡:")
            logger.info(f"  ğŸ“ æ‘˜è¦å—: {len(result['summary'])} ä¸ª")
            logger.info(f"  ğŸ“‹ å¤§çº²å—: {len(result['outline'])} ä¸ª")
            logger.info(f"  ğŸ“„ å†…å®¹å—: {len(result['content'])} ä¸ª")
            logger.info(f"  ğŸ“Š æ€»æ–‡æ¡£æ•°: {total_docs} ä¸ª")
            logger.info(f"  â±ï¸ å¤„ç†æ—¶é—´: {processing_time:.2f} ç§’")
            
            # éªŒè¯æœ€ç»ˆç»“æœ
            if total_docs == 0:
                logger.error("âŒ ç®€å•åˆ†å—æœ€ç»ˆç»“æœä¸ºç©º")
                return {
                    'summary': [],
                    'outline': [],
                    'content': []
                }
            
            logger.info(f"ğŸ‰ ç®€å•åˆ†å—å®Œæˆï¼Œæ€»å…±ç”Ÿæˆ {total_docs} ä¸ªæ–‡æ¡£")
            return result
            
        except Exception as e:
            processing_time = time.time() - start_time
            logger.error(f"âŒ ç®€å•åˆ†å—è¿‡ç¨‹å¤±è´¥ (è€—æ—¶: {processing_time:.2f}s): {e}")
            import traceback
            logger.error(f"ğŸ“‹ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            
            # è¿”å›ç©ºç»“æœ
            return {
                'summary': [],
                'outline': [],
                'content': []
            }

# å‘åå…¼å®¹çš„ç±»ååˆ«å
HierarchicalTextSplitter = IntelligentHierarchicalSplitter 