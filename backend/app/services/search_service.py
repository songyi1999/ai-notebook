from typing import List, Dict, Any, Optional, Union
import logging
from sqlalchemy.orm import Session
from datetime import datetime
import time

from ..models.file import File
from ..models.search_history import SearchHistory
from ..services.file_service import FileService
from ..services.ai_service_langchain import AIService
from ..dynamic_config import settings

logger = logging.getLogger(__name__)

class SearchService:
    """æœç´¢æœåŠ¡ç±»ï¼Œç»Ÿä¸€ç®¡ç†æ‰€æœ‰æœç´¢åŠŸèƒ½"""
    
    def __init__(self, db: Session):
        self.db = db
        self.file_service = FileService(db)
        self.ai_service = AIService(db)
    
    def search(
        self,
        query: str,
        search_type: str = "mixed",
        limit: int = 50,
        similarity_threshold: float = None
    ) -> Dict[str, Any]:
        """
        ç»Ÿä¸€æœç´¢å…¥å£
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            search_type: æœç´¢ç±»å‹ ("keyword", "semantic", "mixed")
            limit: ç»“æœé™åˆ¶
            similarity_threshold: è¯­ä¹‰æœç´¢ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            æœç´¢ç»“æœå­—å…¸ï¼ŒåŒ…å«ç»“æœå’Œå…ƒæ•°æ®
        """
        start_time = time.time()
        
        # å¦‚æœæ²¡æœ‰ä¼ é€’ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤å€¼
        if similarity_threshold is None:
            similarity_threshold = settings.semantic_search_threshold
        
        try:
            degraded = False
            degradation_reason = None
            
            # æ£€æŸ¥AIå¯ç”¨æ€§å¹¶å¤„ç†é™çº§
            ai_available = self.ai_service.is_available()
            
            if search_type == "keyword":
                results = self._keyword_search(query, limit)
                result_type = "keyword"
            elif search_type == "semantic":
                if not ai_available:
                    # è¯­ä¹‰æœç´¢é™çº§ä¸ºå…³é”®è¯æœç´¢
                    logger.warning("AIæœåŠ¡ä¸å¯ç”¨ï¼Œè¯­ä¹‰æœç´¢é™çº§ä¸ºå…³é”®è¯æœç´¢")
                    results = self._keyword_search(query, limit)
                    result_type = "keyword"
                    degraded = True
                    degradation_reason = "AIåŠŸèƒ½å·²ç¦ç”¨" if not settings.is_ai_enabled() else "AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨"
                else:
                    results = self._semantic_search(query, limit, similarity_threshold)
                    result_type = "semantic"
            elif search_type == "mixed":
                if not ai_available:
                    # æ··åˆæœç´¢é™çº§ä¸ºå…³é”®è¯æœç´¢
                    logger.warning("AIæœåŠ¡ä¸å¯ç”¨ï¼Œæ··åˆæœç´¢é™çº§ä¸ºå…³é”®è¯æœç´¢")
                    results = self._keyword_search(query, limit)
                    result_type = "keyword"
                    degraded = True
                    degradation_reason = "AIåŠŸèƒ½å·²ç¦ç”¨" if not settings.is_ai_enabled() else "AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨"
                else:
                    results = self._mixed_search(query, limit, similarity_threshold)
                    result_type = "mixed"
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æœç´¢ç±»å‹: {search_type}")
            
            # è®¡ç®—å“åº”æ—¶é—´
            response_time = (time.time() - start_time) * 1000  # æ¯«ç§’
            
            # è®°å½•æœç´¢å†å²
            self._record_search_history(query, result_type, len(results), response_time)
            
            response_data = {
                "query": query,
                "search_type": result_type,
                "total": len(results),
                "results": results,
                "response_time_ms": round(response_time, 2)
            }
            
            # æ·»åŠ é™çº§ä¿¡æ¯
            if degraded:
                response_data["degraded"] = True
                response_data["degradation_reason"] = degradation_reason
                
            return response_data
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            response_time = (time.time() - start_time) * 1000
            self._record_search_history(query, search_type, 0, response_time)
            raise
    
    def _keyword_search(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """å…³é”®è¯æœç´¢ - è¿”å›æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶"""
        try:
            files = self.file_service.search_files(query, limit=limit)
            return [self._file_to_dict(file, "keyword") for file in files]
        except Exception as e:
            logger.error(f"å…³é”®è¯æœç´¢å¤±è´¥: {e}")
            return []
    
    def _semantic_search(self, query: str, limit: int, similarity_threshold: float) -> List[Dict[str, Any]]:
        """è¯­ä¹‰æœç´¢ - è¿”å›å‰Nä¸ªæœ€ç›¸å…³çš„æ–‡ä»¶ï¼Œå¹¶è‡ªåŠ¨è·å–å®Œæ•´ä¸Šä¸‹æ–‡"""
        try:
            if not self.ai_service.is_available():
                logger.warning("AIæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œè¯­ä¹‰æœç´¢")
                return []
            
            semantic_results = self.ai_service.semantic_search(
                query, limit, similarity_threshold
            )
            
            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼å¹¶å¢å¼ºä¸Šä¸‹æ–‡
            results = []
            for result in semantic_results:
                enhanced_result = {
                    "file_id": result["file_id"],
                    "file_path": result["file_path"],
                    "title": result["title"],
                    "content_preview": result["chunk_text"][:200] + "..." if len(result["chunk_text"]) > 200 else result["chunk_text"],
                    "search_type": "semantic",
                    "similarity": result["similarity"],
                    "chunk_index": result["chunk_index"],
                    "created_at": result["created_at"],
                    "updated_at": result["updated_at"]
                }
                
                # å¢å¼ºçŸ¥è¯†æ£€ç´¢ï¼šè·å–å®Œæ•´æ–‡æ¡£çš„æ€»ç»“å’Œæçº²
                logger.info(f"ğŸ” æ­£åœ¨è·å–æ–‡ä»¶ {result['file_id']} çš„å¢å¼ºä¸Šä¸‹æ–‡...")
                enhanced_context = self._get_enhanced_context(result["file_id"], result.get("chunk_text", ""))
                if enhanced_context:
                    enhanced_result["enhanced_context"] = enhanced_context
                    logger.info(f"âœ… è·å–åˆ°å¢å¼ºä¸Šä¸‹æ–‡ - ç±»å‹: {enhanced_context.get('chunk_type')}, ç­–ç•¥: {enhanced_context.get('enhancement_strategy')}")
                else:
                    logger.warning(f"âš ï¸ æ–‡ä»¶ {result['file_id']} æœªèƒ½è·å–å¢å¼ºä¸Šä¸‹æ–‡")
                
                results.append(enhanced_result)
            
            return results
        except Exception as e:
            logger.error(f"è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            return []
    
    def _mixed_search(self, query: str, limit: int, similarity_threshold: float) -> List[Dict[str, Any]]:
        """æ··åˆæœç´¢ - ç»“åˆå…³é”®è¯å’Œè¯­ä¹‰æœç´¢ç»“æœï¼ŒåŒ…å«å¢å¼ºä¸Šä¸‹æ–‡"""
        try:
            # å…³é”®è¯æœç´¢ - è·å–æ‰€æœ‰åŒ¹é…
            keyword_results = self._keyword_search(query, limit)
            
            # è¯­ä¹‰æœç´¢ - è·å–å‰10ä¸ªæœ€ç›¸å…³
            semantic_limit = min(10, limit)
            semantic_results = self._semantic_search(query, semantic_limit, similarity_threshold)
            
            # åˆå¹¶ç»“æœå¹¶å»é‡
            combined_results = self._merge_search_results(keyword_results, semantic_results)
            
            # é™åˆ¶æœ€ç»ˆç»“æœæ•°é‡
            return combined_results[:limit]
            
        except Exception as e:
            logger.error(f"æ··åˆæœç´¢å¤±è´¥: {e}")
            # å¦‚æœæ··åˆæœç´¢å¤±è´¥ï¼Œå›é€€åˆ°å…³é”®è¯æœç´¢
            return self._keyword_search(query, limit)
    
    def _merge_search_results(
        self, 
        keyword_results: List[Dict[str, Any]], 
        semantic_results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """åˆå¹¶æœç´¢ç»“æœå¹¶å»é‡"""
        merged = []
        seen_file_ids = set()
        
        # ä¼˜å…ˆæ·»åŠ è¯­ä¹‰æœç´¢ç»“æœï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰
        for result in semantic_results:
            if result["file_id"] not in seen_file_ids:
                merged.append(result)
                seen_file_ids.add(result["file_id"])
        
        # æ·»åŠ å…³é”®è¯æœç´¢ç»“æœï¼ˆå»é™¤é‡å¤ï¼‰
        for result in keyword_results:
            if result["file_id"] not in seen_file_ids:
                merged.append(result)
                seen_file_ids.add(result["file_id"])
        
        return merged
    
    def _file_to_dict(self, file: File, search_type: str) -> Dict[str, Any]:
        """å°†Fileå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        content_preview = file.content[:200] + "..." if len(file.content) > 200 else file.content
        
        # å®‰å…¨åœ°å¤„ç†datetimeå­—æ®µ
        def safe_datetime_to_iso(dt_field):
            if dt_field is None:
                return None
            if hasattr(dt_field, 'isoformat'):
                return dt_field.isoformat()
            return str(dt_field)  # å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›
        
        return {
            "file_id": file.id,
            "file_path": file.file_path,
            "title": file.title,
            "content_preview": content_preview,
            "search_type": search_type,
            "file_size": file.file_size,
            "created_at": safe_datetime_to_iso(file.created_at),
            "updated_at": safe_datetime_to_iso(file.updated_at),
            "tags": []  # æ ‡ç­¾ä¿¡æ¯ç°åœ¨é€šè¿‡file_tagså…³è”è¡¨è·å–
        }
    
    def _get_enhanced_context(self, file_id: int, chunk_text: str) -> Optional[Dict[str, Any]]:
        """è·å–å¢å¼ºçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ - åŒ…æ‹¬æ–‡æ¡£æ€»ç»“å’Œæçº²"""
        try:
            if not self.ai_service.is_available():
                return None
            
            # è·å–æ–‡æ¡£çš„æ€»ç»“å’Œæçº²
            summary_and_outline = self.ai_service.get_document_summary_and_outline(file_id)
            
            if not summary_and_outline:
                return None
            
            # æ ¹æ®æ£€ç´¢åˆ°çš„å†…å®¹ç±»å‹å†³å®šè¿”å›ç­–ç•¥
            chunk_type = self._detect_chunk_type(chunk_text)
            
            enhanced_context = {
                "chunk_type": chunk_type,
                "document_summary": summary_and_outline.get("summary", ""),
                "document_outline": summary_and_outline.get("outline", []),
                "enhancement_strategy": self._get_enhancement_strategy(chunk_type)
            }
            
            return enhanced_context
            
        except Exception as e:
            logger.error(f"è·å–å¢å¼ºä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return None
    
    def _detect_chunk_type(self, chunk_text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬å—ç±»å‹"""
        # ç®€å•çš„å¯å‘å¼æ£€æµ‹
        if len(chunk_text) < 100:
            return "outline"
        elif "æ€»ç»“" in chunk_text or "æ‘˜è¦" in chunk_text:
            return "summary"
        else:
            return "content"
    
    def _get_enhancement_strategy(self, chunk_type: str) -> str:
        """æ ¹æ®å—ç±»å‹è¿”å›å¢å¼ºç­–ç•¥"""
        if chunk_type == "content":
            return "æä¾›å®Œæ•´æ–‡æ¡£çš„æ€»ç»“å’Œæçº²ä»¥ä¸°å¯Œä¸Šä¸‹æ–‡"
        elif chunk_type == "summary":
            return "æä¾›å®Œæ•´æ–‡æ¡£çš„æçº²ä»¥è¡¥å……ç»“æ„ä¿¡æ¯"
        elif chunk_type == "outline":
            return "æä¾›å®Œæ•´æ–‡æ¡£çš„æ€»ç»“ä»¥è¡¥å……å†…å®¹æ¦‚è¿°"
        else:
            return "æä¾›å®Œæ•´æ–‡æ¡£çš„æ€»ç»“å’Œæçº²"
    
    def _record_search_history(
        self, 
        query: str, 
        search_type: str, 
        results_count: int, 
        response_time: float
    ) -> None:
        """è®°å½•æœç´¢å†å²"""
        try:
            search_history = SearchHistory(
                query=query,
                search_type=search_type,
                results_count=results_count,
                response_time=response_time
            )
            
            self.db.add(search_history)
            self.db.commit()
            
        except Exception as e:
            logger.error(f"è®°å½•æœç´¢å†å²å¤±è´¥: {e}")
            # ä¸å½±å“æœç´¢åŠŸèƒ½ï¼Œåªè®°å½•é”™è¯¯
    
    def get_search_history(self, limit: int = 20) -> List[Dict[str, Any]]:
        """è·å–æœç´¢å†å²"""
        try:
            histories = (
                self.db.query(SearchHistory)
                .order_by(SearchHistory.created_at.desc())
                .limit(limit)
                .all()
            )
            
            return [
                {
                    "id": h.id,
                    "query": h.query,
                    "search_type": h.search_type,
                    "results_count": h.results_count,
                    "response_time": h.response_time,
                    "created_at": h.created_at.isoformat() if h.created_at else None
                }
                for h in histories
            ]
            
        except Exception as e:
            logger.error(f"è·å–æœç´¢å†å²å¤±è´¥: {e}")
            return []
    
    def get_popular_queries(self, limit: int = 10) -> List[Dict[str, Any]]:
        """è·å–çƒ­é—¨æœç´¢æŸ¥è¯¢"""
        try:
            # ç®€å•ç»Ÿè®¡ï¼šæŒ‰æŸ¥è¯¢åˆ†ç»„ï¼Œè®¡ç®—å‡ºç°æ¬¡æ•°
            from sqlalchemy import func
            
            popular = (
                self.db.query(
                    SearchHistory.query,
                    func.count(SearchHistory.id).label('count'),
                    func.avg(SearchHistory.results_count).label('avg_results')
                )
                .group_by(SearchHistory.query)
                .order_by(func.count(SearchHistory.id).desc())
                .limit(limit)
                .all()
            )
            
            return [
                {
                    "query": p.query,
                    "search_count": p.count,
                    "avg_results": round(p.avg_results, 1)
                }
                for p in popular
            ]
            
        except Exception as e:
            logger.error(f"è·å–çƒ­é—¨æŸ¥è¯¢å¤±è´¥: {e}")
            return [] 