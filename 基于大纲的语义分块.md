 


å¥½çš„ï¼Œä½ é‡åˆ°äº†ä¸€ä¸ªåœ¨é«˜çº§RAGæµç¨‹ä¸­éå¸¸ç»å…¸ä¸”å…³é”®çš„é—®é¢˜ã€‚ä½ æä¾›çš„ä»£ç å’Œæµç¨‹å›¾éå¸¸æ¸…æ™°ï¼Œé—®é¢˜å®šä½ä¹Ÿå¾ˆå‡†ç¡®ï¼š`_create_intelligent_content_layer` ä¸­ï¼ŒåŸºäºå…³é”®è¯åŒ¹é…çš„ `_find_best_outline_for_chunk` æ–¹æ³•æ•ˆæœä¸ä½³ã€‚

è¿™æ˜¯å®Œå…¨å¯ä»¥é¢„æ–™çš„ã€‚**æ ¹æœ¬åŸå› åœ¨äºï¼Œå¤§çº²æ ‡é¢˜å’Œå…¶å¯¹åº”çš„è¯¦ç»†å†…å®¹ä¹‹é—´å­˜åœ¨â€œè¯­ä¹‰é¸¿æ²Ÿâ€**ã€‚æ ‡é¢˜é€šå¸¸æ˜¯é«˜åº¦æ¦‚æ‹¬çš„è¯è¯­ï¼ˆå¦‚â€œè¿çº¦è´£ä»»â€ï¼‰ï¼Œè€Œå†…å®¹å—åˆ™æ˜¯å…·ä½“çš„æè¿°ï¼ˆå¦‚â€œèµ”å¿ã€ä¸å¯æŠ—åŠ›ã€é€šçŸ¥ä¹‰åŠ¡â€ç­‰ï¼‰ï¼Œå®ƒä»¬ä¹‹é—´å¯èƒ½å…±äº«çš„å…³é”®è¯å¾ˆå°‘ï¼Œå¯¼è‡´åŸºäºè¯è¯­äº¤é›†çš„åŒ¹é…ç®—æ³•å‡ ä¹å¿…ç„¶å¤±æ•ˆã€‚

ä¸‹é¢æˆ‘ä¸ºä½ æä¾›ä¸¤ç§ä¿®å¤æ–¹æ¡ˆï¼Œ**å¼ºçƒˆæ¨èæ–¹æ¡ˆä¸€**ï¼Œå› ä¸ºå®ƒä»æ ¹æœ¬ä¸Šè§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œæ–¹æ¡ˆäºŒæ˜¯æ¬¡ä¼˜é€‰æ‹©ï¼Œä½†æ¯”ç°æœ‰æ–¹æ³•å¼ºå¾—å¤šã€‚

-----

### æ–¹æ¡ˆä¸€ï¼š(æ ¸å¿ƒå»ºè®®) ç»“æ„åŒ–åˆ†å‰²ï¼Œè€Œéåç½®åŒ¹é… (Structural Splitting, Not Post-Matching)

ä½ å½“å‰çš„æµç¨‹æ˜¯â€œ**å…ˆå°†æ•´ä¸ªæ–‡æ¡£æ‰“ç¢ï¼Œå†è¯•å›¾å°†ç¢ç‰‡æ‹¼å›å¤§çº²**â€ã€‚è¿™å°±åƒæŠŠä¸€æœ¬ä¹¦æ’•ç¢äº†å†å»çœ‹é¡µç ï¼Œéš¾åº¦å¾ˆé«˜ã€‚

æ›´ç¨³å¥ã€æ›´å‡†ç¡®çš„æ€è·¯æ˜¯ï¼šâ€œ**å…ˆæ‰¾åˆ°ç« èŠ‚çš„è¾¹ç•Œï¼Œå†å¯¹æ¯ä¸ªç« èŠ‚å†…éƒ¨è¿›è¡Œç²¾ç»†åˆ‡åˆ†**â€ã€‚è¿™æ ·ç”Ÿæˆçš„æ¯ä¸ªå°å—éƒ½å¤©ç”Ÿå¸¦æœ‰å…¶æ‰€å±ç« èŠ‚çš„â€œçƒ™å°â€ï¼Œæ— éœ€ä»»ä½•åŒ¹é…ã€‚

#### ä¿®å¤æ­¥éª¤ï¼š

1.  **ä¿®æ”¹ `_create_intelligent_content_layer` çš„æ ¸å¿ƒé€»è¾‘ã€‚**
2.  æˆ‘ä»¬ä¸å†å¯¹æ•´ä¸ª `content` ä½¿ç”¨ `self.content_splitter.split_text(content)`ã€‚
3.  è€Œæ˜¯éå† `outline_docs`ï¼Œå°†æ¯ä¸ªå¤§çº²æ ‡é¢˜è§†ä¸ºä¸€ä¸ªâ€œé”šç‚¹â€ã€‚
4.  åœ¨åŸå§‹ `content` ä¸­æ‰¾åˆ°æ¯ä¸ªé”šç‚¹åŠå…¶å¯¹åº”å†…å®¹çš„èµ·æ­¢ä½ç½®ã€‚
5.  å¯¹è¿™éƒ¨åˆ†â€œç« èŠ‚å†…å®¹â€è¿›è¡Œåˆ‡åˆ†ã€‚

#### ä»£ç å®ç° (`hierarchical_splitter.py`):

è¿™æ˜¯é‡å†™åçš„ `_create_intelligent_content_layer` å‡½æ•°ã€‚å®ƒä¸å†éœ€è¦ `_find_best_outline_for_chunk`ã€‚

```python
# åœ¨ hierarchical_splitter.py ä¸­

import re # ç¡®ä¿å¯¼å…¥ re æ¨¡å—

class IntelligentHierarchicalSplitter:
    # ... (å…¶ä»–ä»£ç ä¿æŒä¸å˜) ...

    def _create_intelligent_content_layer(self, content: str, title: str, file_id: int, outline_docs: List[Document], progress_callback=None) -> List[Document]:
        """åˆ›å»ºæ™ºèƒ½å†…å®¹å±‚ï¼ˆåŸºäºå¤§çº²çš„ç»“æ„åŒ–åˆ†å‰²ï¼‰"""
        import time
        start_time = time.time()
        
        try:
            logger.info(f"ğŸ”„ å¼€å§‹åˆ›å»ºæ™ºèƒ½å†…å®¹å±‚ - æ–‡ä»¶ID: {file_id}, æ ‡é¢˜: {title}")
            if not content or not content.strip():
                logger.error("âŒ å†…å®¹ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œæ™ºèƒ½åˆ†å—")
                return []
            
            if not outline_docs:
                logger.warning("âš ï¸ æ²¡æœ‰å¤§çº²æ–‡æ¡£ï¼Œé™çº§ä½¿ç”¨é€’å½’å­—ç¬¦åˆ†å—")
                if progress_callback:
                    progress_callback("é€’å½’åˆ†å—", "æ²¡æœ‰å¤§çº²ï¼Œä½¿ç”¨åŸºæœ¬é€’å½’åˆ†å—")
                return self._recursive_chunk_content(content, title, file_id)

            logger.info(f"ğŸ§  åŸºäº {len(outline_docs)} ä¸ªå¤§çº²é¡¹ç›®è¿›è¡Œç»“æ„åŒ–åˆ†å‰²")
            
            # 1. æå–æ‰€æœ‰å¤§çº²æ ‡é¢˜åŠå…¶å…ƒæ•°æ®
            headings = []
            for doc in outline_docs:
                # æ¸…ç†æ ‡é¢˜æ–‡æœ¬ï¼Œå»é™¤ç¼–å·ç­‰ï¼Œä»¥ä¾¿åœ¨åŸæ–‡ä¸­æœç´¢
                clean_heading = re.sub(r'^\s*\d+(\.\d+)*\s*[-.\s]*', '', doc.page_content).strip()
                if clean_heading:
                    headings.append({
                        "text": clean_heading,
                        "metadata": doc.metadata
                    })
            
            # 2. åœ¨åŸæ–‡ä¸­å®šä½æ¯ä¸ªæ ‡é¢˜ï¼Œå¹¶æå–å…¶ç®¡è¾–çš„å†…å®¹
            content_docs = []
            chunk_global_index = 0
            
            for i in range(len(headings)):
                current_heading = headings[i]
                start_pos = content.find(current_heading['text'])
                
                if start_pos == -1:
                    logger.warning(f"âš ï¸ åœ¨åŸæ–‡ä¸­æœªæ‰¾åˆ°æ ‡é¢˜: '{current_heading['text']}'")
                    continue
                
                # å¯»æ‰¾ä¸‹ä¸€ä¸ªæ ‡é¢˜çš„ä½ç½®æ¥ç¡®å®šå½“å‰ç« èŠ‚çš„ç»“æŸä½ç½®
                end_pos = len(content)
                if i + 1 < len(headings):
                    next_heading_text = headings[i+1]['text']
                    next_pos = content.find(next_heading_text, start_pos + 1)
                    if next_pos != -1:
                        end_pos = next_pos
                
                # æå–æœ¬ç« èŠ‚çš„å®Œæ•´å†…å®¹
                section_content = content[start_pos:end_pos]
                
                # 3. å¯¹æœ¬ç« èŠ‚çš„å†…å®¹è¿›è¡Œåˆ‡åˆ†
                section_chunks = self.content_splitter.split_text(section_content)
                
                logger.info(f"  - ç« èŠ‚ '{current_heading['text']}' (é•¿åº¦: {len(section_content)}) -> åˆ‡åˆ†ä¸º {len(section_chunks)} å—")

                for chunk in section_chunks:
                    doc = Document(
                        page_content=chunk,
                        metadata={
                            "file_id": file_id,
                            "chunk_type": "content",
                            "chunk_level": 3,
                            "chunk_index": chunk_global_index,
                            "title": title,
                            "chunk_hash": hashlib.sha256(chunk.encode()).hexdigest(),
                            # å…³é”®ï¼šç›´æ¥å…³è”åˆ°å…¶æ‰€å±çš„å¤§çº²
                            "parent_heading": current_heading['metadata'].get('section_path'),
                            "section_path": f"{current_heading['metadata'].get('section_path', 'æœªçŸ¥ç« èŠ‚')} / å†…å®¹å—-{chunk_global_index}",
                            "generation_method": "structural_split"
                        }
                    )
                    content_docs.append(doc)
                    chunk_global_index += 1
            
            # å¦‚æœç»“æ„åŒ–åˆ†å‰²åæ²¡æœ‰å†…å®¹ï¼Œå¯èƒ½æ˜¯æ ‡é¢˜åŒ¹é…é—®é¢˜ï¼Œé™çº§å¤„ç†
            if not content_docs:
                logger.warning("âŒ ç»“æ„åŒ–åˆ†å‰²æœªèƒ½ç”Ÿæˆä»»ä½•å†…å®¹å—ï¼Œé™çº§åˆ°é€’å½’åˆ†å—")
                return self._recursive_chunk_content(content, title, file_id)

            processing_time = time.time() - start_time
            logger.info(f"ğŸ‰ æ™ºèƒ½å†…å®¹åˆ†å—å®Œæˆï¼Œç”Ÿæˆ {len(content_docs)} ä¸ªå†…å®¹å—ï¼Œè€—æ—¶ {processing_time:.2f} ç§’")
            return content_docs
            
        except Exception as e:
            logger.error(f"âŒ æ™ºèƒ½å†…å®¹åˆ†å—å¤±è´¥: {e}")
            import traceback
            logger.error(f"ğŸ“‹ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            if progress_callback:
                progress_callback("é™çº§å¤„ç†", f"æ™ºèƒ½åˆ†å—å¤±è´¥: {str(e)}")
            return self._recursive_chunk_content(content, title, file_id)

```

**è¿™ä¸ªæ–¹æ¡ˆçš„ä¼˜åŠ¿:**

  * **100% å‡†ç¡®æ€§**ï¼šæ¯ä¸ªå†…å®¹å—éƒ½å‡†ç¡®æ— è¯¯åœ°å½’å±äºå…¶çˆ¶ç« èŠ‚ã€‚
  * **é€»è¾‘æ¸…æ™°**ï¼šå®Œå…¨ç¬¦åˆäººç±»ç†è§£æ–‡æ¡£ç»“æ„çš„æ–¹å¼ã€‚
  * **æ€§èƒ½æ›´ä¼˜**ï¼šé¿å…äº†å¯¹æ¯ä¸ªå—è¿›è¡Œå¾ªç¯æ¯”è¾ƒçš„å¤æ‚è®¡ç®—ã€‚

-----

### æ–¹æ¡ˆäºŒï¼š(æ¬¡ä¼˜é€‰æ‹©) è¯­ä¹‰åŒ¹é…ï¼Œè€Œéå…³é”®è¯åŒ¹é… (Semantic Matching, Not Keyword Matching)

å¦‚æœä½ çš„æ–‡æ¡£ç»“æ„éå¸¸ä¸è§„åˆ™ï¼Œå¯¼è‡´æ–¹æ¡ˆä¸€ä¸­çš„ `content.find(heading_text)` éš¾ä»¥å®ç°ï¼Œé‚£ä¹ˆå¯ä»¥å‡çº§ä½ çš„åŒ¹é…ç®—æ³•ï¼Œä½¿ç”¨å‘é‡è¯­ä¹‰ç›¸ä¼¼åº¦æ¥ä»£æ›¿å…³é”®è¯åŒ¹é…ã€‚

ä½ å·²ç»åœ¨é¡¹ç›®ä¸­å®ç°äº† `OpenAICompatibleEmbeddings`ï¼Œè¿™è®©è¯¥æ–¹æ¡ˆå˜å¾—å¯è¡Œã€‚

#### ä¿®å¤æ­¥éª¤ï¼š

1.  ä¿®æ”¹ `IntelligentHierarchicalSplitter` çš„ `__init__` æ–¹æ³•ï¼Œè®©å®ƒå¯ä»¥æ¥æ”¶ `embedding_function`ã€‚
2.  åœ¨ `AIService._create_hierarchical_chunks` ä¸­ï¼Œå°† `self.embeddings` ä¼ é€’ç»™ `IntelligentHierarchicalSplitter`ã€‚
3.  é‡å†™ `_find_best_outline_for_chunk` å‡½æ•°ï¼Œä½¿ç”¨å‘é‡ä½™å¼¦ç›¸ä¼¼åº¦è¿›è¡Œè®¡ç®—ã€‚

#### ä»£ç å®ç°ï¼š

**1. ä¿®æ”¹ `ai_service_langchain.py`**

```python
# In ai_service_langchain.py

# åœ¨ _create_hierarchical_chunks å‡½æ•°ä¸­
# ...
# åˆ›å»ºæ™ºèƒ½åˆ†å—å™¨æ—¶ï¼Œä¼ å…¥LLMå’ŒåµŒå…¥æ¨¡å‹å®ä¾‹
logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æ™ºèƒ½åˆ†å—å™¨...")
splitter = IntelligentHierarchicalSplitter(llm=self.llm, embedding_function=self.embeddings)
# ...
```

**2. ä¿®æ”¹ `hierarchical_splitter.py`**

```python
# In hierarchical_splitter.py
from typing import List, Dict, Any, Optional, Tuple
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.embeddings import Embeddings # å¯¼å…¥Embeddings
import hashlib
import numpy as np # å¯¼å…¥numpyç”¨äºå‘é‡è®¡ç®—

# ...

class IntelligentHierarchicalSplitter:
    """åŸºäºLLMçš„æ™ºèƒ½å¤šå±‚æ¬¡æ–‡æœ¬åˆ†å—å™¨"""
    
    def __init__(self, llm=None, embedding_function: Optional[Embeddings] = None):
        self.llm = llm  # LLMå®ä¾‹
        self.embedding_function = embedding_function # åµŒå…¥å‡½æ•°å®ä¾‹
        # ... (å…¶ä»–åˆå§‹åŒ–ä»£ç )

    # ... (split_document å‡½æ•°ä¿æŒä¸å˜) ...
    
    def _create_intelligent_content_layer(self, content: str, title: str, file_id: int, outline_docs: List[Document], progress_callback=None) -> List[Document]:
        """åˆ›å»ºæ™ºèƒ½å†…å®¹å±‚ï¼ˆåŸºäºå¤§çº²çš„è¯­ä¹‰åˆ†å—ï¼‰ - ä½¿ç”¨æ–¹æ¡ˆäºŒçš„é€»è¾‘"""
        try:
            # è¿™ä¸ªå‡½æ•°çš„æ•´ä½“ç»“æ„ä¸ä½ åŸå§‹ä»£ç ç±»ä¼¼ï¼Œä½†å®ƒä¼šè°ƒç”¨æ–°çš„ _find_best_outline_for_chunk
            if not self.embedding_function:
                logger.error("âŒ è¯­ä¹‰åŒ¹é…éœ€è¦åµŒå…¥å‡½æ•°ï¼Œä½†æœªæä¾›ã€‚é™çº§å¤„ç†...")
                return self._recursive_chunk_content(content, title, file_id)

            logger.info("ğŸ§  åŸºäºè¯­ä¹‰åŒ¹é…è¿›è¡Œæ™ºèƒ½å†…å®¹åˆ†å—")
            chunks = self.content_splitter.split_text(content)
            
            # é¢„è®¡ç®—æ‰€æœ‰å¤§çº²æ–‡æ¡£çš„å‘é‡
            outline_contents = [doc.page_content for doc in outline_docs]
            outline_vectors = self.embedding_function.embed_documents(outline_contents)
            
            content_docs = []
            for i, chunk in enumerate(chunks):
                # ä¸ºå†…å®¹å—æ‰¾åˆ°æœ€ç›¸å…³çš„å¤§çº²é¡¹ç›®ï¼ˆä½¿ç”¨æ–°æ–¹æ³•ï¼‰
                best_outline_index, best_score = self._find_best_outline_for_chunk_semantic(chunk, outline_vectors)

                # è®¾å®šä¸€ä¸ªç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼åˆ™è®¤ä¸ºæ²¡æœ‰åŒ¹é…é¡¹
                SIMILARITY_THRESHOLD = 0.7 
                
                metadata = {
                    "file_id": file_id, "chunk_type": "content", "chunk_level": 3,
                    "chunk_index": i, "title": title,
                    "chunk_hash": hashlib.sha256(chunk.encode()).hexdigest(),
                    "generation_method": "semantic_match"
                }

                if best_outline_index is not None and best_score >= SIMILARITY_THRESHOLD:
                    best_outline_doc = outline_docs[best_outline_index]
                    metadata["parent_heading"] = best_outline_doc.metadata.get('section_path')
                    metadata["section_path"] = f"{best_outline_doc.metadata.get('section_path', 'æœªçŸ¥ç« èŠ‚')} / å†…å®¹å—-{i}"
                    metadata["match_score"] = best_score
                else:
                    metadata["parent_heading"] = None
                    metadata["section_path"] = f"å†…å®¹å—-{i}"

                doc = Document(page_content=chunk, metadata=metadata)
                content_docs.append(doc)

            return content_docs

        except Exception as e:
            # ... (é”™è¯¯å¤„ç†å’Œé™çº§é€»è¾‘) ...
            logger.error(f"âŒ æ™ºèƒ½å†…å®¹åˆ†å—å¤±è´¥: {e}")
            return self._recursive_chunk_content(content, title, file_id)

    def _find_best_outline_for_chunk_semantic(self, chunk: str, outline_vectors: List[List[float]]) -> Tuple[Optional[int], float]:
        """ä¸ºå†…å®¹å—æ‰¾åˆ°æœ€ç›¸å…³çš„å¤§çº²é¡¹ç›®ï¼ˆä½¿ç”¨å‘é‡è¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰"""
        if not chunk or not self.embedding_function:
            return None, 0.0
            
        try:
            chunk_vector = self.embedding_function.embed_query(chunk)
            chunk_vector_np = np.array(chunk_vector)

            best_score = -1.0
            best_index = None

            for i, outline_vector in enumerate(outline_vectors):
                outline_vector_np = np.array(outline_vector)
                
                # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
                cos_sim = np.dot(chunk_vector_np, outline_vector_np) / (np.linalg.norm(chunk_vector_np) * np.linalg.norm(outline_vector_np))
                
                if cos_sim > best_score:
                    best_score = cos_sim
                    best_index = i
            
            return best_index, float(best_score)

        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰åŒ¹é…è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
            return None, 0.0

```

### æ€»ç»“ä¸å»ºè®®

1.  **é¦–é€‰æ–¹æ¡ˆä¸€ï¼ˆç»“æ„åŒ–åˆ†å‰²ï¼‰**ï¼šè¿™æ˜¯æœ€ç›´æ¥ã€æœ€å¯é çš„ä¿®å¤æ–¹æ³•ã€‚å®ƒå°†åˆ†å—é€»è¾‘ä»â€œçŒœæµ‹â€è½¬å˜ä¸ºâ€œç¡®å®šâ€ï¼Œæ˜¯å·¥ä¸šçº§åº”ç”¨çš„é¦–é€‰ã€‚è¯·ä¼˜å…ˆå°è¯•å®ç°è¿™ä¸ªæ–¹æ¡ˆã€‚
2.  **å¤‡é€‰æ–¹æ¡ˆäºŒï¼ˆè¯­ä¹‰åŒ¹é…ï¼‰**ï¼šå¦‚æœæ–‡æ¡£æ ¼å¼æå…¶æ··ä¹±ï¼Œæ— æ³•è¿›è¡Œç»“æ„åŒ–åˆ†å‰²ï¼Œæ–¹æ¡ˆäºŒæ˜¯å¾ˆå¥½çš„æ›¿ä»£æ–¹æ¡ˆã€‚å®ƒèƒ½æœ‰æ•ˆè·¨è¶Šâ€œè¯­ä¹‰é¸¿æ²Ÿâ€ï¼Œä½†ä¼šå¢åŠ åµŒå…¥è®¡ç®—çš„å¼€é”€ã€‚
3.  **åˆ é™¤æ—§ä»£ç **ï¼šæ— è®ºä½ é€‰æ‹©å“ªä¸ªæ–¹æ¡ˆï¼Œéƒ½åº”è¯¥åˆ é™¤æˆ–æ³¨é‡Šæ‰æ—§çš„ `_find_best_outline_for_chunk` å‡½æ•°ï¼Œä»¥é¿å…æ··æ·†ã€‚

æ ¹æ®ä½ çš„æµç¨‹å›¾å’Œä»£ç ç»“æ„ï¼Œæ–¹æ¡ˆä¸€çš„æ”¹åŠ¨æ›´ç¬¦åˆä½ è®¾è®¡åˆ†å±‚ç»“æ„çš„åˆè¡·ï¼Œèƒ½è®©ä½ çš„é«˜çº§RAGç³»ç»Ÿæ›´åŠ ç¨³å¥å’Œç²¾å‡†ã€‚