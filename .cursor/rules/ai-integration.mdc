---
description: AI集成指导，涉及到ai 集成和使用时参考
globs: 
alwaysApply: false
---
# AI集成指导

## AI架构概述

本项目采用纯本地AI方案，通过OpenAI兼容接口连接外部Ollama服务，使用标准化调用方式实现LLM和嵌入模型的统一管理。

## Ollama集成方案

### 外部服务架构
- **LLM模型**：通过宿主机Ollama服务管理（如qwen2.5:0.5b）
- **嵌入模型**：通过宿主机Ollama服务管理（如quentinz/bge-large-zh-v1.5:latest）
- **接口标准**：使用OpenAI兼容接口调用所有模型
- **网络连接**：Docker容器通过host.docker.internal访问宿主机Ollama服务

### 环境变量配置（实际使用的变量名）
```bash
# Ollama服务配置
OPENAI_BASE_URL=http://host.docker.internal:11434
OPENAI_API_KEY=ollama  # 可选，本地通常不需要

# LLM模型配置
OPENAI_MODEL=qwen2.5:0.5b
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2048

# 嵌入模型配置
EMBEDDING_MODEL_NAME=quentinz/bge-large-zh-v1.5:latest
EMBEDDING_DIMENSION=1024
```

### Docker网络配置
```yaml
# docker-compose.yml中的实际配置
backend:
  environment:
    - OPENAI_BASE_URL=http://host.docker.internal:11434
    - OPENAI_API_KEY=ollama
    - EMBEDDING_MODEL_NAME=quentinz/bge-large-zh-v1.5:latest 
    - OPENAI_MODEL=qwen2.5:0.5b
    - LLM_TEMPERATURE=0.7
    - LLM_MAX_TOKENS=2048
    - EMBEDDING_DIMENSION=1024
```

## RAG问答系统

### 系统架构
参考 [README.md](mdc:README.md) 中的"AI问答系统(RAG)流程图"了解详细时序。

### 核心组件

#### 1. 文档预处理
- **文本分块**：将长文档分割为1000字符的块
- **重叠处理**：块之间保持200字符重叠
- **元数据保存**：记录块的来源文件、位置等信息

#### 2. 向量化处理
- **嵌入生成**：调用Ollama嵌入模型API生成向量
- **向量存储**：存储到ChromaDB向量数据库
- **索引建立**：建立高效的向量索引

#### 3. 检索系统
- **混合搜索**：结合关键词模糊搜索和ChromaDB语义搜索
- **相关性排序**：基于相似度和关键词匹配度排序
- **上下文构建**：选择最相关的文档片段作为上下文

#### 4. 生成系统
- **提示工程**：构建包含上下文的提示模板
- **LLM调用**：通过OpenAI兼容接口调用Ollama LLM
- **答案生成**：生成基于上下文的准确答案
- **来源引用**：提供答案来源的文档引用

## 实现细节

### OpenAI兼容接口调用

#### LLM调用示例（实际代码实现）
```python
import httpx

# 配置Ollama客户端
async def call_llm(messages, model="qwen2.5:0.5b"):
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{OPENAI_BASE_URL}/v1/chat/completions",
            headers={"Authorization": f"Bearer {OPENAI_API_KEY}"},
            json={
                "model": model,
                "messages": messages,
                "temperature": 0.7,
                "max_tokens": 2048
            }
        )
        return response.json()
```

#### 嵌入模型调用示例（实际代码实现）
```python
# 生成嵌入向量
async def create_embeddings(text, model="quentinz/bge-large-zh-v1.5:latest"):
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{OPENAI_BASE_URL}/v1/embeddings",
            headers={"Authorization": f"Bearer {OPENAI_API_KEY}"},
            json={
                "model": model,
                "input": text
            }
        )
        result = response.json()
        return result["data"][0]["embedding"]
```

### ChromaDB集成

#### 集合管理
```python
import chromadb

# 初始化ChromaDB客户端
client = chromadb.PersistentClient(path="./data/chroma_db")

# 创建或获取集合
collection = client.get_or_create_collection(
    name="notebook_embeddings",
    metadata={"description": "AI笔记本文档嵌入"}
)
```

#### 向量存储和检索
```python
# 添加文档向量
collection.add(
    embeddings=[embedding_vector],
    documents=[chunk_text],
    metadatas=[{"file_id": file_id, "chunk_index": chunk_index}],
    ids=[f"{file_id}_{chunk_index}"]
)

# 语义搜索
results = collection.query(
    query_embeddings=[query_embedding],
    n_results=10,
    include=["documents", "metadatas", "distances"]
)
```

## 性能优化

### 启动时自动重建机制
- **完全重建策略**：每次容器重启时自动删除现有数据库和向量库
- **避免数据不一致**：消除数据库状态不一致导致的各种错误
- **简化维护逻辑**：不再需要复杂的增量更新和状态检查

### 异步处理
- **后台索引**：文档更新后异步重建索引
- **批量处理**：批量生成嵌入向量提高效率
- **同步保存 + 异步索引**：保证用户体验的同时维护搜索准确性

### 内存管理
- **向量压缩**：使用1024维向量平衡精度和性能
- **分块策略**：1000字符分块大小平衡上下文和检索精度
- **清理机制**：启动时清理所有缓存和临时数据

## 错误处理

### 模型服务异常
- **连接重试**：Ollama服务不可用时的重试机制
- **降级方案**：模型调用失败时的备用方案
- **错误日志**：详细记录AI服务调用的错误信息

### 数据一致性
- **事务处理**：确保向量和元数据的一致性
- **恢复机制**：服务异常后的数据恢复
- **版本管理**：模型版本变更时的数据迁移

## 监控和调试

### 性能监控
- **响应时间**：监控AI服务的响应时间
- **成功率**：统计AI调用的成功率
- **资源使用**：监控内存和GPU使用情况

### 调试工具
- **日志系统**：详细的AI调用日志
- **调试接口**：提供AI服务状态查询接口
- **测试工具**：RAG系统的端到端测试工具

## 扩展性设计

### 模型切换
- **配置化**：通过环境变量轻松切换模型
- **A/B测试**：支持同时运行多个模型进行对比
- **热更新**：支持不重启服务切换模型

### 多模态支持
- **图像理解**：预留图像模型集成接口
- **语音处理**：预留语音模型集成接口
- **多语言**：支持多语言嵌入和生成模型

### 实际模型配置示例
```bash
# 当前使用的模型配置
OPENAI_MODEL=qwen2.5:0.5b  # 轻量级中文LLM
EMBEDDING_MODEL_NAME=quentinz/bge-large-zh-v1.5:latest  # 中文嵌入模型
EMBEDDING_DIMENSION=1024  # BGE模型的实际维度
```

参考 [DATABASE.md](mdc:DATABASE.md) 了解AI相关数据表的详细设计。

