---
description: 整体架构理解
globs: 
alwaysApply: false
---
# 架构设计指导

## 整体架构原则

### 精简高效
- 去掉了不必要的复杂组件（如Redis缓存、Elasticsearch、知识图谱）
- 使用轻量级方案替代重型组件
- 核心服务精简为2个Docker容器 + 外部AI服务

### 纯本地化
- 所有数据和AI模型都在本地运行
- 确保用户隐私和数据安全
- 无需外部API依赖

### 标准化接口
- 使用OpenAI兼容接口，便于扩展
- 统一的API调用方式
- 灵活的配置管理

## 核心架构决策

### 缓存策略
**决策**：去掉Redis缓存
**原因**：个人使用场景不需要复杂缓存
**替代方案**：Python内存缓存和LRU缓存

### 搜索引擎方案
**决策**：使用SQLite LIKE模糊搜索 + ChromaDB混合搜索
**原因**：
- SQLite LIKE：简单可靠的模糊搜索，中文支持好
- ChromaDB：向量搜索，支持语义检索
- 混合搜索：结合关键词和语义搜索的优势

**实现**：
- 使用LIKE操作符进行关键词模糊匹配
- 实现混合搜索（关键词模糊搜索 + ChromaDB语义搜索）

### AI模型管理
**决策**：通过OpenAI兼容接口连接外部Ollama服务
**方案**：
- LLM和嵌入模型都通过外部Ollama管理
- 使用OpenAI兼容接口调用
- 通过环境变量配置API地址、密钥、模型名称
- Docker网络通过host.docker.internal访问宿主机Ollama服务

### 知识图谱功能
**决策**：去掉复杂的知识图谱功能
**原因**：成本效益不匹配，技术复杂度过高，对个人用户价值有限
**替代方案**：
- 链接关系可视化
- 智能标签系统  
- 智能推荐
- 节省2-3周开发时间

## 数据库设计

参考 [DATABASE.md](mdc:DATABASE.md) 了解完整的数据库结构设计。

### 核心数据表（10个）
1. **files** - 文件元信息
2. **links** - 双向链接关系
3. **embeddings** - 向量嵌入
4. **tags** - 智能标签
5. **file_tags** - 文件标签关联
6. **搜索功能** - 基于LIKE操作符的关键词搜索
7. **search_history** - 搜索历史
8. **chat_sessions** - AI对话会话
9. **chat_messages** - AI对话消息
10. **system_config** - 系统配置

## 笔记更新机制

### 同步保存
- 立即更新.md文件和SQLite
- 关键词搜索使用LIKE操作符，简单可靠
- 立即响应用户操作

### 异步索引
- 后台重新生成向量、链接、标签等AI相关数据
- 不阻塞用户操作
- 保证数据一致性

## 混合搜索实现

### 关键词搜索
- 使用SQLite LIKE操作符
- 支持模糊匹配，中文友好
- 简单可靠，无分词问题

### 语义搜索
- 使用ChromaDB向量相似度搜索
- 基于语义理解的内容检索
- 支持跨语言和同义词搜索

### 结果融合
- 智能排序和聚合
- 综合关键词匹配度和语义相似度
- 提供最相关的搜索结果

## 环境变量配置

通过环境变量实现灵活配置：

### 应用配置
- APP_NAME, APP_VERSION, DEBUG_MODE
- SERVER_HOST, SERVER_PORT

### 数据库配置  
- DATABASE_URL, CHROMA_DB_PATH

### AI模型配置（实际使用的变量名）
- OPENAI_BASE_URL, OPENAI_API_KEY
- OPENAI_MODEL, EMBEDDING_MODEL_NAME
- LLM_TEMPERATURE, LLM_MAX_TOKENS
- EMBEDDING_DIMENSION

### 搜索引擎配置
- SEARCH_CHUNK_SIZE, SEARCH_OVERLAP

### 文件存储配置
- NOTES_DIRECTORY, MAX_FILE_SIZE

## 服务架构

### 2个核心Docker服务 + 外部AI服务
1. **frontend** - React前端应用（端口3000）
2. **backend** - FastAPI + SQLite + ChromaDB（端口8000）
3. **外部Ollama服务** - 在宿主机运行的AI模型服务（端口11434）

### 服务通信
- 前端通过HTTP API与后端通信
- 后端通过host.docker.internal:11434访问宿主机Ollama服务
- 使用OpenAI兼容接口与Ollama通信
- 前后端服务通过Docker网络连接

### 实际Docker配置
```yaml
# 实际的docker-compose.yml配置
services:
  frontend:
    build: ./frontend
    ports: ["3000:80"]
    depends_on: [backend]
    
  backend:
    build: ./backend
    ports: ["8000:8000"]
    environment:
      - OPENAI_BASE_URL=http://host.docker.internal:11434
      - EMBEDDING_MODEL_NAME=quentinz/bge-large-zh-v1.5:latest
      - OPENAI_MODEL=qwen2.5:0.5b
    volumes:
      - ./backend/data:/app/data
      - ./notes:/app/notes
```

## 开发指导

### 架构图使用
参考 [README.md](mdc:README.md) 中的5个核心流程图：
1. 整体架构流程图
2. 核心业务流程图  
3. 数据流转图
4. AI问答系统(RAG)流程图
5. 笔记更新工作流程图

### 开发流程
1. **先看架构图理解设计**
2. **按流程实现功能**
3. **数据优先设计**
4. **每个流程节点都要有对应的测试用例**

### 性能考虑
- 使用内存缓存减少数据库查询
- 异步处理耗时操作
- 向量搜索结果缓存
- 文件变更检测优化

### 启动时自动重建机制
- **完全重建策略**：每次容器重启时自动删除现有数据库和向量库
- **避免数据不一致**：消除数据库状态不一致导致的各种错误
- **简化维护逻辑**：不再需要复杂的增量更新和状态检查

