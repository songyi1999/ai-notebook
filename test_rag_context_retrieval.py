#!/usr/bin/env python3
"""
æµ‹è¯•RAGç³»ç»Ÿçš„å›æº¯èƒ½åŠ›
éªŒè¯ä»å†…å®¹å—å¦‚ä½•è·å–æ–‡ä»¶åã€æ€»ç»“å’Œå¤§çº²
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'backend'))

from backend.app.database.session import get_db
from backend.app.services.ai_service_langchain import AIService
from backend.app.models.file import File
from backend.app.models.embedding import Embedding
from sqlalchemy import desc
import json

def test_rag_context_retrieval():
    """æµ‹è¯•RAGç³»ç»Ÿçš„å›æº¯èƒ½åŠ›"""
    
    # è·å–æ•°æ®åº“è¿æ¥
    db = next(get_db())
    
    # åˆå§‹åŒ–AIæœåŠ¡
    ai_service = AIService(db)
    
    if not ai_service.is_available():
        print("âŒ AIæœåŠ¡ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return
    
    print("ğŸ” æµ‹è¯•RAGç³»ç»Ÿå›æº¯èƒ½åŠ›\n")
    
    # 1. æ£€æŸ¥æ•°æ®åº“ä¸­çš„æ–‡ä»¶å’Œå‘é‡æ•°æ®
    print("=== 1. æ•°æ®åº“çŠ¶æ€æ£€æŸ¥ ===")
    
    # ç»Ÿè®¡æ–‡ä»¶æ•°é‡
    total_files = db.query(File).filter(File.is_deleted == False).count()
    print(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {total_files}")
    
    # ç»Ÿè®¡å‘é‡æ•°é‡ï¼ˆæŒ‰ç±»å‹ï¼‰
    summary_count = db.query(Embedding).filter(Embedding.chunk_type == 'summary').count()
    outline_count = db.query(Embedding).filter(Embedding.chunk_type == 'outline').count()
    content_count = db.query(Embedding).filter(Embedding.chunk_type == 'content').count()
    
    print(f"ğŸ“Š å‘é‡ç»Ÿè®¡:")
    print(f"  - æ‘˜è¦å—: {summary_count}")
    print(f"  - å¤§çº²å—: {outline_count}")
    print(f"  - å†…å®¹å—: {content_count}")
    print(f"  - æ€»è®¡: {summary_count + outline_count + content_count}")
    
    # 2. é€‰æ‹©ä¸€ä¸ªæœ‰å®Œæ•´å‘é‡æ•°æ®çš„æ–‡ä»¶è¿›è¡Œæµ‹è¯•
    print("\n=== 2. é€‰æ‹©æµ‹è¯•æ–‡ä»¶ ===")
    
    # æŸ¥æ‰¾æœ‰å®Œæ•´ä¸‰å±‚å‘é‡çš„æ–‡ä»¶
    files_with_complete_vectors = db.query(File).join(Embedding).filter(
        File.is_deleted == False,
        Embedding.chunk_type.in_(['summary', 'outline', 'content'])
    ).distinct().limit(3).all()
    
    if not files_with_complete_vectors:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰å‘é‡æ•°æ®çš„æ–‡ä»¶")
        return
    
    test_file = files_with_complete_vectors[0]
    print(f"ğŸ“„ æµ‹è¯•æ–‡ä»¶: {test_file.title}")
    print(f"ğŸ—‚ï¸  æ–‡ä»¶è·¯å¾„: {test_file.file_path}")
    print(f"ğŸ“ æ–‡ä»¶å¤§å°: {len(test_file.content)} å­—ç¬¦")
    
    # æ£€æŸ¥è¯¥æ–‡ä»¶çš„å‘é‡åˆ†å¸ƒ
    file_vectors = db.query(Embedding).filter(Embedding.file_id == test_file.id).all()
    vector_types = {}
    for vec in file_vectors:
        chunk_type = vec.chunk_type
        if chunk_type not in vector_types:
            vector_types[chunk_type] = []
        vector_types[chunk_type].append({
            'chunk_index': vec.chunk_index,
            'chunk_preview': vec.chunk_text[:100] + '...' if len(vec.chunk_text) > 100 else vec.chunk_text,
            'section_path': vec.section_path
        })
    
    print(f"\nğŸ“Š è¯¥æ–‡ä»¶çš„å‘é‡åˆ†å¸ƒ:")
    for chunk_type, chunks in vector_types.items():
        print(f"  {chunk_type}: {len(chunks)} ä¸ªå—")
        for chunk in chunks[:2]:  # åªæ˜¾ç¤ºå‰2ä¸ª
            print(f"    - å—{chunk['chunk_index']}: {chunk['chunk_preview']}")
    
    # 3. æµ‹è¯•è¯­ä¹‰æœç´¢çš„å›æº¯èƒ½åŠ›
    print("\n=== 3. æµ‹è¯•è¯­ä¹‰æœç´¢å›æº¯ ===")
    
    # ä½¿ç”¨æ–‡ä»¶å†…å®¹çš„ä¸€ä¸ªç‰‡æ®µä½œä¸ºæŸ¥è¯¢
    query = test_file.content[100:200]  # å–ä¸­é—´ä¸€æ®µä½œä¸ºæŸ¥è¯¢
    print(f"ğŸ” æŸ¥è¯¢å†…å®¹: {query}")
    
    # æ‰§è¡Œè¯­ä¹‰æœç´¢
    search_results = ai_service.semantic_search(query, limit=5)
    print(f"\nğŸ“‹ æœç´¢ç»“æœ: {len(search_results)} ä¸ª")
    
    for i, result in enumerate(search_results, 1):
        print(f"\n--- ç»“æœ {i} ---")
        print(f"ğŸ“„ æ–‡ä»¶å: {result.get('title', 'N/A')}")
        print(f"ğŸ—‚ï¸  æ–‡ä»¶è·¯å¾„: {result.get('file_path', 'N/A')}")
        print(f"ğŸ“Š ç›¸ä¼¼åº¦: {result.get('similarity', 0):.4f}")
        print(f"ğŸ·ï¸  å—ç±»å‹: {result.get('chunk_type', 'content')}")
        print(f"ğŸ¯ ç« èŠ‚è·¯å¾„: {result.get('section_path', 'N/A')}")
        print(f"ğŸ“ å†…å®¹é¢„è§ˆ: {result.get('chunk_text', '')[:150]}...")
        
        # éªŒè¯å›æº¯èƒ½åŠ›
        file_id = result.get('file_id')
        if file_id:
            file_info = db.query(File).filter(File.id == file_id).first()
            if file_info:
                print(f"âœ… æˆåŠŸå›æº¯åˆ°æ–‡ä»¶: {file_info.title}")
            else:
                print(f"âŒ æ— æ³•å›æº¯åˆ°æ–‡ä»¶: file_id={file_id}")
    
    # 4. æµ‹è¯•RAGé—®ç­”çš„å›æº¯èƒ½åŠ›
    print("\n=== 4. æµ‹è¯•RAGé—®ç­”å›æº¯ ===")
    
    # åŸºäºæ–‡ä»¶å†…å®¹æ„é€ ä¸€ä¸ªé—®é¢˜
    question = f"å…³äº{test_file.title}çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ"
    print(f"â“ é—®é¢˜: {question}")
    
    # æ‰§è¡ŒRAGé—®ç­”
    rag_result = ai_service.chat_with_context(question, max_context_length=2000, search_limit=3)
    
    print(f"\nğŸ’¬ RAGå›ç­”:")
    print(f"ğŸ“ ç­”æ¡ˆ: {rag_result.get('answer', 'N/A')[:300]}...")
    print(f"ğŸ“Š å¤„ç†æ—¶é—´: {rag_result.get('processing_time', 0)}ç§’")
    print(f"ğŸ“‹ ç›¸å…³æ–‡æ¡£æ•°: {len(rag_result.get('related_documents', []))}")
    
    # æ£€æŸ¥ç›¸å…³æ–‡æ¡£çš„å›æº¯ä¿¡æ¯
    print(f"\nğŸ”— ç›¸å…³æ–‡æ¡£å›æº¯ä¿¡æ¯:")
    for i, doc in enumerate(rag_result.get('related_documents', []), 1):
        print(f"  {i}. æ–‡ä»¶: {doc.get('title', 'N/A')}")
        print(f"     è·¯å¾„: {doc.get('file_path', 'N/A')}")
        print(f"     ç±»å‹: {doc.get('chunk_type', 'content')}")
        print(f"     ç« èŠ‚: {doc.get('section_path', 'N/A')}")
        print(f"     ç›¸ä¼¼åº¦: {doc.get('similarity', 0):.4f}")
    
    # 5. æµ‹è¯•å¤šå±‚æ¬¡ä¸Šä¸‹æ–‡æœç´¢
    print("\n=== 5. æµ‹è¯•å¤šå±‚æ¬¡ä¸Šä¸‹æ–‡æœç´¢ ===")
    
    # æµ‹è¯•ä¸åŒç±»å‹çš„é—®é¢˜
    test_questions = [
        ("æ¦‚è§ˆæ€§é—®é¢˜", "è¯·ä»‹ç»ä¸€ä¸‹ç³»ç»Ÿçš„æ•´ä½“æ¶æ„"),
        ("å…·ä½“æ€§é—®é¢˜", "å¦‚ä½•å®ç°å‘é‡æœç´¢åŠŸèƒ½ï¼Ÿ"),
        ("å¹³è¡¡æ€§é—®é¢˜", "æ•°æ®åº“è®¾è®¡æœ‰å“ªäº›ç‰¹ç‚¹ï¼Ÿ")
    ]
    
    for question_type, question in test_questions:
        print(f"\n--- {question_type} ---")
        print(f"â“ é—®é¢˜: {question}")
        
        # ä½¿ç”¨å¤šå±‚æ¬¡ä¸Šä¸‹æ–‡æœç´¢
        try:
            context_results = ai_service._hierarchical_context_search(question, search_limit=3)
            print(f"ğŸ“Š å¤šå±‚æ¬¡æœç´¢ç»“æœ: {len(context_results)} ä¸ª")
            
            # ç»Ÿè®¡ä¸åŒç±»å‹çš„å—
            type_counts = {}
            for result in context_results:
                chunk_type = result.get('chunk_type', 'content')
                type_counts[chunk_type] = type_counts.get(chunk_type, 0) + 1
            
            print(f"ğŸ“‹ å—ç±»å‹åˆ†å¸ƒ: {type_counts}")
            
        except Exception as e:
            print(f"âŒ å¤šå±‚æ¬¡æœç´¢å¤±è´¥: {e}")
    
    # 6. éªŒè¯å›æº¯åŠŸèƒ½çš„å®Œæ•´æ€§
    print("\n=== 6. å›æº¯åŠŸèƒ½å®Œæ•´æ€§éªŒè¯ ===")
    
    # éšæœºé€‰æ‹©ä¸€ä¸ªå‘é‡å—ï¼ŒéªŒè¯æ˜¯å¦èƒ½å®Œæ•´å›æº¯
    random_embedding = db.query(Embedding).filter(Embedding.chunk_type == 'content').first()
    if random_embedding:
        print(f"ğŸ¯ éšæœºé€‰æ‹©å‘é‡å—: chunk_index={random_embedding.chunk_index}")
        print(f"ğŸ“„ å—ç±»å‹: {random_embedding.chunk_type}")
        print(f"ğŸ—‚ï¸  ç« èŠ‚è·¯å¾„: {random_embedding.section_path}")
        
        # 1. è·å–æ–‡ä»¶å
        file_info = db.query(File).filter(File.id == random_embedding.file_id).first()
        if file_info:
            print(f"âœ… æ–‡ä»¶å: {file_info.title}")
            print(f"âœ… æ–‡ä»¶è·¯å¾„: {file_info.file_path}")
        else:
            print("âŒ æ— æ³•è·å–æ–‡ä»¶å")
        
        # 2. è·å–æ–‡ä»¶æ€»ç»“
        summary_embedding = db.query(Embedding).filter(
            Embedding.file_id == random_embedding.file_id,
            Embedding.chunk_type == 'summary'
        ).first()
        if summary_embedding:
            print(f"âœ… æ–‡ä»¶æ€»ç»“: {summary_embedding.chunk_text[:100]}...")
        else:
            print("âŒ æ— æ³•è·å–æ–‡ä»¶æ€»ç»“")
        
        # 3. è·å–æ–‡ä»¶å¤§çº²
        outline_embeddings = db.query(Embedding).filter(
            Embedding.file_id == random_embedding.file_id,
            Embedding.chunk_type == 'outline'
        ).all()
        if outline_embeddings:
            print(f"âœ… æ–‡ä»¶å¤§çº²: {len(outline_embeddings)} ä¸ªç« èŠ‚")
            for outline in outline_embeddings[:2]:
                print(f"   - {outline.section_path}: {outline.chunk_text[:80]}...")
        else:
            print("âŒ æ— æ³•è·å–æ–‡ä»¶å¤§çº²")
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
    print("\nğŸ“Š ç»“è®º:")
    print("âœ… ç³»ç»Ÿå·²å…·å¤‡å®Œæ•´çš„å›æº¯èƒ½åŠ›")
    print("âœ… å¯ä»¥ä»å†…å®¹å—è·å–æ–‡ä»¶åã€æ€»ç»“å’Œå¤§çº²")
    print("âœ… RAGé—®ç­”èƒ½å¤Ÿåˆ©ç”¨å¤šå±‚æ¬¡ä¿¡æ¯")
    print("âœ… æ”¯æŒæ™ºèƒ½ä¸Šä¸‹æ–‡æ‰©å±•")
    
    db.close()

if __name__ == "__main__":
    test_rag_context_retrieval() 